@book{Mokken1971,
address = {Berlin},
author = {Mokken, R J},
isbn = {9783110813203 3110813203},
language = {English},
publisher = {De Gruyter},
title = {{A Theory and Procedure of Scale Analysis}},
url = {https://doi.org/10.1515/9783110813203},
year = {1971}
}
@article{Gelman1995,
abstract = {This article presents the authors' comments on the article Bayesian Model Selection in Social Research by Adrian E. Raftery in the January 1995 issue. Raftery's paper addresses two important problems in the statistical analysis of social science data: (1) choosing an appropriate model when so much data are available that standard P-values reject all parsimonious models; and (2) making estimates and predictions when there are not enough data available to fit the desired model using standard techniques. For both problems, we agree with Raftery that classical frequentist methods fail and that Raftery's suggested methods based on Bayesian information criterion (BIC) can point in better directions. Nevertheless, we disagree with his solutions because, in principle, they are still directed off-target and only by serendipity manage to hit the target in special circumstances. Our primary criticisms of Raftery's proposals are that (1) he promises the impossible: the selection of a model that is adequate for specific purposes without consideration of those purposes; and (2) he uses the same limited tool for model averaging as for model selection, thereby depriving himself of the benefits of the broad range of available Bayesian procedures.},
author = {Gelman, Andrew and Rubin, Donald B.},
doi = {10.2307/271064},
issn = {00811750},
journal = {Sociological Methodology},
pages = {1--7},
title = {{Avoiding Model Selection in Bayesian Social Research}},
volume = {25},
year = {1995}
}
@book{Fischer1974,
address = {Bern},
author = {Fischer, Gerhard H},
isbn = {3456800398 9783456800394},
language = {German},
publisher = {H. Huber},
title = {{Einf{\"{u}}hrung in die Theorie psychologischer Tests: Grundlagen und Anwendungen}},
year = {1974}
}
@article{Muller-Schneider2001,
abstract = {In diesem Beitrag geht es um eines der zentralen methodischen Probleme, die bei der sozialwissenschaftlichen Skalierung auftreten: das Erkennen mehrdimensionaler Datenstrukturen. In der Forschungspraxis setzt man dazu in aller Regel die Faktorenanalyse ein. Dieses Verfahren f{\"{u}}hrt aber aufgrund seiner restriktiven Modellannahmen nicht immer zum gew{\"{u}}nschten Ergebnis. Anhand eines Fallbeispiels zeige ich, dass komplexe Dimensionen unter Umst{\"{a}}nden nur „orthogonal fragmentiert“ wiedergegeben werden. Ein alternatives Verfahren, das ohne st{\"{o}}rende Modellrestriktionen auskommt, ist die von Mokken (1971) entwickelte multiple Skalierung dichotomer Items. Die Mokkenskalierung verf{\"{a}}hrt nach dem Kristallisationsprinzip und entdeckt zuverl{\"{a}}ssig mehrdimensionale Strukturen. Die auf dem Kristallisationsprinzip aufbauende multiple Skalierung ist ein allgemeines mehrdimensionales Skalierungsverfahren, das mit Hilfe der hierarchischen Clusteranalyse auch im Rahmen der klassischen Testtheorie eingesetzt werden kann.This paper focuses on one of the central methodological problems of scaling in the social sciences: the identification of multidimensional structures. In most practical research contexts exploratory factor analysis is used to solve this problem. However, the problem with this statistical procedure is that - due to certain restrictions - it does not always produce the desired outcome. Drawing evidence from one example, the author shows that under certain conditions complex dimensions are “fragmented” by orthogonal exploratory factor analysis. An alternative method which does not have distorting restrictions is a multiple scaling procedure for dichotomous items developed by Mokken (1971). Mokken scaling operates according to the principle of crystallization and reliably identifies multidimensional structures. Multiple scaling is a general multidimensional scaling method that, with the help of cluster analysis, can be implemented within classical test theory.},
author = {M{\"{u}}ller-Schneider, Thomas},
doi = {10.1515/zfsoz-2001-0404},
issn = {0340-1804},
journal = {Zeitschrift f{\"{u}}r Soziologie},
number = {4},
pages = {305--315},
title = {{Multiple Skalierung nach dem Kristallisationsprinzip / Multiple Scaling According to the Principle of Crystallization}},
volume = {30},
year = {2001}
}
@article{Meijer1990,
abstract = {The Mokken model of monotone homogeneity, the Mokken model of double monotonicity, and the Rasch model are theoretically and empirically compared. These models are compared with respect to restrictiveness to empirical test data, properties of the scale, and accuracy of measurement. Appli cation of goodness-of-fit procedures to empirical data largely confirmed the expected order of the models according to restrictiveness: Almost all items were in concordance with the model of mono tone homogeneity, and fewer items complied with the model of double monotonicity and the Rasch model. The model of monotone homogeneity was found to be a suitable alternative to more restric tive models for basic testing applications; more sophisticated applications, such as equating and adaptive testing, appear to require the use of para metric models. {\textcopyright} 1990, Sage Publications. All rights reserved.},
author = {Meijer, Rob R. and Sijtsma, Klaas and Smid, Nico G.},
doi = {10.1177/014662169001400306},
issn = {15523497},
journal = {Applied Psychological Measurement},
number = {3},
title = {{Theoretical and Empirical Comparison of the Mokken and the Rasch Approach to IRT}},
volume = {14},
year = {1990}
}
@article{Sijtsma2011,
abstract = {We explain why invariant item ordering (IIO) is an important property in non-cognitive measurement and we discuss that IIO cannot be easily generalized from dichotomous data to polytomous data, as some authors seem to suggest. Methods are discussed to investigate IIO for polytomous items and an empirical example shows how these methods can be used in practice. {\textcopyright} 2010 Elsevier Ltd.},
author = {Sijtsma, Klaas and Meijer, Rob R. and {Andries van der Ark}, L.},
doi = {10.1016/J.PAID.2010.08.016},
issn = {0191-8869},
journal = {Personality and Individual Differences},
keywords = {Hierarchical scale,Invariant item ordering,Item response theory,Mokken scaling,Non-cognitive measurement,Personality measurement},
month = {jan},
number = {1},
pages = {31--37},
publisher = {Pergamon},
title = {{Mokken scale analysis as time goes by: An update for scaling practitioners}},
volume = {50},
year = {2011}
}
@article{Hemker1995,
abstract = {An automated item selection procedure for selecting unidimensional scales of polytomous items from multi dimensional datasets is developed for use in the context of the Mokken item response theory model of monotone homogeneity (Mokken {\&} Lewis, 1982). The selection procedure is directly based on the selection procedure proposed by Mokken (1971, p. 187) and relies heavily on the scalability coefficient H (Loevinger, 1948; Molenaar, 1991). New theoretical results relating the latent model structure to H are provided. The item selec tion procedure requires selection of a lower bound for H. A simulation study determined ranges of H for which the unidimensional item sets were retrieved from multidimensional datasets. If multidimensionality is suspected in an empirical dataset, well-chosen lower bound values can be used effectively to detect the unidi mensional scales. {\textcopyright} 1995, Sage Publications. All rights reserved.},
author = {Hemker, Bas T. and Sijtsma, Klaas and Molenaar, Ivo W.},
doi = {10.1177/014662169501900404},
issn = {15523497},
journal = {Applied Psychological Measurement},
number = {4},
title = {{Selection of Unidimensional Scales From a Multidimensional Item Bank in the Polytomous Mokken I RT Model}},
volume = {19},
year = {1995}
}
@book{Bortz2016,
address = {Berlin, Heidelberg},
author = {Bortz, J{\"{u}}rgen and Schuster, Christof},
isbn = {9783662503737 3662503735},
language = {German},
publisher = {Springer},
title = {{Statistik f{\"{u}}r Human- und Sozialwissenschaftler}},
year = {2016}
}
@article{Asun2016,
abstract = {This study compares the performance of two approaches in analysing four-point Likert rating scales with a factorial model: the classical factor analysis (FA) and the item factor analysis (IFA). For FA, maximum likelihood and weighted least squares estimations using Pearson correlation matrices among items are compared. For IFA, diagonally weighted least squares and unweighted least squares estimations using items polychoric correlation matrices are compared. Two hundred and ten conditions were simulated in a Monte Carlo study considering: one to three factor structures (either, independent and correlated in two levels), medium or low quality of items, three different levels of item asymmetry and five sample sizes. Results showed that IFA procedures achieve equivalent and accurate parameter estimates; in contrast, FA procedures yielded biased parameter estimates. Therefore, we do not recommend classical FA under the conditions considered. Minimum requirements for achieving accurate results using IFA procedures are discussed.},
author = {As{\'{u}}n, Rodrigo A. and Rdz-Navarro, Karina and Alvarado, Jes{\'{u}}s M.},
doi = {10.1177/0049124114566716},
issn = {15528294},
journal = {Sociological Methods and Research},
keywords = {Likert scales,classical factor analysis,four-point items,item factor analysis,polychoric correlation},
number = {1},
pages = {109--133},
title = {{Developing Multidimensional Likert Scales Using Item Factor Analysis: The Case of Four-point Items}},
volume = {45},
year = {2016}
}
@article{Zumbo2007,
abstract = {Two new reliability indices, ordinal coefficient alpha and ordinal coefficient theta, are introduced. A simulation study was conducted in order to compare the new ordinal reliability estimates to each other and to coefficient alpha with Likert data. Results indicate that ordinal coefficients alpha and theta are consistently suitable estimates of the theoretical reliability, regardless of the magnitude of the theoretical reliability, the number of scale points, and the skewness of the scale point distributions. In contrast, coefficient alpha is in general a negatively biased estimate of reliability. The use of ordinal coefficients alpha and theta as alternatives to coefficient alpha when estimating the reliability based on Likert response items are recommended. The choice between the two ordinal coefficients depends on whether one is assuming a factor analysis model (ordinal coefficient alpha) or a principal components analysis model (ordinal coefficient theta). Copyright {\textcopyright} 2007 JMASM, Inc.},
author = {Zumbo, Bruno D. and Gadermann, Anne M. and Zeisser, Cornelia},
doi = {10.22237/jmasm/1177992180},
issn = {15389472},
journal = {Journal of Modern Applied Statistical Methods},
keywords = {Coefficient alpha,Coefficient theta,Internal consistency,Reliability},
number = {1},
pages = {21--29},
title = {{Ordinal versions of coefficients alpha and theta for likert rating scales}},
volume = {6},
year = {2007}
}
@article{TenBerge1999,
abstract = {It has been shown by Kaiser that the sum of coefficients alpha of a set of principal components does not change when the components are transformed by an orthogonal rotation. In this paper, Kaiser's result is generalized. First, the invariance property is shown to hold for any set of orthogonal components. Next, a similar invariance property is derived for the reliability of any set of components. Both generalizations are established by considering simultaneously optimal weights for components with maximum alpha and with maximum reliability, respectively. A short-cut formula is offered to evaluate the coefficients alpha for orthogonally rotated principal components from rotation weights and eigenvalues of the correlation matrix. Finally, the greatest lower bound to reliability and a weighted version are discussed.},
author = {{Ten Berge}, Jos M.F. and Hofstee, Willem K.B.},
doi = {10.1007/BF02294321},
issn = {00333123},
journal = {Psychometrika},
keywords = {Coefficient alpha,Components,Factors,Greatest lower bound to reliability,Reliability,Rotation},
number = {1},
pages = {83--90},
title = {{Coefficients alpha and reliabilities of unrotated and rotated components}},
volume = {64},
year = {1999}
}
@article{Conti2014,
abstract = {This paper develops and applies a Bayesian approach to Exploratory Factor Analysis that improves on ad hoc classical approaches. Our framework relies on dedicated factor models and simultaneously determines the number of factors, the allocation of each measurement to a unique factor, and the corresponding factor loadings. Classical identification criteria are applied and integrated into our Bayesian procedure to generate models that are stable and clearly interpretable. A Monte Carlo study confirms the validity of the approach. The method is used to produce interpretable low dimensional aggregates from a high dimensional set of psychological measurements.},
author = {Conti, Gabriella and Fr{\"{u}}hwirth-Schnatter, Sylvia and Heckman, James J. and Piatek, R{\'{e}}mi},
doi = {10.1016/j.jeconom.2014.06.008},
issn = {18726895},
journal = {Journal of Econometrics},
number = {1},
title = {{Bayesian exploratory factor analysis}},
volume = {183},
year = {2014}
}
@misc{Fabrigar1999,
abstract = {Despite the widespread use of exploratory factor analysis in psychological research, researchers often make questionable decisions when conducting these analyses. This article reviews the major design and analytical decisions that must be made when conducting a factor analysis and notes that each of these decisions has important consequences for the obtained results. Recommendations that have been made in the methodological literature are discussed. Analyses of 3 existing empirical data sets are used to illustrate how questionable decisions in conducting factor analyses can yield problematic results. The article presents a survey of 2 prominent journals that suggests that researchers routinely conduct analyses using such questionable methods. The implications of these practices for psychological research are discussed, and the reasons for current practices are reviewed.},
author = {Fabrigar, Leandre R. and MacCallum, Robert C. and Wegener, Duane T. and Strahan, Erin J.},
booktitle = {Psychological Methods},
doi = {10.1037/1082-989X.4.3.272},
issn = {1082989X},
number = {3},
title = {{Evaluating the use of exploratory factor analysis in psychological research}},
volume = {4},
year = {1999}
}
@book{Ueberla1971,
address = {Berlin},
author = {{\"{U}}berla, Karl},
edition = {2. Aufl},
isbn = {3540043683},
publisher = {Springer},
title = {{Faktorenanalyse}},
year = {1971}
}
@book{Lord1968,
address = {Reading, Mass.},
author = {Lord, Frederic M and Tukey, John W and Novick, Melvin R},
isbn = {0201043106 9780201043105},
language = {In English.},
publisher = {Addison-Wesley Pub. Co.},
title = {{Statistical theories of mental test scores}},
year = {1968}
}
@article{Jung2011,
abstract = {Traditionally, two distinct approaches have been employed for exploratory factor analysis: maximum likelihood factor analysis and principal component analysis. A third alternative, called regularized exploratory factor analysis, was introduced recently in the psychometric literature. Small sample size is an important issue that has received considerable discussion in the factor analysis literature. However, little is known about the differential performance of these three approaches to exploratory factor analysis in a small sample size scenario. A simulation study and an empirical example demonstrate that regularized exploratory factor analysis may be recommended over the two traditional approaches, particularly when sample sizes are small (below 50) and the sample covariance matrix is near singular. {\textcopyright} 2011 Psychonomic Society, Inc.},
author = {Jung, Sunho and Lee, Soonmook},
doi = {10.3758/s13428-011-0077-9},
issn = {15543528},
journal = {Behavior Research Methods},
keywords = {Exploratory factor analysis,Monte Carlo simulations,Near singular covariance matrix,Regularization,Small sample size},
number = {3},
pages = {701--709},
pmid = {21431996},
title = {{Exploratory factor analysis for small samples}},
volume = {43},
year = {2011}
}
@article{Hogarty2005,
abstract = {The purpose of this study was to investigate the relationship between sample size and the quality of factor solutions obtained from exploratory factor analysis. This research expanded upon the range of conditions previously examined, employing a broad selection of criteria for the evaluation of the quality of sample factor solutions. Results showed that when communalities are high, sample size tended to have less influence on the quality of factor solutions than when communalities are low. Overdetermination of factors was also shown to improve the factor analysis solution. Finally, decisions about the quality of the factor solution depended upon which criteria were examined. {\textcopyright} 2005 Sage Publications.},
author = {Hogarty, Kristine Y. and Hines, Constance V. and Kromrey, Jeffrey D. and Perron, John M. and Mumford, Karen R.},
doi = {10.1177/0013164404267287},
issn = {00131644},
journal = {Educational and Psychological Measurement},
keywords = {Commonality,Eexploratory factor analysis,Factor recovery,Factor retention,Overdetermination,Sample size},
number = {2},
pages = {202--226},
title = {{The quality of factor solutions in exploratory factor analysis: The influence of sample size, communality, and overdetermination}},
volume = {65},
year = {2005}
}
@article{Zhao2009,
abstract = {I did an apprentice project studying the reasons why students withdrew from their online courses. In this project, I got a dataset that had 35 variables indicating various withdrawal reasons. I wanted to use factor analysis to reduce the 35 variables to a few categories of withdrawal reasons. However, I only have 47 casese in the dataset. Many people suggested that the number of cases was too small for performing a factor analysis. But, I really do not want to waste the time and energy I had spent and just throw away this dataset. Yes! I want to "explain the most with the least" (Henson {\&} Roberts, 2006, p. 393).},
author = {Zhao, Nathan},
journal = {Wiki of Encorelab Toronto},
number = {1992},
pages = {1--7},
title = {{The minimum sample size in factor analysis}},
volume = {250},
year = {2009}
}
@article{Pearson2010,
abstract = {Minimum sample sizes are recommended for conducting exploratory factor analysis on dichotomous data. A Monte Carlo simulation was conducted, varying the level of communalities, number of factors, variable-to-factor ratio and dichotomization threshold. Sample sizes were identified based on congruence between rotated population and sample factor loadings. {\textcopyright} 2010 JMASM, Inc.},
author = {Pearson, Robert H. and Mundfrom, Daniel J.},
doi = {10.22237/jmasm/1288584240},
issn = {15389472},
journal = {Journal of Modern Applied Statistical Methods},
keywords = {Dichotomous data,Exploratory factor analysis,Sample size.},
number = {2},
pages = {359--368},
title = {{Recommended sample size for conducting exploratory factor analysis on dichotomous data}},
volume = {9},
year = {2010}
}
@article{MacCallum2001,
abstract = {This article examines effects of sample size and other design features on correspondence between factors obtained from analysis of sample data and those present in the population from which the samples were drawn. We extend earlier work on this question by examining these phenomena in the situation in which the common factor model does not hold exactly in the population. We present a theoretical framework for representing such lack of fit and examine its implications in the population and sample. Based on this approach we hypothesize that lack of fit of the model in the population will not, on the average, influence recovery of population factors in analysis of sample data, regardless of degree of model error and regardless of sample size. Rather, such recovery will be affected only by phenomena related to sampling error which have been studied previously. These hypotheses are investigated and verified in two sampling studies, one using artificial data and one using empirical data.},
author = {MacCallum, Robert C. and Widaman, Keith F. and Preacher, Kristopher J. and Hong, Sehee},
doi = {10.1207/S15327906MBR3604_06},
issn = {00273171},
journal = {Multivariate Behavioral Research},
number = {4},
pages = {611--637},
title = {{Sample size in factor analysis: The role of model error}},
volume = {36},
year = {2001}
}
@book{Kline2014,
abstract = {The aim of the Easy Guide is to provide an explication of the basic mathematics of factor analysis which anybody who can manage any form of tertiary education can follow, so that, at the end of the book, readers will understand factor analysis. All mathematical terms, even the most simple, will be explained. All mathematical processes will be illustrated by examples. Readers will not be expected to make mathematical inferences. Each step will be explained. If the basic mathematics of factor analysis is understood, readers will then be able to use the technique effectively for research and, perhaps more importantly, they will be able to evaluate its use in journal papers. For much factor analytic research, as has been shown by Cattell (1978), is technically inadequate, rendering the results valueless. In addition I believe that, after reading this book, the excellent books to which I have already referred will become the useful texts which they were intended to be.},
address = {London},
author = {Kline, Paul},
booktitle = {An Easy Guide to Factor Analysis},
doi = {10.4324/9781315788135},
publisher = {Routledge},
title = {{An Easy Guide to Factor Analysis}},
year = {1994}
}
@article{Rummel1967,
author = {Rummel, R. J.},
doi = {10.1177/002200276701100405},
issn = {15528766},
journal = {Journal of Conflict Resolution},
number = {4},
pages = {444--480},
title = {{Understanding factor analysis}},
volume = {11},
year = {1967}
}
@article{Heise1973,
author = {Heise, David R.},
doi = {10.2307/270830},
issn = {00811750},
journal = {Sociological Methodology},
title = {{Some Issues in Sociological Measurement}},
volume = {5},
year = {1973}
}
@book{McDonald1999,
address = {Hillsdale},
author = {McDonald, Roderick P},
language = {English},
publisher = {Erlbaum},
title = {{Test theory: A Unified Treatment}},
year = {1999}
}
@article{Brown2009,
abstract = {QUESTION: In Chapter 7 of the 2008 book on heritage language learning that you co- edited with Kimi Kondo-Brown, there is a study (Lee and Kim, 2008) comparing the attitudes of 111 Korean heritage language learners. On page 167 of that book, a principal components analysis (with varimax rotation) describes the relationships among 16 purported reasons for studying Korean with four broader factors. Several questions come to mind. What is a principal components analysis? How does principal components analysis differ from factor analysis? What guidelines do researchers need to bear in mind when selecting "factors"? And finally, what is a varimax rotation, and why is it applied? ANSWER: This is an interesting question, but a big one, made up of at least three sets of sub-questions: (a) What are principal components analysis (PCA) and exploratory factor analysis (EFA), how are they different, and how do researchers decide which to use? (b) How do investigators determine the number of components or factors to include in the analysis? (c) What is rotation, what are the different types, and how do researchers decide which to use? And, (d) how are PCA and EFA used in language test and questionnaire development? I will address the first one (a) in this column. And, I'll turn to the},
author = {Brown, James Dean},
journal = {JALT Testing {\&} Evaluation SIG Newsletter},
number = {1},
title = {{Principal components analysis and exploratory factor analysis— Definitions, differences, and choices}},
volume = {13},
year = {2009}
}
@article{VanAlphen1994,
abstract = {In nursing research many concepts are measured by questionnaires Respondents are asked to respond to a set of related statements or questions In unidimensional scaling these statements or questions are indicants of the same concept Scaling means to assign numbers to respondents, according to their position on the continuum underlying the concept It is very common to use the summative Likert scaling procedure The sumscore of the responses to the items is the estimator of the position of the patient on the continuum The rationale behind this procedure is classical test theory The main assumption in this theory is that all items are parallel instruments The Rasch model offers an alternative scaling procedure With Rasch both respondents and items are scaled on the same continuum Whereas in Likert scaling all items have the same weight in the summating procedure, in the Rasch model items are differentiated from each other by‘difficulty'The model holds that the probability of a positive response to an item is dependent on the difference between the difficulty of the item and the value of the person on the latent trait The rationale behind this procedure is item response theory In this paper both scaling procedures and their rationales are discussed Copyright {\textcopyright} 1994, Wiley Blackwell. All rights reserved},
author = {van Alphen, Arnold and Halfens, Ruud and Hasman, Arie and Imbos, Tjaart},
doi = {10.1046/j.1365-2648.1994.20010196.x},
issn = {13652648},
journal = {Journal of Advanced Nursing},
number = {1},
pages = {196--201},
pmid = {7930122},
title = {{Likert or Rasch? Nothing is more applicable than good theory}},
volume = {20},
year = {1994}
}
@article{Jackson1977,
abstract = {Let $\Sigma$x be the (population) dispersion matrix, assumed well-estimated, of a set of non-homogeneous item scores. Finding the greatest lower bound for the reliability of the total of these scores is shown to be equivalent to minimizing the trace of $\Sigma$x by reducing the diagonal elements while keeping the matrix non-negative definite. Using this approach, Guttman's bounds are reviewed, a method is established to determine whether his $\lambda$4 (maximum split-half coefficient alpha) is the greatest lower bound in any instance, and three new bounds are discussed. A geometric representation, which sheds light on many of the bounds, is described. {\textcopyright} 1977 Psychometric Society.},
author = {Jackson, Paul H. and Agunwamba, Christian C.},
doi = {10.1007/BF02295979},
issn = {00333123},
journal = {Psychometrika},
keywords = {coefficient alpha,non-homogeneous composites,reliability bounds},
number = {4},
pages = {567--578},
title = {{Lower bounds for the reliability of the total score on a test composed of non-homogeneous items: I: Algebraic lower bounds}},
volume = {42},
year = {1977}
}
@article{McDonald1978,
abstract = {It is shown that if a behavior domain can be described by the common factor model with a finite number of factors, the squared correlation between the sum of a selection of items and the domain total score is actually greater than coefficient alpha. Equality is attained only if the selected items are parallel. Generalizability can be correctly assessed as a function of the item uniquenesses and the test variance. {\textcopyright} 1978, Sage Publications. All rights reserved.},
author = {McDonald, Roderick P.},
doi = {10.1177/001316447803800111},
issn = {15523888},
journal = {Educational and Psychological Measurement},
number = {1},
pages = {75--79},
title = {{Generalizability in factorable domains: “Domain validity and generalizability”}},
volume = {38},
year = {1978}
}
@article{Cho2015,
abstract = {This study disproves the following six common misconceptions about coefficient alpha: (a) Alpha was first developed by Cronbach. (b) Alpha equals reliability. (c) A high value of alpha is an indication of internal consistency. (d) Reliability will always be improved by deleting items using “alpha if item deleted.” (e) Alpha should be greater than or equal to.7 (or, alternatively,.8). (f) Alpha is the best choice among all published reliability coefficients. This study discusses the inaccuracy of each of these misconceptions and provides a correct statement. This study recommends that the assumptions of unidimensionality and tau-equivalency be examined before the application of alpha and that structural equation modeling (SEM)–based reliability estimators be substituted for alpha when one of these conditions is not satisfied. This study also provides formulas for SEM-based reliability estimators that do not rely on matrix notation and step-by-step explanations for the computation of SEM-based reliability estimates.},
author = {Cho, Eunseong and Kim, Seonghoon},
doi = {10.1177/1094428114555994},
issn = {15527425},
journal = {Organizational Research Methods},
keywords = {coefficient alpha,internal consistency,multidimensionality,multiple-factor model,reliability,tau-equivalency},
number = {2},
pages = {207--230},
title = {{Cronbach's Coefficient Alpha: Well Known but Poorly Understood}},
volume = {18},
year = {2015}
}
@article{Sijtsma2015,
abstract = {We discuss reliability definitions from the perspectives of classical test theory, factor analysis, and generalizability theory. For each method, we discuss the rationale, the estimation of reliability, and the goodness of fit of the model that defines the reliability coefficient to the data. Similarities and differences in the three approaches are highlighted. Finally, we provide a computational example using generated data to illustrate the differences among the different reliability methods.},
author = {Sijtsma, Klaas and {Van Der Ark}, L. Andries},
doi = {10.1097/NNR.0000000000000077},
issn = {15389847},
journal = {Nursing Research},
keywords = {classical test theory reliability,factor analysis reliability,generalizability coefficients,test score reliability},
number = {2},
pages = {128--136},
pmid = {25738624},
title = {{Conceptions of reliability revisited and practical recommendations}},
volume = {64},
year = {2015}
}
@article{Barbaranelli2015,
abstract = {Knowledge of a scale's dimensionality is an essential preliminary step to the application of any measure of reliability derived from classical test theory - an approach commonly used is nursing research. The focus of this article is on the applied aspects of reliability and dimensionality testing. Throughout the article, the Self-Care of Heart Failure Index is used to exemplify real-world data challenges of quantifying reliability and to provide insight into how to overcome such challenges.},
author = {Barbaranelli, Claudio and Lee, Christopher S. and Vellone, Ercole and Riegel, Barbara},
doi = {10.1097/NNR.0000000000000079},
issn = {15389847},
journal = {Nursing Research},
keywords = {Self-Care of Heart Failure Index,psychological measurement,psychometrics,questionnaires,reliability,reproducibility of results},
number = {2},
pages = {140--145},
pmid = {25738626},
title = {{The problem with Cronbach's alpha: Comment on Sijtsma and van der Ark (2015)}},
volume = {64},
year = {2015}
}
@article{McNeish2018,
abstract = {Empirical studies in psychology commonly report Cronbach's alpha as a measure of internal consistency reliability despite the fact that many methodological studies have shown that Cronbach's alpha is riddled with problems stemming from unrealistic assumptions. In many circumstances, violating these assumptions yields estimates of reliability that are too small, making measures look less reliable than they actually are. Although methodological critiques of Cronbach's alpha are being cited with increasing frequency in empirical studies, in this tutorial we discuss how the trend is not necessarily improving methodology used in the literature. That is, many studies continue to use Cronbach's alpha without regard for its assumptions or merely cite methodological articles advising against its use to rationalize unfavorable Cronbach's alpha estimates. This tutorial first provides evidence that recommendations against Cronbach's alpha have not appreciably changed how empirical studies report reliability. Then, we summarize the drawbacks of Cronbach's alpha conceptually without relying on mathematical or simulation-based arguments so that these arguments are accessible to a broad audience. We continue by discussing several alternative measures that make less rigid assumptions which provide justifiably higher estimates of reliability compared to Cronbach's alpha. We conclude with empirical examples to illustrate advantages of alternative measures of reliability including omega total, Revelle's omega total, the greatest lower bound, and Coefficient H. A detailed software appendix is also provided to help researchers implement alternative methods.},
author = {McNeish, Daniel},
doi = {10.1037/met0000144},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {Cronbach's alpha,Internal consistency,Psychometrics,Reliability},
number = {3},
pages = {412--433},
pmid = {28557467},
title = {{Thanks coefficient alpha, We'll take it from here}},
volume = {23},
year = {2018}
}
@article{Trizano-Hermosilla2016,
abstract = {The Cronbach's alpha is the most widely used method for estimating internal consistency reliability. This procedure has proved very resistant to the passage of time, even if its limitations are well documented and although there are better options as omega coefficient or the different versions of glb, with obvious advantages especially for applied research in which the {\'{i}}tems differ in quality or have skewed distributions. In this paper, using Monte Carlo simulation, the performance of these reliability coefficients under a one-dimensional model is evaluated in terms of skewness and no tau-equivalence. The results show that omega coefficient is always better choice than alpha and in the presence of skew items is preferable to use omega and glb coefficients even in small samples.},
author = {Trizano-Hermosilla, Italo and Alvarado, Jes{\'{u}}s M.},
doi = {10.3389/fpsyg.2016.00769},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Alpha,Asymmetrical measures,Greatest lower bound,Omega,Reliability},
number = {MAY},
title = {{Best alternatives to Cronbach's alpha reliability in realistic conditions: Congeneric and asymmetrical measurements}},
volume = {7},
year = {2016}
}
@article{Hayes2020,
abstract = {Cronbach's alpha ($\alpha$) is a widely-used measure of reliability used to quantify the amount of random measurement error that exists in a sum score or average generated by a multi-item measurement scale. Yet methodologists have warned that $\alpha$ is not an optimal measure of reliability relative to its more general form, McDonald's omega ($\omega$). Among other reasons, that the computation of $\omega$ is not available as an option in many popular statistics programs and requires items loadings from a confirmatory factor analysis (CFA) have probably hindered more widespread adoption. After a bit of discussion of $\alpha$ versus $\omega$, we illustrate the computation of $\omega$ using two structural equation modeling programs (Mplus and AMOS) and the MBESS package for R. We then describe a macro for SPSS and SAS (OMEGA) that calculates $\omega$ in two ways without relying on the estimation of loadings or error variances using CFA. We show that it produces estimates of $\omega$ that are nearly identical to when using CFA-based estimates of item loadings and error variances. We also discuss the use of the OMEGA macro for certain forms of item analysis and brief form construction based on the removal of items from a longer scale.},
author = {Hayes, Andrew F. and Coutts, Jacob J.},
doi = {10.1080/19312458.2020.1718629},
issn = {19312466},
journal = {Communication Methods and Measures},
title = {{Use Omega Rather than Cronbach's Alpha for Estimating Reliability. But{\ldots}}},
year = {2020}
}
@article{Cronbach1951,
abstract = {A general formula ($\alpha$) of which a special case is the Kuder-Richardson coefficient of equivalence is shown to be the mean of all split-half coefficients resulting from different splittings of a test. $\alpha$ is therefore an estimate of the correlation between two random samples of items from a universe of items like those in the test. $\alpha$ is found to be an appropriate index of equivalence and, except for very short tests, of the first-factor concentration in the test. Tests divisible into distinct subtests should be so divided before using the formula. The index {\{}Mathematical expression{\}}, derived from $\alpha$, is shown to be an index of inter-item homogeneity. Comparison is made to the Guttman and Loevinger approaches. Parallel split coefficients are shown to be unnecessary for tests of common types. In designing tests, maximum interpretability of scores is obtained by increasing the first-factor concentration in any separately-scored subtest and avoiding substantial group-factor clusters within a subtest. Scalability is not a requisite. {\textcopyright} 1951 Psychometric Society.},
author = {Cronbach, Lee J.},
doi = {10.1007/BF02310555},
issn = {00333123},
journal = {Psychometrika},
number = {3},
pages = {297--334},
title = {{Coefficient alpha and the internal structure of tests}},
volume = {16},
year = {1951}
}
@inproceedings{Hwang2002,
abstract = {This study compared classical test theory (CTT) and item response theory (IRT). The behavior of the item and person statistics derived from these two measurement frameworks was examined analytically and empirically using a data set obtained from BILOG (R. Mislay and D. Block, 1997). The example was a 15-item test with a sample size of 600 examinees (eighth-grade level). The empirical findings indicate that the item and person statistics derived from the two measurement frameworks are quite comparable. The study used a specific characteristic of the test items. Different test score distributions for various item characteristics are recommended for future studies.},
author = {Hwang, Dae-Yeop},
booktitle = {Annual Meeting of Southwest Educational Research Association},
title = {{Classical test theory and item response theory: Analytical and empirical comparisons}},
year = {2002}
}
@article{Xitao1998,
abstract = {Despite theoretical differences between item response theory (IRT) and classical test theory (CTT), there is a lack of empirical knowledge about how, and to what extent, the IRT- and CTT-based item and person statistics behave differently. This study empirically examined the behaviors of the item and person statistics derived from these two measurement frameworks. The study focused on two issues: (a) What are the empirical relationships between IRT- and CTT-based item and person statistics? and (b) To what extent are the item statistics from IRT and those from CTT invariant across different participant samples? A large-scale statewide assessment database was used in the study. The findings indicate that the person and item statistics derived from the two measurement frameworks are quite comparable. The degree of invariance of item statistics across samples, usually considered as the theoretical superiority IRT models, also appeared to be similar for the two measurement frameworks},
author = {Xitao, Fan},
journal = {Educational and Psychological Measurement},
number = {June},
title = {{Item response theory and classical test theory : an empirical comparison of their item/peson statistics}},
volume = {58},
year = {1998}
}
@article{Erguven2014,
abstract = {Although it is considered an interdisciplinary subject, theory of measurement is mostly based on the mathematical foundations. In the last quarter of the 19th century, various psychometric approaches have been developed on the strength of those scientific foundations. Those theories allow users to analyze and determine relationship among individuals' latent abilities and their response patterns with respect to surveys , aptitude tests, examinations or other educational measurement tools. Besides, these models of measurement are enhanced to estimate model parameters on the strength of features of items. Item-person statistics are used for the educational measurement and assessment purposes mostly. In that context, Classical Test Theory (CTT) and Rasch model are focused and elaborated to evaluate School Olympiad Examination. The way how to implement both theories and how to interpret results of the analyses is presented in the study. Comparison of both theories is done. This comparison gives chance to determine similarities and differences between them. Using the IRTPRO program, some notable features of the items are described and discussed in detail. According to gender, item-person statistics/parameters are determined and interpreted. Behaviors of Item Characteristics Curves are elaborated with respect to both genders. Reliability of the test is defined with Cronbach's alpha in the whole test and with respect to genders. As a conclusion it is suggested that SOE (School Olympiad Examination) has a high reliability as an educational test, groups of female and male students handled almost all of the items similarly. CTT and Rasch model give similar information with respect to different genders in general. However, Rasch model gives better and more informative results in the ability determination and comparison.},
author = {Erg{\"{u}}ven, Mehtap},
journal = {Journal of Education},
keywords = {acteristic curve,classical test theory,difficulty index,discrimination index,item char -,item response theory,item-person statistics,rasch model,reliability},
number = {1},
pages = {33--38},
title = {{An empirical evaluation and comparison of Classical Test Theory and Rasch Model}},
volume = {3},
year = {2014}
}
@article{MacDonald2002,
abstract = {Despite the well-known theoretical advantages of item response theory (IRT) over classical test theory (CTT), research examining their empirical properties has failed to reveal consistent, demonstrable differences. Using Monte Carlo techniques with simulated test data, this study examined the behavior of item and person statistics obtained from these two measurement frameworks. The findings suggest IRT- and CTT-based item difficulty and person ability estimates were highly comparable, invariant, and accurate in the test conditions simulated. However, whereas item discrimination estimates based on IRT were accurate across most of the experimental conditions, CTT-based item discrimination estimates proved accurate under some conditions only. Implications of the results of this study for psychometric item analysis and item selection are discussed.},
author = {MacDonald, Paul and Paunonen, Sampo V.},
doi = {10.1177/0013164402238082},
issn = {00131644},
journal = {Educational and Psychological Measurement},
number = {6},
pages = {921--943},
title = {{A Monte Carlo comparison of item and person statistics based on item response theory versus classical test theory}},
volume = {62},
year = {2002}
}
@article{Fan1998,
abstract = {Despite theoretical differences between item response theory (IRT) and classical test theory (CTT), there is a lack of empirical knowledge about how, and to what extent, the IRT- and CTT-based item and person statistics behave differently. This study empirically examined the behaviors of the item and person statistics derived from these two measurement frameworks. The study focused on two issues: (a) What are the empirical relationships between IRT- and CTT-based item and person statistics? and (b) To what extent are the item statistics from IRT and those from CTT invariant across different participant samples? A large-scale statewide assessment database was used in the study. The findings indicate that the person and item statistics derived from the two measurement frameworks are quite comparable. The degree of invariance of item statistics across samples, usually considered as the theoretical superiority IRT models, also appeared to be similar for the two measurement frameworks.},
author = {Fan, Xitao},
doi = {10.1177/0013164498058003001},
issn = {00131644},
journal = {Educational and Psychological Measurement},
number = {3},
pages = {357--381},
title = {{Item response theory and classical test theory: An empirical comparison of their item/person statistics}},
volume = {58},
year = {1998}
}
@article{Thissen1989,
abstract = {It is not always convenient or appropriate to construct tests in which individual items are fungible. There are situations in which small clusters of items (testlets) are the units that are assembled to create a test. Using data from a test of reading comprehension constructed of four passages with several questions following each passage, we show that local independence fails at the level of the individual questions. The questions following each passage, however, constitute a testlet. We discuss the application to testlet scoring of some multiple‐category models originally developed for individual items, In the example examined, the concurrent validity of the testlet scoring equaled or exceeded that of individual‐item‐level scoring Copyright {\textcopyright} 1989, Wiley Blackwell. All rights reserved},
author = {Thissen, David and Steinberg, Lynne and Mooney, Jo Ann},
doi = {10.1111/j.1745-3984.1989.tb00331.x},
issn = {17453984},
journal = {Journal of Educational Measurement},
number = {3},
pages = {247--260},
title = {{Trace Lines for Testlets: A Use of Multiple‐Categorical‐Response Models}},
volume = {26},
year = {1989}
}
@article{Wardrop1987,
abstract = {Bentler (1980) Latent variable analysis},
author = {Wardrop, James L. and Loehlin, John C.},
doi = {10.2307/1165058},
issn = {03629791},
journal = {Journal of Educational Statistics},
number = {4},
pages = {410},
title = {{Latent Variable Models: An Introduction to Factor, Path, and Structural Analysis}},
volume = {12},
year = {1987}
}
@article{Likert1932,
abstract = {The project conceived in 1929 by Gardner Murphy and the writer aimed first to present a wide array of problems having to do with five major "attitude areas"—international relations, race relations, economic conflict, political conflict, and religion. The kind of questionnaire material falls into four classes: yes-no, multiple choice, propositions to be responded to by degrees of approval, and a series of brief newspaper narratives to be approved or disapproved in various degrees. The monograph aims to describe a technique rather than to give results. The appendix, covering ten pages, shows the method of constructing an attitude scale. A bibliography is also given.},
author = {Likert, R},
journal = {Archives of Psychology},
keywords = {m{\'{e}}thode},
pages = {44--53},
title = {{A technique for the measurement of attitudes}},
volume = {140},
year = {1932}
}
@book{Rasch1960,
abstract = {This monograph attempts a new approach for testing construction in psychology. A probabilistic model is developed for 3 different types of tests. Each model implies 2 types of parameters: a "difficulty" for each test or item and an "ability" for each person, independent of which set of tests or items has been employed. Both parameters are estimated from the data. Chapters 1-7 present the basic theory with a minimum of mathematics. Chapters 8-10 present in detail the mathematics underlying the models. From Psyc Abstracts 36:05:5HB84R. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
author = {Rasch, Georg},
booktitle = {Studies in mathematical psychology: I. Probabilistic models for some intelligence and attainment tests.},
pages = {xiii, 184--xiii, 184},
publisher = {Nielsen and Lydiche},
title = {{Studies in mathematical psychology: I. Probabilistic models for some intelligence and attainment tests.}},
year = {1960}
}
@article{Guttman1944,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.},
author = {Guttman, Louis},
doi = {10.2307/2086306},
issn = {00031224},
journal = {American Sociological Review},
number = {2},
pages = {139},
title = {{A Basis for Scaling Qualitative Data}},
volume = {9},
year = {1944}
}
@article{Andersson2021,
abstract = {Factor score regression has recently received growing interest as an alternative for structural equation modeling. However, many applications are left without guidance because of the focus on normally distributed outcomes in the literature. We perform a simulation study to examine how a selection of factor scoring methods compare when estimating regression coefficients in generalized linear factor score regression. The current study evaluates the regression method and the correlation-preserving method as well as two sum score methods in ordinary, logistic, and Poisson factor score regression. Our results show that scoring method performance can differ notably across the considered regression models. In addition, the results indicate that the choice of scoring method can substantially influence research conclusions. The regression method generally performs the best in terms of coefficient and standard error bias, accuracy, and empirical Type I error rates. Moreover, the regression method and the correlation-preserving method mostly outperform the sum score methods.},
author = {Andersson, Gustaf and Yang-Wallentin, Fan},
doi = {10.1177/0013164420975149},
issn = {15523888},
journal = {Educational and Psychological Measurement},
keywords = {GLM,Poisson regression,factor score indeterminacy,logistic regression,meta-analysis,sum score methods},
number = {4},
pages = {617--643},
title = {{Generalized Linear Factor Score Regression: A Comparison of Four Methods}},
volume = {81},
year = {2021}
}
@article{DiStefano2009,
abstract = {Following an exploratory factor analysis, factor scores may be computed and used in subsequent analyses. Factor scores are composite variables which provide information about an individual's placement on the factor(s). This article discusses popular methods to create factor scores under two different classes: refined and non-refined. Strengths and considerations of the various methods, and for using factor scores in general, are discussed. {\textcopyright} retained by the first or sole author.},
author = {DiStefano, Christine and Zhu, Min and M{\^{i}}ndrilǎ, Diana},
file = {:home/steven/Downloads/Understanding and Using Factor Scores Considerations for the App.pdf:pdf},
issn = {15317714},
journal = {Practical Assessment, Research and Evaluation},
number = {20},
title = {{Understanding and using factor scores: Considerations for the applied researcher}},
volume = {14},
year = {2009}
}
@article{Grice2001,
abstract = {A variety of methods for computing factor scores can be found in the psychological literature. These methods grew out of a historic debate regarding the indeterminate nature of the common factor model. Unfortunately, most researchers are unaware of the indeterminacy issue and the problems associated with a number of the factor scoring procedures. This article reviews the history and nature of factor score indeterminacy. Novel computer programs for assessing the degree of indeterminacy in a given analysis, as well as for computing and evaluating different types of factor scores, are then presented and demonstrated using data from the Wechsler Intelligence Scale for Children - Third Edition. It is argued that factor score indeterminacy should be routinely assessed and reported as part of any exploratory factor analysis and that factor scores should be thoroughly evaluated before they are reported or used in subsequent statistical analyses.},
author = {Grice, James W.},
doi = {10.1037/1082-989x.6.4.430},
issn = {1082989X},
journal = {Psychological Methods},
number = {3},
pages = {430--450},
pmid = {11778682},
title = {{Computing and evaluating factor scores}},
volume = {6},
year = {2001}
}
@article{Grice2001a,
abstract = {Six different methods of computing factor scores were investigated in a simulation study. Population scores created from oblique factor patterns selected from the psychological literature served as the bases for the simulations, and the stability of the different methods was assessed through cross-validation in a subject-sampling model. Results from 5 evaluative criteria indicated that a simplified, unit-weighting procedure based on the factor score coefficients was generally superior to several unit-weighting procedures based on the pattern or structure coefficients. This simplified method of computing factor scores also compared favorably with an exact-weighting scheme based on the full factor score coefficient matrix. Results are discussed with regard to their potential impact on current practice, and several recommendations are offered.},
author = {Grice, James W.},
doi = {10.1037/1082-989x.6.1.67},
issn = {1082-989X},
journal = {Psychological Methods},
number = {1},
pages = {67--83},
title = {{A comparison of factor scores under conditions of factor obliquity.}},
volume = {6},
year = {2001}
}
@article{Preacher2003,
abstract = {Proper use of exploratory factor analysis (EFA) requires the researcher to make a series of careful decisions. Despite attempts by Floyd and Widaman (1995), Fabrigar, Wegener, MacCallum, and Strahan (1999), and others to elucidate critical issues involved in these ...},
author = {Preacher, Kristopher J. and MacCallum, Robert C.},
doi = {10.1207/s15328031us0201_02},
issn = {1534-844X},
journal = {Understanding Statistics},
number = {1},
pages = {13--43},
title = {{Repairing Tom Swift's Electric Factor Analysis Machine}},
volume = {2},
year = {2003}
}
@article{Green2015,
abstract = {Traditional parallel analysis (T-PA) estimates the number of factors by sequentially comparing sample eigenvalues with eigenvalues for randomly generated data. Revised parallel analysis (R-PA) sequentially compares the kth eigenvalue for sample data to the kth eigenvalue for generated data sets, conditioned on k− 1 underlying factors. T-PA and R-PA are conceptualized as stepwise hypothesis-testing procedures and, thus, are alternatives to sequential likelihood ratio test (LRT) methods. We assessed the accuracy of T-PA, R-PA, and LRT methods using a Monte Carlo approach. Although no method was uniformly more accurate across all 180 conditions, the PA approaches outperformed LRT methods overall. Relative to T-PA, R-PA tended to perform better within the framework of hypothesis testing and to evidence greater accuracy in conditions with higher factor loadings.},
author = {Green, Samuel B. and Thompson, Marilyn S. and Levy, Roy and Lo, Wen Juo},
doi = {10.1177/0013164414546566},
issn = {15523888},
journal = {Educational and Psychological Measurement},
keywords = {factor analysis,parallel analysis,revised parallel analysis},
number = {3},
pages = {428--457},
title = {{Type I and Type II Error Rates and Overall Accuracy of the Revised Parallel Analysis Method for Determining the Number of Factors}},
volume = {75},
year = {2015}
}
@article{Hayashi2007,
abstract = {In the exploratory factor analysis, when the number of factors exceeds the true number of factors, the likelihood ratio test statistic no longer follows the chi-square distribution due to a problem of rank deficiency and nonidentifiability of model parameters. As a result, decisions regarding the number of factors may be incorrect. Several researchers have pointed out this phenomenon, but it is not well known among applied researchers who use exploratory factor analysis. We demonstrate that overfactoring is one cause for the well-known fact that the likelihood ratio test tends to find too many factors. Copyright {\textcopyright} 2007, Lawrence Erlbaum Associates, Inc.},
author = {Hayashi, Kentaro and Bentler, Peter M. and Yuan, Ke Hai},
doi = {10.1080/10705510701301891},
issn = {10705511},
journal = {Structural Equation Modeling},
number = {3},
pages = {505--526},
title = {{On the likelihood ratio test for the number of factors in exploratory factor analysis}},
volume = {14},
year = {2007}
}
@article{Green2012,
abstract = {A number of psychometricians have argued for the use of parallel analysis to determine the number of factors. However, parallel analysis must be viewed at best as a heuristic approach rather than a mathematically rigorous one. The authors suggest a revision to parallel analysis that could improve its accuracy. A Monte Carlo study is conducted to compare revised and traditional parallel analysis approaches. Five dimensions are manipulated in the study: number of observations, number of factors, number of measured variables, size of the factor loadings, and degree of correlation between factors. Based on the results, the revised parallel analysis method, using principal axis factoring and the 95th percentile eigenvalue rule, offers promise. {\textcopyright} The Author(s) 2012.},
author = {Green, Samuel B. and Levy, Roy and Thompson, Marilyn S. and Lu, Min and Lo, Wen Juo},
doi = {10.1177/0013164411422252},
issn = {15523888},
journal = {Educational and Psychological Measurement},
keywords = {factor analysis,number of factors,parallel analysis},
number = {3},
pages = {357--374},
title = {{A Proposed Solution to the Problem With Using Completely Random Data to Assess the Number of Factors With Parallel Analysis}},
volume = {72},
year = {2012}
}
@article{Ruscio2012,
abstract = {Exploratory factor analysis (EFA) is used routinely in the development and validation of assessment instruments. One of the most significant challenges when one is performing EFA is determining how many factors to retain. Parallel analysis (PA) is an effective stopping rule that compares the eigenvalues of randomly generated data with those for the actual data. PA takes into account sampling error, and at present it is widely considered the best available method. We introduce a variant of PA that goes even further by reproducing the observed correlation matrix rather than generating random data. Comparison data (CD) with known factorial structure are first generated using 1 factor, and then the number of factors is increased until the reproduction of the observed eigenvalues fails to improve significantly. We evaluated the performance of PA, CD with known factorial structure, and 7 other techniques in a simulation study spanning a wide range of challenging data conditions. In terms of accuracy and robustness across data conditions, the CD technique outperformed all other methods, including a nontrivial superiority to PA. We provide program code to implement the CD technique, which requires no more specialized knowledge or skills than performing PA. {\textcopyright} 2011 American Psychological Association.},
author = {Ruscio, John and Roche, Brendan},
doi = {10.1037/a0025697},
file = {:home/steven/Downloads/Ruscio-Roche-2012-PA-Factor-Analysis.pdf:pdf},
issn = {10403590},
journal = {Psychological Assessment},
keywords = {Comparison data,Exploratory factor analysis,Kaiser criterion,Number of factors,Parallel analysis},
number = {2},
pages = {282--292},
pmid = {21966933},
title = {{Determining the number of factors to retain in an exploratory factor analysis using comparison data of known factorial structure}},
volume = {24},
year = {2012}
}
@article{Cho2009,
abstract = {The purpose of this study was to investigate the application of the parallel analysis (PA) method for choosing the number of factors in component analysis for situations in which data are dichotomous or ordinal. Although polychoric correlations are sometimes used as input for component analyses, the random data matrices generated for use in PA typically consist of Pearson correlations. In this study, the authors matched the type of random data matrix to the type of input matrix. Analyses were conducted on both polychoric and Pearson correlation matrices, and random matrices of the same type (polychoric or Pearson) were generated for the PA procedure. PA based on random Pearson correlations was found to perform at least as well as PA based on random polychoric correlations, for nearly all of the conditions studied. {\textcopyright} 2009 SAGE Publications.},
author = {Cho, Sun Joo and Li, Feiming and Bandalos, Deborah},
doi = {10.1177/0013164409332229},
issn = {15523888},
journal = {Educational and Psychological Measurement},
keywords = {Parallel analysis,Pearson correlation,Polychoric correlation},
number = {5},
pages = {748--759},
title = {{Accuracy of the parallel analysis procedure with polychoric correlations}},
volume = {69},
year = {2009}
}
@article{Joreskog1990,
author = {J{\"{o}}reskog, Karl G.},
doi = {10.1007/BF00152012},
issn = {00335177},
journal = {Quality and Quantity},
number = {4},
pages = {387--404},
title = {{New developments in LISREL: analysis of ordinal variables using polychoric correlations and weighted least squares}},
volume = {24},
year = {1990}
}
@article{Muthen1984,
abstract = {A structural equation model is proposed with a generalized measurement part, allowing for dichotomous and ordered categorical variables (indicators) in addition to continuous ones. A computationally feasible three-stage estimator is proposed for any combination of observed variable types. This approach provides large-sample chi-square tests of fit and standard errors of estimates for situations not previously covered. Two multiple-indicator modeling examples are given. One is a simultaneous analysis of two groups with a structural equation model underlying skewed Likert variables. The second is a longitudinal model with a structural model for multivariate probit regressions. {\textcopyright} 1984 The Psychometric Society.},
author = {Muth{\'{e}}n, Bengt},
doi = {10.1007/BF02294210},
issn = {00333123},
journal = {Psychometrika},
keywords = {generalized least-squares,polychoric correlations,probit regressions,weight matrix},
number = {1},
pages = {115--132},
title = {{A general structural equation model with dichotomous, ordered categorical, and continuous latent variable indicators}},
volume = {49},
year = {1984}
}
@article{Shapiro2002,
abstract = {For any given number of factors, Minimum Rank Factor Analysis yields optimal communalities for an observed covariance matrix in the sense that the unexplained common variance with that number of factors is minimized, subject to the constraint that both the diagonal matrix of unique variances and the observed covariance matrix minus that diagonal matrix are positive semidefinite. As a result, it becomes possible to distinguish the explained common variance from the total common variance. The percentage of explained common variance is similar in meaning to the percentage of explained observed variance in Principal Component Analysis, but typically the former is much closer to 100 than the latter. So far, no statistical theory of MRFA has been developed. The present paper is a first start. It yields closed-form expressions for the asymptotic bias of the explained common variance, or, more precisely, of the unexplained common variance, under the assumption of multivariate normality. Also, the asymptotic variance of this bias is derived, and also the asymptotic covariance matrix of the unique variances that define a MRFA solution. The presented asymptotic statistical inference is based on a recently developed perturbation theory of semidefinite programming. A numerical example is also offered to demonstrate the accuracy of the expressions.},
author = {Shapiro, Alexander and {Ten Berge}, Jos M.F.},
doi = {10.1007/BF02294710},
issn = {00333123},
journal = {Psychometrika},
keywords = {Asymptotic bias,Asymptotic normality,Communalities,Explained common variance,Factor analysis,Large samples asymptotics,Proper solutions,Semidefinite programming},
number = {1},
pages = {79--94},
title = {{Statistical inference of minimum rank factor analysis}},
volume = {67},
year = {2002}
}
@article{Steger2006,
abstract = {Structural validity has long been regarded as critical to psychological measurement. However, in practical application, issues central to structural validity are often neglected. The purpose of this study was to illustrate the importance of several crucial choices that face researchers attempting to evaluate the structure of data from a given scale. In this study, I compared the structural solutions derived via principal components analysis and principal axis factoring using eigenvalues, scree plots, and traditional parallel analyses with data from the Purpose in Life Test (Crumbaugh {\&} Maholick, 1964). I discuss the importance of structural validity for overall construct validity and the importance of carefully considering factor analytic methodology. I provide recommendations for uses of factor analysis. Copyright {\textcopyright} 2006, Lawrence Erlbaum Associates, Inc.},
author = {Steger, Michael F.},
doi = {10.1207/s15327752jpa8603_03},
issn = {00223891},
journal = {Journal of Personality Assessment},
number = {3},
pages = {263--272},
pmid = {16740111},
title = {{An illustration of issues in factor extraction and identification of dimensionality in psychological assessment data}},
volume = {86},
year = {2006}
}
@article{Timmerman2011,
abstract = {Parallel analysis (PA) is an often-recommended approach for assessment of the dimensionality of a variable set. PA is known in different variants, which may yield different dimensionality indications. In this article, the authors considered the most appropriate PA procedure to assess the number of common factors underlying ordered polytomously scored variables. They proposed minimum rank factor analysis (MRFA) as an extraction method, rather than the currently applied principal component analysis (PCA) and principal axes factoring. A simulation study, based on data with major and minor factors, showed that all procedures consistently point at the number of major common factors. A polychoric-based PA slightly outperformed a Pearson-based PA, but convergence problems may hamper its empirical application. In empirical practice, PA-MRFA with a 95{\%} threshold based on polychoric correlations or, in case of nonconvergence, Pearson correlations with mean thresholds appear to be a good choice for identification of the number of common factors. PA-MRFA is a common-factor-based method and performed best in the simulation experiment. PA based on PCA with a 95{\%} threshold is second best, as this method showed good performances in the empirically relevant conditions of the simulation experiment. {\textcopyright} 2011 American Psychological Association.},
author = {Timmerman, Marieke E. and Lorenzo-Seva, Urbano},
doi = {10.1037/a0023353},
file = {:home/steven/Downloads/timmermanlorenzoseva{\_}PM{\_}2011.pdf:pdf},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {Exploratory factor analysis,Minimum rank factor analysis,Number of common factors,Polychoric correlation,Principal component analysis,Tetrachoric correlation},
number = {2},
pages = {209--220},
pmid = {21500916},
title = {{Dimensionality assessment of ordered polytomous items with parallel analysis}},
volume = {16},
year = {2011}
}
@article{Turner1998,
abstract = {Selecting the correct number of factors to retain in a factor analysis is a crucial step in developing psychometric tools or developing theories. The present study assessed the accuracy of parallel analysis, a technique in which the observed eigenvalues are compared to eigenvalues from simulated data in which no real factors are present. Study 1 investigated the effect of the presence of one real factor on the size of subsequent noise eigenvalues. The size of real factors and the sample size were manipulated. Study 2 examined the effect that the pattern of structure coefficients and continuousness of the variables have on the size of real and noise eigenvalues. Study 3 compared the results of Studies 1 and 2 to actual psychometric data. These examples illustrate the importance of modeling the data more closely when parallel analysis is used to determine the number of real factors.},
author = {Turner, Nigel E.},
doi = {10.1177/0013164498058004001},
issn = {00131644},
journal = {Educational and Psychological Measurement},
number = {4},
pages = {541--568},
title = {{The effect of common variance and structure pattern on random data eigenvalues: Implications for the accuracy of parallel analysis}},
volume = {58},
year = {1998}
}
@incollection{Velicer2000,
abstract = {Several years ago, in a paper entitled “Construct validity and personality assessment” Jackson (1979) argued that the greatest impact of the construct approach to psychological measurement had been felt in the area of personality. In describing his own work, and that of others, he demonstrated the value of this approach for understanding personality and for personality scale construction. As in personality assessment work, much of the research conducted by organizational psychologists relies on questionnaire measures developed to assess complex constructs. It is interesting to note that, around the same time as Jackson's comments, Schwab (1980, p.34) lamented that “organizational behavior has suffered” from a lack of attention to construct validation. In the intervening 20 years, attention to the construct and its measurement has increased in organizational research. Nonetheless, one could not argue that the construct approach has gained center stage in this area or that concerns about conceptual and measurement adequacy in organizational research have been ameliorated dramatically. Indeed, in recent discussions of questionnaire measures developed for use in organizations, Hinkin (1995, 1998) suggested that concerns about adequate construct measurement represented “perhaps the greatest difficulty in conducting survey research” (Hinkin, 1995, p. 967).},
author = {Velicer, Wayne F. and Eaton, Cheryl A. and Fava, Joseph L.},
booktitle = {Problems and Solutions in Human Assessment},
doi = {10.1007/978-1-4615-4397-8_3},
file = {:home/steven/Downloads/A.18VelicerEatonFava-DeterminingtheNumberofFactors-2000-Chapter.pdf:pdf},
pages = {41--71},
title = {{Construct Explication through Factor or Component Analysis: A Review and Evaluation of Alternative Procedures for Determining the Number of Factors or Components}},
year = {2000}
}
@article{Peres-Neto2005,
abstract = {Principal component analysis is one of the most widely applied tools in order to summarize common patterns of variation among variables. Several studies have investigated the ability of individual methods, or compared the performance of a number of methods, in determining the number of components describing common variance of simulated data sets. We identify a number of shortcomings related to these studies and conduct an extensive simulation study where we compare a larger number of rules available and develop some new methods. In total we compare 20 stopping rules and propose a two-step approach that appears to be highly effective. First, a Bartlett's test is used to test the significance of the first principal component, indicating whether or not at least two variables share common variation in the entire data set. If significant, a number of different rules can be applied to estimate the number of non-trivial components to be retained. However, the relative merits of these methods depend on whether data contain strongly correlated or uncorrelated variables. We also estimate the number of non-trivial components for a number of field data sets so that we can evaluate the applicability of our conclusions based on simulated data. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Peres-Neto, Pedro R. and Jackson, Donald A. and Somers, Keith M.},
doi = {10.1016/j.csda.2004.06.015},
file = {:home/steven/Downloads/10.1.1.723.8937.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Monte Carlo simulations,Principal component analysis,Stopping rules},
number = {4},
pages = {974--997},
title = {{How many principal components? stopping rules for determining the number of non-trivial axes revisited}},
volume = {49},
year = {2005}
}
@article{Bentler1990,
abstract = {Normed and nonnormed fit indexes are frequently used as adjuncts to chi-square statistics for evaluating the fit of a structural model. A drawback of existing indexes is that they estimate no known population parameters. A new coefficient is proposed to summarize the relative reduction in the noncentrality parameters of two nested models. Two estimators of the coefficient yield new normed (CFI) and nonnormed (FI) fit indexes. CFI avoids the underestimation of fit often noted in small samples for Bentler and Bonett's (1980) normed fit index (NFI). FI is a linear function of Bentler and Bonett's non-normed fit index (NNFI) that avoids the extreme underestimation and overestimation often found in NNFI. Asymptotically, CFI, FI, NFI, and a new index developed by Bollen are equivalent measures of comparative fit, whereas NNFI measures relative fit by comparing noncentrality per degree of freedom. All of the indexes are generalized to permit use of Wald and Lagrange multiplier statistics. An example illustrates the behavior of these indexes under conditions of correct specification and misspecification. The new fit indexes perform very well at all sample sizes.},
author = {Bentler, P. M.},
doi = {10.1037/0033-2909.107.2.238},
issn = {00332909},
journal = {Psychological Bulletin},
number = {2},
pages = {238--246},
pmid = {2320703},
title = {{Comparative fit indexes in structural models}},
volume = {107},
year = {1990}
}
@article{Lorenzo-Seva2011,
abstract = {A common problem in exploratory factor analysis is how many factors need to be extracted from a particular data set. We propose a new method for selecting the number of major common factors: the Hull method, which aims to find a model with an optimal balance between model fit and number of parameters. We examine the performance of the method in an extensive simulation study in which the simulated data are based on major and minor factors. The study compares the method with four other methods such as parallel analysis and the minimum average partial test, which were selected because they have been proven to perform well and/or they are frequently used in applied research. The Hull method outperformed all four methods at recovering the correct number of major factors. Its usefulness was further illustrated by its assessment of the dimensionality of the Five-Factor Personality Inventory (Hendriks, Hofstee, {\&} De Raad, 1999). This inventory has 100 items, and the typical methods for assessing dimensionality prove to be useless: the large number of factors they suggest has no theoretical justification. The Hull method, however, suggested retaining the number of factors that the theoretical background to the inventory actually proposes.},
author = {Lorenzo-Seva, Urbano and Timmerman, Marieke E. and Kiers, Henk A.L.},
doi = {10.1080/00273171.2011.564527},
file = {:home/steven/Downloads/The Hull Method for Selecting the Number of Common Factors.pdf:pdf},
issn = {00273171},
journal = {Multivariate Behavioral Research},
number = {2},
pages = {340--364},
title = {{The hull method for selecting the number of common factors}},
volume = {46},
year = {2011}
}
@article{Ceulemans2006,
abstract = {Several three-mode principal component models can be considered for the modelling of three-way, three-mode data, including the Candecomp/Parafac, Tucker3, Tucker2, and Tucker1 models. The following question then may be raised: given a specific data set, which of these models should be selected, and at what complexity (i.e. with how many components)? We address this question by proposing a numerical model selection heuristic based on a convex hull. Simulation results show that this heuristic performs almost perfectly, except for Tucker3 data arrays with at least one small mode and a relatively large amount of error. {\textcopyright} 2006 The British Psychological Society.},
author = {Ceulemans, Eva and Kiers, Henk A.L.},
doi = {10.1348/000711005X64817},
file = {:home/steven/Downloads/000711005X64817.pdf:pdf},
issn = {00071102},
journal = {British Journal of Mathematical and Statistical Psychology},
number = {1},
pages = {133--150},
pmid = {16709283},
title = {{Selecting among three-mode principal component models of different types and complexities: A numerical convex hull based method}},
volume = {59},
year = {2006}
}
@article{Ledesma2007,
abstract = {Parallel Analysis is a Monte Carlo simulation technique that aids researchers in determining the number of factors to retain in Principal Component and Exploratory Factor Analysis. This method provides a superior alternative to other techniques that are commonly used for the same purpose, such as the Scree test or the Kaiser's eigenvalue-greater-than-one rule. Nevertheless, Parallel Analysis is not well known among researchers, in part because it is not included as an analysis option in the most popular statistical packages. This paper describes and illustrates how to apply Parallel Analysis with an easy-to-use computer program called ViSta-PARAN. ViSta-PARAN is a user-friendly application that can compute and interpret Parallel Analysis. Its user interface is fully graphic and includes a dialog box to specify parameters, and specialized graphics to visualize the analysis output. {\textcopyright} retained by the first or sole author.},
author = {Ledesma, Rub{\'{e}}n Daniel and Valero-Mora, Pedro},
doi = {10.7275/WJNC-NM63},
issn = {15317714},
journal = {Practical Assessment, Research and Evaluation},
number = {2},
title = {{Determining the number of factors to retain in EFA: An easy-to-use computer program for carrying out Parallel Analysis}},
volume = {12},
year = {2007}
}
@article{Horn1979,
abstract = {It is demonstrated that Cattell's scree test and Bartlett's chi-square test for the number of factors are both based on the same rationale, so the former reflects statistical (subject sampling) variability and the latter usually involves psychometric (variable sampling) influences. If the alpha-level (implicit in the scree test) is set the same, the two tests should lead to the same conclusions. Analyses with some examples suggest that if the alpha-level for the Bartlett test is set (explicitly) in the neighborhood of 0003 for sample Ns of 100 to 150, the results from applications of this test will indicate approximately the same number of factors as estimated on the basis of a scree test determined on a much larger (N ≅ 600) sample. Used in this way, the Bartlett test may yield fairly good “population” estimates of the number of factors. Relationships between the Bartlett test, hence the scree test, and tests for a common factor model and for the significance of a correlation matrix are explicated.},
author = {Horn, John L. and Engstrom, Robert},
doi = {10.1207/s15327906mbr1403_1},
issn = {15327906},
journal = {Multivariate Behavioral Research},
number = {3},
pages = {283--300},
title = {{Cattell's scree test in relation to Bartlett's Chi-square test and other observations on the number of factors problem}},
volume = {14},
year = {1979}
}
@article{Zwick1986b,
abstract = {The performance of five methods for determining the number of components to retain (Horn's parallel analysis, Velicer's minimum average partial [MAP], Cattell's scree test, Bartlett's chi-square test, and Kaiser's eigenvalue greater than 1.0 rule) was investigated across seven systematically varied conditions (sample size, number of variables, number of components, component saturation, equal or unequal numbers of variables per component, and the presence or absence of unique and complex variables). We generated five sample correlation matrices at each of two sample sizes from the 48 known population correlation matrices representing six levels of component pattern complexity. The performance of the parallel analysis and MAP methods was generally the best across all situations. The scree test was generally accurate but variable. Bartlett's chi-square test was less accurate and more variable than the scree test. Kaiser's method tended to severely overestimate the number of components. We discuss recommendations concerning the conditions under which each of the methods are accurate, along with the most effective and useful methods combinations. {\textcopyright} 1986 American Psychological Association.},
author = {Zwick, William R. and Velicer, Wayne F.},
doi = {10.1037/0033-2909.99.3.432},
issn = {00332909},
journal = {Psychological Bulletin},
number = {3},
pages = {432--442},
title = {{Comparison of Five Rules for Determining the Number of Components to Retain}},
volume = {99},
year = {1986}
}
@article{Crawford1979,
abstract = {The inter-rater reliability of Cattell's scree and Linn's mean square ratio test of the number of factors was studied. Sample correlation matrices were generated from a population correlation matrix by means of standard Monte Carlo procedures such that there were 100 samples based on each of 3 sample sizes. Each matrix was factored and the scree test and the mean square ratio test were each applied by five raters. For both tests, the inter-rater reliabilities were very low. These results suggest that inexperienced factor analysts should be wary of these tests of the number of factors.},
author = {Crawford, Charles B. and Koopman, Penny},
doi = {10.2466/pms.1979.49.1.223},
issn = {0031-5125},
journal = {Perceptual and Motor Skills},
number = {1},
pages = {223--226},
title = {{Note: Inter-Rater Reliability of Scree Test and Mean Square Ratio Test of Number of Factors}},
volume = {49},
year = {1979}
}
@article{Tucker1969,
abstract = {In order to study the effectiveness of factor analytic methods, a procedure was developed for computing simulated correlation matrices which are more similar to real data correlation matrices than are those matrices computed from the factor analysis structural model. In the present investigation, three methods of factor extraction were studied as applied to 54 simulated correlation matrices which varied in proportion of variance derived from a major factor domain, number of factors in the major domain, and closeness of the simulation procedure to the factor analysis structural model. While the factor extraction methods differed little from one another in quality of results for matrices more dissimilar to the factor analytic model, major differences in quality of results were associated with fewer factors in the major domain, higher proportion of variance from the major domain, and closeness of the simulation procedure to the factor analysis structural model. {\textcopyright} 1969 Psychometric Society.},
author = {Tucker, Ledyard R. and Koopman, Raymond F. and Linn, Robert L.},
doi = {10.1007/BF02290601},
issn = {18600980},
journal = {Psychometrika},
number = {4},
pages = {421--459},
title = {{Evaluation of factor analytic research procedures by means of simulated correlation matrices}},
volume = {34},
year = {1969}
}
@article{Revelle1979,
abstract = {A new procedure for determining the optimal number of interpretable factors to extract from a correlation matrix is introduced and compared to more conventional procedures. The new method evaluates the magnitude of the Very Simple Structure index of goodness of fit for factor solutions of increasing rank. The number of factors which maximizes the VSS criterion is taken as being the optimal number of factors to extract. Thirty-two artificial and two real data sets are used in order to compare this procedure with such methods as maximum likelihood, the eigenvalue greater than 1.0 rule, and comparison of the observed eigenvalues with those expected from random data.},
author = {Revelle, William and Rocklin, Thomas},
doi = {10.1207/s15327906mbr1404_2},
issn = {15327906},
journal = {Multivariate Behavioral Research},
number = {4},
pages = {403--414},
title = {{Very simple structure: An alternative procedure for estimating the optimal number of interpretable factors}},
volume = {14},
year = {1979}
}
@article{Velicer1990,
abstract = {Should one do a component analysis or a factor analysis? The choice is not obvious, because the two broad classes of procedures serve a similar purpose, and share many important mathematical characteristics. Despite many textbooks describing common factor analysis as the preferred procedure, principal component analysis has been the most widely applied. Here we summarize relevant information for the prospective factor/component analyst. First, we discuss the key algebraic similarities and differences. Next, we analyze a number of theoretical and practical issues. The more practical aspects include: the degree of numeric similarity between solutions from the two methods, some common rules for the number of factors to be retained, effects resulting from overextraction, problems with improper solutions, and comparisons in computational efficiency. Finally, we review some broader theoretical issues: the factor indeterminacy issue, the differences between exploratory and confirmatory procedures, and the issue of latent versus manifest variables.},
author = {Velicer, Wayne F. and Jackson, Douglas N.},
doi = {10.1207/s15327906mbr2501_1},
issn = {15327906},
journal = {Multivariate Behavioral Research},
number = {1},
pages = {1--28},
title = {{Component Analysis versus Common Factor Analysis: Some Issues in Selecting an Appropriate Procedure}},
volume = {25},
year = {1990}
}
@article{Lee1979,
abstract = {A currently popular procedure in empirical factor analytic studies is to use unities in the main diagonal as communality estimates, extract all factors with eigenvalues of 1.0 or higher, and rotate these factors by varimax. This procedure and others are applied to several previously published correlation matrices and some artificial matrices. This procedure results in the retention of too many factors, unrealistic elevation of the amount of common factor variance analyzed, and distortions in the conclusions drawn from the factor analytic investigation. Ways of avoiding these difficulties; are discussed.},
author = {Lee, Howard B. and Comrey, Andrew L.},
doi = {10.1207/s15327906mbr1403_2},
issn = {15327906},
journal = {Multivariate Behavioral Research},
number = {3},
pages = {301--321},
title = {{Distortions in a commonly used factor analytic procedure}},
volume = {14},
year = {1979}
}
@article{Browne1968,
abstract = {It is shown that the lower bounds for the number of common factors, established by Guttman [1954] and modified by Kaiser [1961], cannot decrease as the number of observed variates is increased. The result implies that the lower bounds cannot become weaker if the number of observed variates is increased and the number of factors remains constant. {\textcopyright} 1968 Psychometric Society.},
author = {Browne, Michael W.},
doi = {10.1007/BF02290155},
issn = {00333123},
journal = {Psychometrika},
number = {2},
pages = {233--236},
pmid = {5242173},
title = {{A note on lower bounds for the number of common factors}},
volume = {33},
year = {1968}
}
@article{Zwick1982,
abstract = {The performance of four rules for determining the number of components to retain (Kaiser's eigenvalue greater than unity, Cattell's SCREE, Bartlett's test, and Velicer's MAP) was investigated across four systematically varied factors (sample size, number of variables, number of components, and component saturation). Ten sample correlation matrices were generated from each of 48 known population correlation matrices representing the combinations of conditions. The performance of the SCREE and MAP rules was generally the best across all situations. Bartlett's test was generally adequate except when the number of variables was close to the sample size. Kaiser's rule tended to severely overestimate the number of components.},
author = {Zwick, William R. and Velicer, Wayne F.},
doi = {10.1207/s15327906mbr1702_5},
issn = {15327906},
journal = {Multivariate Behavioral Research},
number = {2},
title = {{Factors influencing four rules for determining the number of components to retain}},
volume = {17},
year = {1982}
}
@article{Hakstian1982,
abstract = {Issues related to the decision of the number of factors to retain in factor analysis are identified, and three widely-used decision rules—the Kaiser-Guttman, scree, and likelihood ratio tests—are isolated for empirical study. Using two differing structural models and incorporating a number of relevant independent variables (such as number of variables, ratio of number of factors to number of variables, variable communality levels, and factorial complexity), the authors simulated 144 population data sets and, then, from these, 288 sample data sets, each with a precisely known (or incorporated) number of factors. The Kaiser-Guttman and scree rules were applied to the population data in Part I of the study, and all three rules were applied to the sample data sets in Part II. Overall trends and intenactive results, in terms of the independent variables examined, are discussed in detail, and methods are presented for assessing the quality of the number-of-factors indicated by a particular rule.},
author = {Hakstian, A. Ralph and Cattell, Raymond B.},
doi = {10.1207/s15327906mbr1702_3},
issn = {15327906},
journal = {Multivariate Behavioral Research},
number = {2},
pages = {193--219},
title = {{The behavior of number-of-factors rules with simulated data}},
volume = {17},
year = {1982}
}
@article{Yeomans1982,
abstract = {This article evaluates the performance of the Guttman-Kaiser criterion in determining the number of significant components or factors in a correlation matrix. CR - Copyright {\&}{\#}169; 1982 Royal Statistical Society},
author = {Yeomans, Keith A. and Golder, Paul A.},
doi = {10.2307/2987988},
issn = {00390526},
journal = {The Statistician},
number = {3},
pages = {221},
title = {{The Guttman-Kaiser Criterion as a Predictor of the Number of Common Factors}},
volume = {31},
year = {1982}
}
@article{Linn1968,
author = {Linn, Robert L.},
doi = {10.1007/BF02289675},
issn = {00333123},
journal = {Psychometrika},
number = {1},
pages = {37--71},
pmid = {5239570},
title = {{A monte carlo approach to the number of factors problem}},
volume = {33},
year = {1968}
}
@article{Finch2020,
abstract = {Exploratory factor analysis (EFA) is widely used by researchers in the social sciences to characterize the latent structure underlying a set of observed indicator variables. One of the primary issues that must be resolved when conducting an EFA is determination of the number of factors to retain. There exist a large number of statistical tools designed to address this question, with none being universally optimal across applications. Recently, researchers have investigated the use of model fit indices that are commonly used in the conduct of confirmatory factor analysis to determine the number of factors to retain in EFA. These results have yielded mixed results, appearing to be effective when used in conjunction with normally distributed indicators, but not being as effective for categorical indicators. The purpose of this simulation study was to compare the performance of difference values for several fit indices as a method for identifying the optimal number of factors to retain in an EFA, with parallel analysis, which is one of the most reliable such extant methods. Results of the simulation demonstrated that the use of fit index difference values outperformed parallel analysis for categorical indicators, and for normally distributed indicators when factor loadings were small. Implications of these findings are discussed.},
author = {Finch, W. Holmes},
doi = {10.1177/0013164419865769},
issn = {15523888},
journal = {Educational and Psychological Measurement},
keywords = {CFI,RMSEA,comparative fit index,exploratory factor analysis,model fit statistics,parallel analysis,root mean square error of approximation},
number = {2},
pages = {217--241},
title = {{Using Fit Statistic Differences to Determine the Optimal Number of Factors to Retain in an Exploratory Factor Analysis}},
volume = {80},
year = {2020}
}
@incollection{Finch2020a,
author = {Finch, W. Holmes},
booktitle = {Exploratory Factor Analysis},
doi = {10.4135/9781544339900.n8},
month = {dec},
pages = {71--92},
publisher = {SAGE Publications, Inc.},
title = {{Methods for Determining the Number of Factors to Retain in Exploratory Factor Analysis}},
year = {2020}
}
@article{Braeken2017,
abstract = {In exploratory factor analysis (EFA), most popular methods for dimensionality assessment such as the screeplot, the Kaiser criterion, or - the current gold standard - parallel analysis, are based on eigenvalues of the correlation matrix. To further understanding and development of factor retention methods, results on population and sample eigenvalue distributions are introduced based on random matrix theory and Monte Carlo simulations. These results are used to develop a new factor retention method, the Empirical Kaiser Criterion. The performance of the Empirical Kaiser Criterion and parallel analysis is examined in typical research settings, with multiple scales that are desired to be relatively short, but still reliable. Theoretical and simulation results illustrate that the new Empirical Kaiser Criterion performs as well as parallel analysis in typical research settings with uncorrelated scales, but much better when scales are both correlated and short. We conclude that the Empirical Kaiser Criterion is a powerful and promising factor retention method, because it is based on distribution theory of eigenvalues, shows good performance, is easily visualized and computed, and is useful for power analysis and sample size planning for EFA.},
author = {Braeken, Johan and {Van Assen}, Marcel A.L.M.},
doi = {10.1037/met0000074},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {Kaiser criterion,exploratory factor analysis,parallel analysis},
number = {3},
pages = {450--466},
pmid = {27031883},
title = {{An empirical Kaiser criterion.}},
volume = {22},
year = {2017}
}
@article{Auerswald2019,
abstract = {Exploratory factor analyses are commonly used to determine the underlying factors of multiple observed variables. Many criteria have been suggested to determine how many factors should be retained. In this study, we present an extensive Monte Carlo simulation to investigate the performance of extraction criteria under varying sample sizes, numbers of indicators per factor, loading magnitudes, underlying multivariate distributions of observed variables, as well as how the performance of the extraction criteria are influenced by the presence of cross-loadings and minor factors for unidimensional, orthogonal, and correlated factor models. We compared several variants of traditional parallel analysis (PA), the Kaiser-Guttman Criterion, and sequential x2 model tests (SMT) with 4 recently suggested methods: revised PA, comparison data (CD), the Hull method, and the Empirical Kaiser Criterion (EKC). No single extraction criterion performed best for every factor model. In unidimensional and orthogonal models, traditional PA, EKC, and Hull consistently displayed high hit rates even in small samples. Models with correlated factors were more challenging, where CD and SMT outperformed other methods, especially for shorter scales. Whereas the presence of cross-loadings generally increased accuracy, non-normality had virtually no effect on most criteria. We suggest researchers use a combination of SMT and either Hull, the EKC, or traditional PA, because the number of factors was almost always correctly retrieved if those methods converged. When the results of this combination rule are inconclusive, traditional PA, CD, and the EKC performed comparatively well. However, disagreement also suggests that factors will be harder to detect, increasing sample size requirements to N ≥ 500.},
author = {Auerswald, Max and Moshagen, Morten},
doi = {10.1037/met0000200},
file = {:home/steven/Downloads/document.pdf:pdf},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {Factor analysis,Monte Carlo simulation,Non-normality,Number of factors},
number = {4},
pages = {468--491},
pmid = {30667242},
title = {{How to determine the number of factors to retain in exploratory factor analysis: A comparison of extraction methods under realistic conditions}},
volume = {24},
year = {2019}
}
@book{Harman1970,
address = {Chicago, Ill.},
author = {Harman, Harry H},
isbn = {0226316513 9780226316512},
language = {English},
publisher = {The University of Chicago Press},
title = {{Modern factor analysis.}},
year = {1970}
}
@book{Tabachnick2007,
address = {Boston},
author = {Tabachnick, Barbara G and Fidell, Linda S},
isbn = {0205459382 9780205459384 0205465250 9780205465255},
language = {English},
publisher = {Pearson/Allyn {\&} Bacon},
title = {{Using multivariate statistics}},
year = {2007}
}
@incollection{Timmerman2017,
abstract = {This chapter focuses on formal criteria to assess the dimensionality for exploratory factor modelling with the aim to facilitate the selection of a proper criterion in empirical practice. It introduces the different foundations that underlie the various criteria and provides an overview of currently available formal criteria, which we selected on the basis of their popularity in empirical practice and/or proven effectiveness. The chapter successively reviews principal component analysis (PCA)-based methods and common factor analysis (CFA)-based methods to assess the number of common factors. To assess the number of factors underlying an empirical data set, the chapter suggests some strategies. It explains the finding in many studies that the Kaiser criterion clearly yields inaccurate indications of the number of PCs and common factors, mostly indicating too many factors. Minimum average partial (MAP) performances in indicating the number of major factors deteriorated when the unique variances increased, with no clear tendency to over- or underindicate the number of factors.},
author = {Timmerman, Marieke E. and Lorenzo-Seva, Urbano and Ceulemans, Eva},
booktitle = {The Wiley Handbook of Psychometric Testing: A Multidisciplinary Reference on Survey, Scale and Test Development},
doi = {10.1002/9781118489772.ch11},
file = {:home/steven/Downloads/9781118489772.ch11.pdf:pdf},
isbn = {9781118489772},
keywords = {Common factor analysis,Dimensionality Assessment Methods,Kaiser Criterion,Minimum average partial performances,Principal component analysis},
pages = {305--324},
title = {{The number of factors problem}},
volume = {1-2},
year = {2017}
}
@article{Kaiser1958,
abstract = {An analytic criterion for rotation is defined. The scientific advantage of analytic criteria over subjective (graphical) rotational procedures is discussed. Carroll's criterion and the quartimax criterion are briefly reviewed; the varimax criterion is outlined in detail and contrasted both logically and numerically with the quartimax criterion. It is shown that the normal varimax solution probably coincides closely to the application of the principle of simple structure. However, it is proposed that the ultimate criterion of a rotational procedure is factorial invariance, not simple structure-although the two notions appear to be highly related. The normal varimax criterion is shown to be a two-dimensional generalization of the classic Spearman case, i.e., it shows perfect factorial invariance for two pure clusters. An example is given of the invariance of a normal varimax solution for more than two factors. The oblique normal varimax criterion is stated. A computational outline for the orthogonal normal varimax is appended. {\textcopyright} 1958 Psychometric Society.},
author = {Kaiser, Henry F.},
doi = {10.1007/BF02289233},
issn = {00333123},
journal = {Psychometrika},
number = {3},
pages = {187--200},
title = {{The varimax criterion for analytic rotation in factor analysis}},
volume = {23},
year = {1958}
}
@article{Kaiser1974a,
abstract = {A desirable property of the equamax criterion for analytic rotation in factor analysis is presented.},
author = {Kaiser, Henry F. and Kaiser, Henry F.},
doi = {10.1207/s15327906mbr0904_9},
issn = {15327906},
journal = {Multivariate Behavioral Research},
number = {4},
pages = {501--503},
title = {{A note on the equamax criterion}},
volume = {9},
year = {1974}
}
@article{Hendrickson1964,
abstract = {A new method for analytical rotation to oblique simple structure is described. Orthogonal simple structure is achieved by means of any of several existing rotation methods and this is then transformed into an oblique solution. 1964 The British Psychological Society},
author = {Hendrickson, Alan E. and White, Paul Owen},
doi = {10.1111/j.2044-8317.1964.tb00244.x},
issn = {20448317},
journal = {British Journal of Statistical Psychology},
number = {1},
pages = {65--70},
title = {{PROMAX: A quick method for rotation to oblique simple structure}},
volume = {17},
year = {1964}
}
@article{Kiers1994,
abstract = {Factor analysis and principal component analysis are usually followed by simple structure rotations of the loadings. These rotations optimize a certain criterion (e.g., varimax, oblimin), designed to measure the degree of simple structure of the pattern matrix. Simple structure can be considered optimal if a (usually large) number of pattern elements is exactly zero. In the present paper, a class of oblique rotation procedures is proposed to rotate a pattern matrix such that it optimally resembles a matrix which has an exact simple pattern. It is demonstrated that this method can recover relatively complex simple structures where other well-known simple structure rotation techniques fail. {\textcopyright} 1994 The Psychometric Society.},
author = {Kiers, Henk A.L.},
doi = {10.1007/BF02294392},
issn = {00333123},
journal = {Psychometrika},
keywords = {Promax,component analysis,factor analysis},
number = {4},
pages = {567--579},
title = {{Simplimax: Oblique rotation to an optimal target with simple structure}},
volume = {59},
year = {1994}
}
@article{Kaiser1974,
abstract = {An index of factorial simplicity, employing the quartimax transformational criteria of Carroll, Wrigley and Neuhaus, and Saunders, is developed. This index is both for each row separately and for a factor pattern matrix as a whole. The index varies between zero and one. The problem of calibrating the index is discussed. {\textcopyright} 1974 Psychometric Society.},
author = {Kaiser, Henry F.},
doi = {10.1007/BF02291575},
issn = {00333123},
journal = {Psychometrika},
number = {1},
pages = {31--36},
title = {{An index of factorial simplicity}},
volume = {39},
year = {1974}
}
@article{Bentler1977,
abstract = {A scale-invariant index of factorial simplicity is proposed as a summary statistic for principal components and factor analysis. The index ranges from zero to one, and attains its maximum when all variables are simple rather than factorially complex. A factor scale-free oblique transformation method is developed to maximize the index. In addition, a new orthogonal rotation procedure is developed. These factor transformation methods are implemented using rapidly convergent computer programs. Observed results indicate that the procedures produce meaningfully simple factor pattern solutions. {\textcopyright} 1977 Psychometric Society.},
author = {Bentler, P. M.},
doi = {10.1007/BF02294054},
issn = {00333123},
journal = {Psychometrika},
keywords = {multivariate analysis,oblique transformation,orthogonal rotation},
number = {2},
pages = {277--295},
title = {{Factor simplicity index and transformations}},
volume = {42},
year = {1977}
}
@article{Lorenzo-Seva2003,
abstract = {We propose an index for assessing the degree of factor simplicity in the context of principal components and exploratory factor analysis. The new index, which is called Loading Simplicity, is based on the idea that the communality of each variable should be related to few components, or factors, so that the loadings in each variable are either zero or as far from zero as possible. This index does not depend on the scale of the factors, and its maximum and minimum are only related to the degree of simplicity in the loading matrix. The aim of the index is to enable the degree of simplicity in loading matrices to be compared.},
author = {Lorenzo-Seva, Urbano},
doi = {10.1007/BF02296652},
issn = {00333123},
journal = {Psychometrika},
keywords = {Exploratory factor analysis,Factor rotation,Factor simplicity,Loading matrix comparison,Simple structure},
number = {1},
pages = {49--60},
title = {{A factor simplicity index}},
volume = {68},
year = {2003}
}
@article{Dombrowski2013,
abstract = {Development of the Woodcock-Johnson (3rd ed.; WJ-III; Woodcock, McGrew {\&} Mather, 2001a) was guided in part by Carroll's (1993) 3-stratum theory of cognitive abilities and based on confirmatory factor analysis (CFA), even though Carroll used exploratory factor analysis (EFA) to derive his theory. Using CFA, McGrew and Woodcock (2001) found a 9-factor model across all age ranges. To determine if the 9-factor structure holds for the full WJ-III battery, we applied currently recognized best practices in EFA to 2 school-aged 42-subtest correlation matrices (ages 9-13 and 14-19 years). Six factors emerged at the 9-13 age range, while 5 factors were indicated at the 14-19 age range. The resulting 1st-order factors displayed patterns of both convergence with and divergence from the WJ-III results presented in the Technical Manual. These results also revealed a robust manifestation of general intelligence (g) that dwarfed the variance attributed to the lower order factors. It is surprising that this study represents the first time the WJ-III full battery was subjected to EFA analyses given the instrument's significant use by practitioners and that it served as the initial evidentiary basis for Cattell-Horn-Carroll (CHC) theory. The lack of confirmation of CFA results with EFA methods in the current study permits questioning of the structure of the WJ-III and its relationship with CHC theory. Additional independent, structural analyses are clearly indicated for the WJ-III full test battery before we can be confident in its structure. {\textcopyright} 2013 American Psychological Association.},
author = {Dombrowski, Stefan C. and Watkins, Marley W.},
doi = {10.1037/a0031335},
issn = {1939134X},
journal = {Psychological Assessment},
keywords = {Cattell-Horn-Carroll theory,Exploratory factor analysis,General intelligence,Higher order factor analysis,Schmid- Leiman orthogonalization},
number = {2},
pages = {442--455},
pmid = {23356683},
title = {{Exploratory and higher order factor analysis of the WJ-III full test battery: A school-aged analysis}},
volume = {25},
year = {2013}
}
@article{Guilford1981,
author = {Guilford, J P},
doi = {10.1207/s15327906mbr1604_1},
issn = {0027-3171},
journal = {Multivariate Behavioral Research},
month = {oct},
number = {4},
pages = {411--435},
publisher = {Routledge},
title = {{Higher-Order Structure-Of-Intellect Abilities}},
volume = {16},
year = {1981}
}
@article{Costello2005,
author = {Costello, Anna B. and Osborne, Jason},
doi = {10.7275/jyj1-4868},
journal = {Practical Assessment, Research, and Evaluation},
pages = {1--10},
title = {{Best practices in exploratory factor analysis: four recommendations for getting the most from your analysis recommendations for getting the most from your analysis}},
volume = {10},
year = {2005}
}
@article{Ford1986,
abstract = {Although factor analysis has been a major contributing factor in advancing psychological research, a systematic assessment of how it has been applied is lacking. For this review we examined the Journal of Applied Psychology, Organizational Behavior and Human Performance, and Personnel Psychology over a ten-year period (1975?1984) and located 152 studies that employed factor analysis. We then analyzed the choices made by the researchers concerning factor model, retention criteria, rotation, interpretation of factors and other issues relevant to factor analysis. The results indicate that choices made by researchers have generally been poor and that reporting practices have not allowed for informed review, cumulation of results, or replicability. A comparison of results by time interval (1975?1979; 1980?1984) revealed minimal differences in choices made or the quality of reporting practices. Suggestions for improving the use of factor analysis and the reporting of results are presented.},
annote = {https://doi.org/10.1111/j.1744-6570.1986.tb00583.x},
author = {Ford, J Kevin and MacCallum, Robert C and Tait, Marianne},
doi = {10.1111/j.1744-6570.1986.tb00583.x},
issn = {0031-5826},
journal = {Personnel Psychology},
month = {jun},
number = {2},
pages = {291--314},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{The application of exploratory factor analysis in applied psychology: a critival review and analysis}},
volume = {39},
year = {1986}
}
@book{Thurstone1969,
address = {Chicago},
author = {Thurstone, Louis Leon},
language = {English},
publisher = {The University of Chicago Press},
title = {{Multiple-factor analysis : a development and expansion of The vectors of mind}},
year = {1969}
}
@book{Thurstone1947,
address = {Chicago},
author = {Thurstone, Louis Leon},
language = {English},
publisher = {University of Chicago Press},
title = {{Multiple - factor analysis.}},
year = {1947}
}
@misc{Thurstone2004,
abstract = {A definitive statement of the position and techniques of the generalized program of factor analysis at date. The argument proceeds in terms of the application of matrix theory to the evaluation of correlation tables, and a mathematical introduction is therefore given to acquaint the reader with the groundwork. Other chapters include the following: the factor problem; the fundamental factor theorem; the centroid method; the principal axes (including Hotelling's special case); the special case of rank one (Spearman's original attack); primary traits; isolation of primary factors; the positive manifold; orthogonal transformations; and the appraisal of abilities. There are appendixes on the concrete steps involved in calculations by the centroid method, a method of finding the roots of a polynomial, and a method for extracting square root with a mechanical calculator. (PsycINFO Database Record (c) 2004 APA, all rights reserved).},
address = {Chicago, IL},
author = {Thurstone, Louis Leon},
language = {English},
publisher = {University of Chicago Press},
title = {{The vectors of mind multiple-factor analysis for the isolation of primary traits.}},
year = {2004}
}
@article{Asparouhov2009,
annote = {doi: 10.1080/10705510903008204},
author = {Asparouhov, Tihomir and Muth{\'{e}}n, Bengt},
doi = {10.1080/10705510903008204},
issn = {1070-5511},
journal = {Structural Equation Modeling: A Multidisciplinary Journal},
month = {jul},
number = {3},
pages = {397--438},
publisher = {Routledge},
title = {{Exploratory Structural Equation Modeling}},
url = {https://doi.org/10.1080/10705510903008204},
volume = {16},
year = {2009}
}
@book{Thurstone1935,
abstract = {Exposition of the theories and techniques of factor analysis; mathematics difficult. Harvard Book List (edited) 1938 {\#}174 (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
address = {University of Chicago Press},
author = {Thurstone, Louis Leon},
booktitle = {The Vectors of Mind.},
publisher = {Chicago},
title = {{The Vectors of Mind.}},
year = {1935}
}
@article{Sass2010,
abstract = {Exploratory factor analysis (EFA) is a commonly used statistical technique for examining the relationships between variables (e.g., items) and the factors (e.g., latent traits) they depict. There are several decisions that must be made when using EFA, with one of the more important being choice of the rotation criterion. This selection can be arduous given the numerous rotation criteria available and the lack of research/literature that compares their function and utility. Historically, researchers have chosen rotation criteria based on whether or not factors are correlated and have failed to consider other important aspects of their data. This study reviews several rotation criteria, demonstrates how they may perform with different factor pattern structures, and highlights for researchers subtle but important differences between each rotation criterion. The choice of rotation criterion is critical to ensure researchers make informed decisions as to when different rotation criteria may or may not be appropriate. The results suggest that depending on the rotation criterion selected and the complexity of the factor pattern matrix, the interpretation of the interfactor correlations and factor pattern loadings can vary substantially. Implications and future directions are discussed.},
author = {Sass, Daniel A. and Schmitt, Thomas A.},
doi = {10.1080/00273170903504810},
issn = {00273171},
journal = {Multivariate Behavioral Research},
number = {1},
pages = {73--103},
title = {{A comparative investigation of rotation criteria within exploratory factor analysis}},
volume = {45},
year = {2010}
}
@article{Loo1979,
abstract = {Examined the current trends in rotation procedures used in factor analyses in clinical research. It was found that orthogonal rotations, usually varimax procedure, were overwhelmingly the rotational choice. It was suggested that in many analyses, investigators in this area of research may be imposing orthogonality on data that might better be interpreted as oblique factors. Guidelines were presented for consideration in the selection and reporting of rotational procedures in clinical research. Copyright {\textcopyright} 1979 Wiley Periodicals, Inc., A Wiley Company},
author = {Loo, Robert},
doi = {10.1002/1097-4679(197910)35:4<762::AID-JCLP2270350414>3.0.CO;2-M},
issn = {10974679},
journal = {Journal of Clinical Psychology},
number = {4},
pages = {762--765},
pmid = {512001},
title = {{The orthogonal rotation of factors in clinical research: A critical note}},
volume = {35},
year = {1979}
}
@article{Jennrich1966,
abstract = {Existing analytic oblique rotation schemes proceed by optimizing a simplicity function applied to the reference structure. This article suggests optimizing a simplicity function applied to primary loadings directly. The feasibility of the suggestion is demonstrated using the quartimin criterion. An algorithm to implement the optimization is derived and the existence of an admissible solution proved. Practical comparisons with the biquartimin method are made using Thurstone's Box Problem and Holzinger and Swineford's Twenty-Four Psychological Tests Problem. {\textcopyright} 1966 Psychometric Society.},
author = {Jennrich, R. I. and Sampson, P. F.},
doi = {10.1007/BF02289465},
issn = {00333123},
journal = {Psychometrika},
number = {3},
pages = {313--323},
pmid = {5221128},
title = {{Rotation for simple loadings}},
volume = {31},
year = {1966}
}
@article{Carroll1953,
abstract = {It is proposed that a satisfactory criterion for an approximation to simple structure is the minimization of the sums of cross-products (across factors) of squares of factor loadings. This criterion is completely analytical and yields a unique solution; it requires no plotting, nor any decisions as to the clustering of variables into subgroups. The equations involved appear to be capable only of iterative solution; for more than three or four factors the computations become extremely laborious but may be feasible for high-speed electronic equipment. Either orthogonal or oblique solutions may be achieved. For illustrations, the Johnson-Reynolds study of "flow" and "selection" factors and the Thurstone box problem are reanalyzed. The presence of factorially complex tests produces a type of hyperplanar fit which the investigator may desire to adjust by graphical rotations; the smaller the number of such tests, the closer the criterion comes to approximating simple structure. {\textcopyright} 1953 Psychometric Society.},
author = {Carroll, John B.},
doi = {10.1007/BF02289025},
issn = {00333123},
journal = {Psychometrika},
number = {1},
pages = {23--38},
title = {{An analytical solution for approximating simple structure in factor analysis}},
volume = {18},
year = {1953}
}
@article{Tucker1955,
abstract = {Requirements for an objective definition of simple structure are investigated and a number of proposed objective criteria are evaluated. A distinction is drawn between exploratory factorial studies and confirmatory factorial studies, with the conclusion drawn that objective definition of simple structure depends on study design as well as on objective criteria. A proposed definition of simple structure is described in terms of linear constellations. This definition lacks only a statistical test to compare with possible chance results. A computational procedure is also described for searching for linear constellations. This procedure is very laborious and might best be accomplished on high-speed automatic computers. There is no guarantee that the procedure will find all linear constellations, but it probably would yield satisfactory results for well-designed studies. {\textcopyright} 1955 Psychometric Society.},
author = {Tucker, Ledyard R.},
doi = {10.1007/BF02289018},
issn = {00333123},
journal = {Psychometrika},
number = {3},
pages = {209--225},
title = {{The objective definition of simple structure in linear factor analysis}},
volume = {20},
year = {1955}
}
@article{Schonemann1972,
abstract = {Some relations between maximum likelihood factor analysis and factor indeterminacy are discussed. Bounds are derived for the minimum average correlation between equivalent sets of correlated factors which depend on the latent roots of the factor intercorrelation matrix $\psi$. Empirical examples are presented to illustrate some of the theory and indicate the extent to which it can be expected to be relevant in practice. {\textcopyright} 1972 Psychometric Society.},
author = {Sch{\"{o}}nemann, Peter H. and Wang, Ming Mei},
doi = {10.1007/BF02291413},
issn = {00333123},
journal = {Psychometrika},
number = {1},
pages = {61--91},
title = {{Some new results on factor indeterminacy}},
volume = {37},
year = {1972}
}
@article{SchOnemann1978,
abstract = {A partition of the vector space of all deviation score vectors for fixed sample size N is used to show that the (indeterminate) factors of the factor model can always be constructed so as to predict any criterion perfectly, including all those that are entirely uncorrected with the observed variables. {\textcopyright} 1978, The Psychonomic Society, Inc.. All rights reserved.},
author = {Sch{\"{o}}nemann, Peter H. and Steiger, James H.},
doi = {10.3758/BF03329685},
issn = {00905054},
journal = {Bulletin of the Psychonomic Society},
number = {4},
pages = {287--290},
title = {{On the validity of indeterminate factor scores}},
volume = {12},
year = {1978}
}
@inproceedings{Saltelli2002,
abstract = {We review briefly some examples that would support an extended role for quantitative sensitivity analysis in the context of model-based analysis (Section 1). We then review what features a quantitative sensitivity analysis needs to have to play such a role (Section 2). The methods that meet these requirements are described in Section 3; an example is provided in Section 4. Some pointers to further research are set out in Section 5.},
author = {Saltelli, Andrea},
booktitle = {Risk Analysis},
doi = {10.1111/0272-4332.00040},
issn = {02724332},
keywords = {Assessment of importance,Computational models,Quantitative sensitivity analysis,Risk analysis,Uncertainty analysis},
number = {3},
pages = {579--590},
pmid = {12088235},
title = {{Sensitivity analysis for importance assessment}},
volume = {22},
year = {2002}
}
@article{DeWinter2012,
abstract = {Principal axis factoring (PAF) and maximum likelihood factor analysis (MLFA) are two of the most popular estimation methods in exploratory factor analysis. It is known that PAF is better able to recover weak factors and that the maximum likelihood estimator is asymptotically efficient. However, there is almost no evidence regarding which method should be preferred for different types of factor patterns and sample sizes. Simulations were conducted to investigate factor recovery by PAF and MLFA for distortions of ideal simple structure and sample sizes between 25 and 5000. Results showed that PAF is preferred for population solutions with few indicators per factor and for overextraction. MLFA outperformed PAF in cases of unequal loadings within factors and for underextraction. It was further shown that PAF and MLFA do not always converge with increasing sample size. The simulation findings were confirmed by an empirical study as well as by a classic plasmode, Thurstone's box problem. The present results are of practical value for factor analysts.},
author = {de Winter, J. C.F. and Dodou, D.},
doi = {10.1080/02664763.2011.610445},
issn = {02664763},
journal = {Journal of Applied Statistics},
keywords = {empirical data,exploratory factor analysis,maximum likelihood factor analysis,parameter estimation,plasmode,principal axis factoring,simulations},
number = {4},
pages = {695--710},
title = {{Factor recovery by principal axis factoring and maximum likelihood factor analysis as a function of factor pattern and sample size}},
volume = {39},
year = {2012}
}
@article{Jackson2001,
abstract = {Anumber of authors have proposed that determining an adequate sample size in structural equation modeling can be aided by considering the number of parameters to be estimated. This study directly investigates this assumption in the context of maximum likelihood confirmatory factor analysis. The findings support previous research on the effect of sample size, measured-variable reliability, and the number of measured variables per factor. However, no practically significant effect was found for the number of observations per estimated parameter. {\textcopyright} 2001, Lawrence Erlbaum Associates, Inc.},
author = {Jackson, Dennis L.},
doi = {10.1207/S15328007SEM0802_3},
issn = {10705511},
journal = {Structural Equation Modeling},
number = {2},
pages = {205--223},
title = {{Sample size and number of parameter estimates in maximum likelihood confirmatory factor analysis: A Monte Carlo investigation}},
volume = {8},
year = {2001}
}
@book{MacCallum2012,
abstract = {Factor analysis is one of the success stories of statistics in the social sciences. The reason for its wide appeal is that it provides a way to investigate latent variables, the fundamental traits and concepts in the study of individual differences. Because of its importance, a conference was held to mark the centennial of the publication of Charles Spearman's seminal 1904 article which introduced the major elements of this invaluable statistical tool. This book evolved from that conference. It provides a retrospective look at major issues and developments as well as a prospective view of future directions in factor analysis and related methods. In so doing, it demonstrates how and why factor analysis is considered to be one of the methodological pillars of behavioral research. Featuring an outstanding collection of contributors, this volume offers unique insights on factor analysis and its related methods. Several chapters have a clear historical perspective, while others present new ideas along with historical summaries. In addition, the book reviews some of the extensions of factor analysis to such techniques as latent growth curve models, models for categorical data, and structural equation models. Factor Analysis at 100 will appeal to graduate students and researchers in the behavioral, social, health, and biological sciences who use this technique in their research. A basic knowledge of factor analysis is required and a working knowledge of linear algebra is helpful.},
address = {London},
author = {Cudeck, Robert and MacCallum, Robert C.},
booktitle = {Factor Analysis at 100: Historical Developments and Future Directions},
doi = {10.4324/9780203936764},
isbn = {9780203936764},
pages = {1--386},
publisher = {Taylor and Francis},
title = {{Factor analysis at 100: Historical developments and future directions}},
year = {2012}
}
@article{SantosSilva2011,
abstract = {In this article, we identify and illustrate some shortcomings of the poisson command in Stata. Specifically, we point out that the command fails to check for the existence of the estimates, and we show that it is very sensitive to numerical problems. While these are serious problems that may prevent users from obtaining estimates or may even produce spurious and misleading results, we show that the informed user often has simple workarounds available for addressing these problems. {\textcopyright} 2011 StataCorp LP.},
author = {{Santos Silva}, J. M.C. and Tenreyro, Silvana},
doi = {10.1177/1536867x1101100203},
issn = {1536867X},
journal = {Stata Journal},
keywords = {Collinearity,Complete separation,Numerical problems,Perfect prediction,Poisson regression,Ppml,References,St0225},
number = {2},
pages = {207--212},
title = {{Poisson: Some convergence issues}},
volume = {11},
year = {2011}
}
@book{Brown2015,
address = {New York, N.Y},
author = {Brown, Timothy A},
isbn = {9781462515363 1462515363},
language = {English},
publisher = {Guilford Press},
title = {{Confirmatory factor analysis for applied research}},
year = {2015}
}
@article{Joreskog1978,
abstract = {A general approach to the analysis of covariance structures is considered, in which the variances and covariances or correlations of the observed variables are directly expressed in terms of the parameters of interest. The statistical problems of identification, estimation and testing of such covariance or correlation structures are discussed. Several different types of covariance structures are considered as special cases of the general model. These include models for sets of congeneric tests, models for confirmatory and exploratory factor analysis, models for estimation of variance and covariance components, regression models with measurement errors, path analysis models, simplex and circumplex models. Many of the different types of covariance structures are illustrated by means of real data. {\textcopyright} 1978 Psychometric Society.},
author = {J{\"{o}}reskog, Karl G.},
doi = {10.1007/BF02293808},
issn = {00333123},
journal = {Psychometrika},
keywords = {circumplex,covariance structure analysis,factor analysis,path analysis,simplex,variance components},
number = {4},
pages = {443--477},
title = {{Structural analysis of covariance and correlation matrices}},
volume = {43},
year = {1978}
}
@article{Harris1964,
author = {Harris, Chester W.},
doi = {10.1177/001316446402400202},
issn = {15523888},
journal = {Educational and Psychological Measurement},
number = {2},
pages = {193--206},
title = {{Some recent developments in factor analysis}},
volume = {24},
year = {1964}
}
@article{Cattell1965,
author = {Cattell, Raymond B. and Gorsuch, Richard L.},
doi = {10.1080/00224545.1965.9922260},
issn = {19401183},
journal = {Journal of Social Psychology},
number = {1},
pages = {77--96},
pmid = {5827925},
title = {{The Definition and Measurement of National Morale and Morality}},
volume = {67},
year = {1965}
}
@book{Osborne2014,
abstract = {"Best Practices in Exploratory Factor Analysis (EFA) is a practitioner-oriented look at this popular and often misunderstood statistical technique. We avoid formulas and matrix algebra, instead focusing on evidence-based best practices so you can focus on getting the most from your data."--Back cover.},
author = {Osborne, Jason W},
isbn = {9781500594343 1500594342},
language = {English},
title = {{Best practices in exploratory factor analysis}},
year = {2014}
}
@article{Yong2013,
abstract = {The following paper discusses exploratory factor analysis and gives an overview of the statistical technique and how it is used in various research designs and applications. A basic outline of how the technique works and its criteria, including its main assumptions are discussed as well as when it should be used. Mathematical theories are explored to enlighten students on how exploratory factor analysis works, an example of how to run an exploratory factor analysis on SPSS is given, and finally a section on how to write up the results is provided. This will allow readers to develop a better understanding of when to employ factor analysis and how to interpret the tables and graphs in the output. The broad purpose of factor analysis is to summarize data so that relationships and patterns can be easily interpreted and understood. It is normally used to regroup variables into a limited set of clusters based on shared variance. Hence, it helps to isolate constructs and concepts.},
author = {Yong, An Gie and Pearce, Sean},
doi = {10.20982/tqmp.09.2.p079},
issn = {1913-4126},
journal = {Tutorials in Quantitative Methods for Psychology},
number = {2},
pages = {79--94},
title = {{A Beginner's Guide to Factor Analysis: Focusing on Exploratory Factor Analysis}},
volume = {9},
year = {2013}
}
@incollection{Revelle2021,
abstract = {What is psychometrics? In physical science a first essential step in the direction of learning any subject is to find principles of numerical reckoning and methods for practicably measuring some quality connected with it. I often say that when you can measure what ... $\backslash$n},
author = {Revelle, William},
booktitle = {An Introduction to Psychometric Theory with Applications in R},
chapter = {6-7},
issn = {00222992},
pages = {145--239},
publisher = {Online publication},
title = {{Constructs, Components, and Factor models and Reliability}},
url = {https://personality-project.org/r/book/},
year = {2021}
}
@misc{Thapa2020,
author = {Thapa, Deependra K. and Visentin, Denis C. and Hunt, Glenn E. and Watson, Roger and Cleary, Michelle},
booktitle = {Journal of Advanced Nursing},
doi = {10.1111/jan.14311},
issn = {13652648},
number = {6},
pages = {1285--1288},
pmid = {32020658},
title = {{Being honest with causal language in writing for publication}},
volume = {76},
year = {2020}
}
@misc{Alavi2020,
author = {Alavi, Mousa and Visentin, Denis C. and Thapa, Deependra K. and Hunt, Glenn E. and Watson, Roger and Cleary, Michelle},
booktitle = {Journal of Advanced Nursing},
doi = {10.1111/jan.14377},
issn = {13652648},
title = {{Exploratory factor analysis and principal component analysis in clinical studies: Which one should you use?}},
year = {2020}
}
@book{Gorsuch2015,
address = {New York, NY},
author = {Gorsuch, Richard L},
isbn = {9781138831988 1138831980 9781138831995 1138831999},
language = {English},
publisher = {Routledge},
title = {{Factor analysis}},
year = {2015}
}
@article{Kaiser1960,
author = {Kaiser, Henry F.},
doi = {10.1177/001316446002000116},
issn = {15523888},
journal = {Educational and Psychological Measurement},
number = {1},
pages = {141--151},
title = {{The Application of Electronic Computers to Factor Analysis}},
volume = {20},
year = {1960}
}
@article{Patil2008,
abstract = {Other things being equal, a theory with fewer constructs is preferable over others. In exploratory factor analysis, a common method used in theory development, the most popular factor retention criterion used in marketing is the eigenvalue greater than one rule. Its use often results in over extraction, which leads to the development of less than parsimonious theories. Even the use of confirmatory factor analysis fails to detect the presence of these superfluous constructs. Although several more accurate criteria exist, they are not discussed in major marketing research texts, journals, and popular statistical software packages. In this paper, we appraise popular factor retention practices in marketing, demonstrate how they may lead to the development of inefficient theories, draw attention to a number of resources for choosing appropriate retention criteria, and develop an easy-to-use Web-based engine to effortlessly implement one such method, parallel analysis. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
author = {Patil, Vivek H. and Singh, Surendra N. and Mishra, Sanjay and {Todd Donavan}, D.},
doi = {10.1016/j.jbusres.2007.05.008},
issn = {01482963},
journal = {Journal of Business Research},
keywords = {Eigenvalue greater than one,Factor analysis,Parallel analysis,Theory development},
month = {feb},
number = {2},
pages = {162--170},
publisher = {Elsevier},
title = {{Efficient theory development and factor retention criteria: Abandon the 'eigenvalue greater than one' criterion}},
volume = {61},
year = {2008}
}
@article{Zwick1986,
abstract = {The performance of five methods for determining the number of components to retain (Horn's parallel analysis, Velicer's minimum average partial [MAP], Cattell's scree test, Bartlett's chi-square test, and Kaiser's eigenvalue greater than 1.0 rule) was investigated across seven systematically varied conditions (sample size, number of variables, number of components, component saturation, equal or unequal numbers of variables per component, and the presence or absence of unique and complex variables). We generated five sample correlation matrices at each of two sample sizes from the 48 known population correlation matrices representing six levels of component pattern complexity. The performance of the parallel analysis and MAP methods was generally the best across all situations. The scree test was generally accurate but variable. Bartlett's chi-square test was less accurate and more variable than the scree test. Kaiser's method tended to severely overestimate the number of components. We discuss recommendations concerning the conditions under which each of the methods are accurate, along with the most effective and useful methods combinations. {\textcopyright} 1986 American Psychological Association.},
author = {Zwick, William R. and Velicer, Wayne F.},
doi = {10.1037/0033-2909.99.3.432},
issn = {00332909},
journal = {Psychological Bulletin},
number = {3},
pages = {432--442},
title = {{Comparison of Five Rules for Determining the Number of Components to Retain}},
volume = {99},
year = {1986}
}
@article{Fava1996,
abstract = {The consequences of underextracting factors and components within and between the methods of maximum likelihood factor analysis (MLFA) and principal components analysis (PCA) were examined. Computer-simulated data sets represented a range of pattern structures. Manipulated conditions included component (factor) structure coefficients (aij = .8, .6, and .4), sample size (N = 75, 150, 225, and 450), and variable-to-component (factor) ratio (p:m = 6:1 and 4:1). The principal components score and the Anderson and Rubin factor score estimate were calculated for both the correct patterns and the incorrect (underextracted) patterns. In Study 1, underextraction led to substantial degradation of scores within both methods, but the component score degraded less rapidly. Score degradation was related to the number of original components (factors). In Study 2, between-method comparisons indicated very high similarity for baseline score patterns, but dissimilarity occurred with underextraction.},
author = {Fava, Joseph L. and Velicer, Wayne F.},
doi = {10.1177/0013164496056006001},
issn = {00131644},
journal = {Educational and Psychological Measurement},
number = {6},
pages = {907--929},
title = {{The effects of underextraction in factor and component analyses}},
volume = {56},
year = {1996}
}
@article{Wood1996,
abstract = {The effects of under- and overextraction on principal axis factor analysis with varimax rotation were examined in 2 Monte Carlo studies involving 6,420 factor analyses. It was found that (a) when underextraction occurs, the estimated factors are likely to contain considerable error; (b) when overextraction occurs, the estimated loadings for true factors usually contain substantially less error than in the case of underextraction; and (c) overextraction can result in factor splitting when a general factor is present and there are no unique variables in the data set. The authors recommend that factor analysts (a) use effective methods to estimate the number of factors; (b) avoid underextraction, even at the risk of overextraction; and (c) include randomly generated unique variables as "insurance" against factor splitting when a general factor may be present. Copyright 1996 by the American Psychological Association, Inc.},
author = {Wood, James M. and Tataryn, Douglas J. and Gorsuch, Richard L.},
doi = {10.1037/1082-989X.1.4.354},
issn = {1082989X},
journal = {Psychological Methods},
number = {4},
pages = {354--365},
title = {{Effects of under- and overextraction on principal axis factor analysis with varimax rotation}},
volume = {1},
year = {1996}
}
@misc{Raykov2011,
address = {New York},
author = {Raykov, Tenko. and Marcoulides, George A},
isbn = {9780415878227 0415878225 9781283037167 1283037165},
language = {English},
publisher = {Routledge},
title = {{Introduction to psychometric theory}},
url = {http://www.myilibrary.com?id=303716},
year = {2011}
}
@article{Majors2001,
abstract = {Exploratory factor analysis was used to provide an organizational framework for data collected from an entering class of students at a large eastern university. Results from the EFA on responses of 1,912 students indicated that items on surveys, based on various interests of administrators, can be grouped into dimensions that can be used to plan student services. Factors that emerged were Religion/Spirituality, Help Seeking, Interracial Relationships, Academic Self-Concept, Cultural Tolerance, Academic Preparedness, Shy/Lonely, and Cult Approval.},
author = {Majors, Mark S. and Sedlacek, William E.},
issn = {08975264},
journal = {Journal of College Student Development},
number = {3},
pages = {272--277},
title = {{Using factor analysis to organize student services}},
volume = {42},
year = {2001}
}
@article{Dwivedi2006,
abstract = {Purpose - To describe the development of a survey instrument designed to measure consumer perceptions of the broadband adoption within the UK households. Design/methodology/approach - A survey research approach was employed to achieve overall aim and following three objectives of this research: to identify initial items that may help to explain the broadband adoption behaviour and determine them employing an exploratory survey approach; to confirm the representativeness of items to a particular construct domain employing content validity approach; and finally, to test the instrument in order to confirm the reliability of items and construct validity. Findings - The final outcome of the instrument development process that culminated from the confirmatory study was a parsimonious, 39-item instrument, consisting of ten scales, all with acceptable levels of content validity, reliability and construct validity. Practical implications - The developed instrument is relevant to both academic and practitioner communities who hold a particular interest in the study and management of broadband adoption from the household consumer perspective. Originality/value - The most conspicuous contribution of the paper is to provide a reliable instrument that is fundamental to measure the household consumer's perceptions of adopting broadband internet. {\textcopyright} Emerald Group Publishing Limited.},
author = {Dwivedi, Yogesh Kumar and Choudrie, Jyoti and Brinkman, Willem Paul},
doi = {10.1108/02635570610666458},
issn = {02635577},
journal = {Industrial Management and Data Systems},
keywords = {Broadband networks,Consumer behaviour,Consumer research,Internet,Surveys,United Kingdom},
number = {5},
pages = {700--718},
title = {{Development of a survey instrument to examine consumer adoption of broadband}},
volume = {106},
year = {2006}
}
@article{Tang2000,
abstract = {Intranet adoption is a topic of increasing importance to enterprises as well as researchers. This paper discusses the development of a theoretical model and testing of research hypotheses. The model captures the critical factors affecting the success of Intranet adoption. An exploratory survey and an empirical study were conducted. Empirical analysis indicates that the proposed model offers enterprises a model that can be used to plan, design, manage, and to evaluate Intranet adoption, and in turn promote the possibility of successful adoption. Moreover, the findings of the success factors on Intranet adoption are consistent with those in other studies.},
author = {Tang, Shung Ming},
doi = {10.1016/S0164-1212(99)00121-1},
issn = {01641212},
journal = {Journal of Systems and Software},
number = {3},
pages = {157--173},
title = {{Impact factor model of Intranet adoption: An exploratory and empirical research}},
volume = {51},
year = {2000}
}
@article{Brillinger2004,
abstract = {This work presents an exploratory data analysis of the trajectories of deer and elk moving about in the Starkey Experimental Forest and Range in eastern Oregon. The animals' movements may be affected by habitat variables and the behavior of the other animals. In the work of this paper a stochastic differential equation-based model is developed in successive stages. Equations of motion are set down motivated by corresponding equations of physics. Functional parameters appearing in the equations are estimated nonparametrically and plots of vector fields of animal movements are prepared. Residuals are used to look for interactions amongst the movements of the animals. There are exploratory analyses of various sorts. Statistical inferences are based on Fourier transforms of the data, which are unequally spaced. The sections of the paper start with motivating quotes and aphorisms from the writings of John W. Tukey. {\textcopyright} 2003 Elsevier B.V. All rights reserved.},
author = {Brillinger, David R. and Preisler, Haiganoush K. and Ager, Alan A. and Kie, John G.},
doi = {10.1016/j.jspi.2003.06.016},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Deer,Dependence,Elk,Exploratory data analysis,Fourier analysis,John W. Tukey,Paths,Starkey Experimental Forest,Time series,Vector field plots},
number = {1-2},
pages = {43--63},
title = {{An exploratory data analysis (EDA) of the paths of moving animals}},
volume = {122},
year = {2004}
}
@article{Cox2001,
abstract = {Examines the applicability of determinants identified in a physical services environment to assess the services relating to e-commerce. It is argued that the lack of human interaction during the Web site experience means that determinants such as competence, courtesy, cleanliness, comfort and friendliness, helpfulness, care, commitment, flexibility are not particularly relevant in e-commerce. On the other hand, determinants such as accessibility, communication, credibility, understanding, appearance, and availability are equally applicable to e-commerce as they are in physical services. The paper argues the need for further research to identify suitable determinants for the e-commerce operating environment. {\textcopyright} 2001, MCB UP Limited},
author = {Cox, J. and Dale, B. G.},
doi = {10.1108/09604520110387257},
issn = {09604529},
journal = {Managing Service Quality: An International Journal},
keywords = {E-commerce,Internet,Service quality},
number = {2},
pages = {121--131},
title = {{Service quality and e-commerce: An exploratory analysis}},
volume = {11},
year = {2001}
}
@article{Yang2003,
abstract = {The purpose of this article is to extend what is know about service quality in realm of the context of Internet retailing. As a result of content analyzing 1,078 consumer anecdotes of online shopping experiences, 14 service quality dimensions representing 42 items were identified. The unique contents of each service quality dimension relate to Internet commerce are examined and discussed. Further, the analysis uncovered a number of contributors to consumer satisfaction and dissatisfaction. The most frequently-mentioned service attributes resulting in consumer satisfaction were responsiveness, credibility, ease of use, reliability, and convenience. On the other hand, different dimensions including responsiveness, reliability, ease of use, credibility, and competence, were likely to dissatisfy online consumers. Finally, this paper provides various managerial implications and recommendations which may suggest avenues for improving service quality in Internet retailing and, as a corollary, expanding experiences by consumers. {\textcopyright} 2003, MCB UP Limited},
author = {Yang, Zhilin and Peterson, Robin T. and Cai, Shaohan},
doi = {10.1108/08876040310501241},
issn = {08876045},
journal = {Journal of Services Marketing},
number = {7},
pages = {685--700},
title = {{Services quality dimensions of Internet retailing: An exploratory analysis}},
volume = {17},
year = {2003}
}
@article{Williams2010,
abstract = {Factor analysis is a multivariate statistical approach commonly used in psychology, education, and more recently in the health-related professions. This paper will attempt to provide novice researchers with a simplified approach to undertaking exploratory factor analysis (EFA). As the paramedic body of knowledge continues to grow, indeed into scale and instrument psychometrics, it is timely that an uncomplicated article such as this be offered to the paramedic readership both nationally and internationally. Factor analysis is an important tool that can be used in the development, refinement, and evaluation of tests, scales, and measures that can be used in education and clinical contexts by paramedics. The objective of the paper is to provide an exploratory factor analysis protocol, offering potential researchers with an empirically-supported systematic approach that simplifies the many guidelines and options associated with completing EFA.},
author = {Williams, Brett and Onsman, Andrys and Brown, Ted},
doi = {10.33151/ajp.8.3.93},
issn = {14474999},
journal = {Journal of Emergency Primary Health Care},
keywords = {Confirmatory factor analysis,Exploratory factor analysis},
number = {3},
pages = {1--13},
title = {{Exploratory factor analysis: A five-step guide for novices}},
volume = {8},
year = {2010}
}
@article{Pitombo2011,
abstract = {This paper aims to find relations between the socioeconomic characteristics, activity participation, land use patterns and travel behavior of the residents in the S{\~{a}}o Paulo Metropolitan Area (SPMA) by using Exploratory Multivariate Data Analysis (EMDA) techniques. The variables influencing travel pattern choices are investigated using: (a) Cluster Analysis (CA), grouping and characterizing the Traffic Zones (TZ), proposing the independent variable called Origin Cluster and, (b) Decision Tree (DT) to find a priori unknown relations among socioeconomic characteristics, land use attributes of the origin TZ and destination choices. The analysis was based on the origin-destination home-interview survey carried out in SPMA in 1997. The DT application revealed the variables of greatest influence on the travel pattern choice. The most important independent variable considered by DT is car ownership, followed by the Use of Transportation "credits" for Transit tariff, and, finally, activity participation variables and Origin Cluster. With these results, it was possible to analyze the influence of a family income, car ownership, position of the individual in the family, use of transportation "credits" for transit tariff (mainly for travel mode sequence choice), activities participation (activity sequence choice) and Origin Cluster (destination/travel distance choice). {\textcopyright} 2010 Elsevier Ltd.},
author = {Pitombo, C. S. and Kawamoto, E. and Sousa, A. J.},
doi = {10.1016/j.tranpol.2010.10.010},
issn = {0967070X},
journal = {Transport Policy},
keywords = {Activity participation,Exploratory Multivariate Data Analysis techniques,Land use,Socioeconomic characteristics},
number = {2},
pages = {347--357},
title = {{An exploratory analysis of relationships between socioeconomic, land use, activity participation variables and travel patterns}},
volume = {18},
year = {2011}
}
@article{Taherdoost2014,
abstract = {Exploratory factor analysis is a complex and multivariate statistical technique commonly employed in information system, social science, education and psychology. This paper intends to provide a simplified collection of information for researchers and practitioners undertaking exploratory factor analysis (EFA) and to make decisions about best practice in EFA. Particularly, the objective of the paper is to provide practical and theoretical information on decision making of sample size, extraction, number of factors to retain and rotational methods.},
author = {Taherdoost, Hamed and Sahibuddin, Shamsul and Jalaliyoon, Neda},
isbn = {9789604743803},
journal = {2nd International Conference on Mathematical, Computational and Statistical Sciences},
keywords = {Exploratory Factor Analysis,Extraction and Rotation Methods,Factor Analysis,Factor Retention Decisions,Scale Development},
pages = {375--382},
title = {{Exploratory factor analysis: Concepts and theory}},
year = {2014}
}
@misc{Hayton2004,
abstract = {The decision of how many factors to retain is a critical component of exploratory factor analysis. Evidence is presented that parallel analysis is one of the most accurate factor retention methods while also being one of the most underutilized in management and organizational research. Therefore, a step-by-step guide to performing parallel analysis is described, and an example is provided using data from the Minnesota Satisfaction Questionnaire. Recommendations for making factor retention decisions are discussed.},
author = {Hayton, James C. and Allen, David G. and Scarpello, Vida},
booktitle = {Organizational Research Methods},
doi = {10.1177/1094428104263675},
issn = {10944281},
keywords = {Exploratory factor analysis,Factor retention decisions,Parallel analysis},
number = {2},
pages = {191--205},
title = {{Factor Retention Decisions in Exploratory Factor Analysis: A Tutorial on Parallel Analysis}},
volume = {7},
year = {2004}
}
@article{Glorfeld1995,
abstract = {One of the most important decisions that can be made in the use of factor analysis is the number of factors to retain. Numerous studies have consistently shown that Horn's parallel analysis is the most nearly accurate methodology for determining the number of factors to retain in an exploratory factor analysis. Although Horn's procedure is relatively accurate, it still tends to error in the direction of indicating the retention of one or two more factors than is actually warranted or of retaining poorly defined factors. A modification of Horn's parallel analysis based on Monte Carlo simulation of the null distributions of the eigenvalues generated from a population correlation identity matrix is introduced. This modification allows identification of any desired upper 1 - a percentile, such as the 95th percentile of this set of distributions. The 1 - ax percentile then can be used to determine whether an eigenvalue is larger than what could be expected by chance. Horn based his original procedure on the average eigenvalues derived from this set of distributions. The modified procedure reduces the tendency of the parallel analysis methodology to overextract. An example is provided that demonstrates this capability. A demonstration is also given that indicates that the parallel analysis procedure and its modification are insensitive to the distributional characteristics of the data used to generate the eigenvalue distributions. {\textcopyright} 1995, Sage Publications. All rights reserved.},
author = {Glorfeld, Louis W.},
doi = {10.1177/0013164495055003002},
issn = {15523888},
journal = {Educational and Psychological Measurement},
number = {3},
pages = {377--393},
title = {{An Improvement on Horn's Parallel Analysis Methodology for Selecting the Correct Number of Factors to Retain}},
volume = {55},
year = {1995}
}
@article{Cattell1966,
author = {Cattell, Raymond B.},
doi = {10.1207/s15327906mbr0102_10},
issn = {15327906},
journal = {Multivariate Behavioral Research},
number = {2},
pages = {245--276},
pmid = {26828106},
title = {{The scree test for the number of factors}},
volume = {1},
year = {1966}
}
@article{Guttman1954,
abstract = {Let R be any correlation matrix of order n, with unity as each main diagonal element. Common-factor analysis, in the Spearman-Thurstone sense, seeks a diagonal matrix U2 such that G = R - U2 is Gramian and of minimum rank r. Let s1 be the number of latent roots of R which are greater than or equal to unity. Then it is proved here that r ≧s1. Two further lower bounds to r are also established that are better than s1. Simple computing procedures are shown for all three lower bounds that avoid any calculations of latent roots. It is proved further that there are many cases where the rank of all diagonal-free submatrices in R is small, but the minimum rank r for a Gramian G is nevertheless very large compared with n. Heuristic criteria are given for testing the hypothesis that a finite r exists for the infinite universe of content from which the sample of n observed variables is selected; in many cases, the Spearman-Thurstone type of multiple common-factor structure cannot hold. {\textcopyright} 1954 Psychometric Society.},
author = {Guttman, Louis},
doi = {10.1007/BF02289162},
issn = {00333123},
journal = {Psychometrika},
number = {2},
pages = {149--161},
title = {{Some necessary conditions for common-factor analysis}},
volume = {19},
year = {1954}
}
@article{Horn1965,
abstract = {It is suggested that if Guttman's latent-root-one lower bound estimate for the rank of a correlation matrix is accepted as a psychometric upper bound, following the proofs and arguments of Kaiser and Dickman, then the rank for a sample matrix should be estimated by subtracting out the component in the latent roots which can be attributed to sampling error, and least-squares "capitalization" on this error, in the calculation of the correlations and the roots. A procedure based on the generation of random variables is given for estimating the component which needs to be subtracted. {\textcopyright} 1965 Psychometric Society.},
author = {Horn, John L.},
doi = {10.1007/BF02289447},
file = {:home/steven/Downloads/Horn1965{\_}Article{\_}ARationaleAndTestForTheNumberO.pdf:pdf},
issn = {00333123},
journal = {Psychometrika},
number = {2},
pages = {179--185},
pmid = {14306381},
title = {{A rationale and test for the number of factors in factor analysis}},
volume = {30},
year = {1965}
}
@article{Hayton2009,
author = {Hayton, James C.},
doi = {10.1080/00273170902939033},
issn = {00273171},
journal = {Multivariate Behavioral Research},
number = {3},
pages = {389--395},
title = {{Commentary on "exploring the sensitivity of Horn's Parallel Analysis to the distributional form of random data"}},
volume = {44},
year = {2009}
}
@article{Dinno2009,
abstract = {Horn's parallel analysis (PA) is the method of consensus in the literature on empirical methods for deciding how many components/factors to retain. Different authors have proposed various implementations of PA. Horn's seminal 1965 article, a 1996 article by Thompson and Daniel, and a 2004 article by Hayton, Allen, and Scarpello all make assertions about the requisite distributional forms of the random data generated for use in PA. Readily available software is used to test whether the results of PA are sensitive to several distributional prescriptions in the literature regarding the rank, normality, mean, variance, and range of simulated data on a portion of the National Comorbidity Survey Replication (Pennell et al., 2004) by varying the distributions in each PA. The results of PA were found not to vary by distributional assumption. The conclusion is that PA may be reliably performed with the computationally simplest distributional assumptions about the simulated data.},
author = {Dinno, Alexis},
doi = {10.1080/00273170902938969},
issn = {00273171},
journal = {Multivariate Behavioral Research},
number = {3},
pages = {362--388},
pmid = {20234802},
title = {{Exploring the sensitivity of Horn's parallel Analysis to the distributional form of random data}},
volume = {44},
year = {2009}
}
@misc{Cavalli-Sforza2003,
abstract = {The past decade of advances in molecular genetic technology has heralded a new era for all evolutionary stud­ies, but especially the science of human evolution. Data on various kinds of DNA variation in human popula­tions have rapidly accumulated. There is increasing recognition of the importance of this variation for medicine and developmental biology and for understanding the history of our species. Haploid markers from mitochon-drial DNA and the Y chromosome have proven invaluable for generating a standard model for evolution of modern humans. Conclusions from earlier research on protein polymorphisms have been generally supported by more sophisticated DNA analysis. Co-evolution of genes with language and some slowly evolving cultural traits, together with the genetic evolution of commensals and parasites that have accompanied modern humans in their expansion from Africa to the other continents, supports and supplements the standard model of genetic evolution. The advances in our understanding of the evolutionary history of humans attests to the advantages of multidisciplinary research. {\textcopyright} 2003 Nature Publishing Group.},
author = {Cavalli-Sforza, L. Luca and Feldman, Marcus W.},
booktitle = {Nature Genetics},
doi = {10.1038/ng1113},
issn = {15461718},
number = {3S},
pages = {266--275},
pmid = {12610536},
title = {{The application of molecular genetic approaches to the study of human evolution}},
volume = {33},
year = {2003}
}
@article{Patterson2006,
abstract = {Current methods for inferring population structure from genetic data do not provide formal significance tests for population differentiation. We discuss an approach to studying population structure (principal components analysis) that was first applied to genetic data by Cavalli-Sforza and colleagues. We place the method on a solid statistical footing, using results from modern statistics to develop formal significance tests. We also uncover a general "phase change" phenomenon about the ability to detect structure in genetic data, which emerges from the statistical theory we use, and has an important implication for the ability to discover structure in genetic data: for a fixed but large dataset size, divergence between two populations (as measured, for example, by a statistic like FST) below a threshold is essentially undetectable, but a little above threshold, detection will be easy. This means that we can predict the dataset size needed to detect structure. {\textcopyright} 2006 Patterson et al.},
author = {Patterson, Nick and Price, Alkes L. and Reich, David},
doi = {10.1371/journal.pgen.0020190},
issn = {15537390},
journal = {PLoS Genetics},
number = {12},
pages = {2074--2093},
pmid = {17194218},
title = {{Population structure and eigenanalysis}},
volume = {2},
year = {2006}
}
@article{Saccenti2017,
abstract = {Horn's parallel analysis is a widely used method for assessing the number of principal components and common factors. We discuss the theoretical foundations of parallel analysis for principal components based on a covariance matrix by making use of arguments from random matrix theory. In particular, we show that (i) for the first component, parallel analysis is an inferential method equivalent to the Tracy–Widom test, (ii) its use to test high-order eigenvalues is equivalent to the use of the joint distribution of the eigenvalues, and thus should be discouraged, and (iii) a formal test for higher-order components can be obtained based on a Tracy–Widom approximation. We illustrate the performance of the two testing procedures using simulated data generated under both a principal component model and a common factors model. For the principal component model, the Tracy–Widom test performs consistently in all conditions, while parallel analysis shows unpredictable behavior for higher-order components. For the common factor model, including major and minor factors, both procedures are heuristic approaches, with variable performance. We conclude that the Tracy–Widom procedure is preferred over parallel analysis for statistically testing the number of principal components based on a covariance matrix.},
author = {Saccenti, Edoardo and Timmerman, Marieke E.},
doi = {10.1007/s11336-016-9515-z},
issn = {00333123},
journal = {Psychometrika},
keywords = {common factor analysis,covariance matrix,number of common factors,number of principal components,principal component analysis},
number = {1},
pages = {186--209},
pmid = {27738958},
title = {{Considering Horn's Parallel Analysis from a Random Matrix Theory Point of View}},
volume = {82},
year = {2017}
}
@article{Kohli2015,
abstract = {There are well-defined theoretical differences between the classical test theory (CTT) and item response theory (IRT) frameworks. It is understood that in the CTT framework, person and item statistics are test- and sample-dependent. This is not the perception with IRT. For this reason, the IRT framework is considered to be theoretically superior to the CTT framework for the purpose of estimating person and item parameters. In previous simulation studies, IRT models were used both as generating and as fitting models. Hence, results favoring the IRT framework could be attributed to IRT being the data-generation framework. Moreover, previous studies only considered the traditional CTT framework for the comparison, yet there is considerable literature suggesting that it may be more appropriate to use CTT statistics based on an underlying normal variable (UNV) assumption. The current study relates the class of CTT-based models with the UNV assumption to that of IRT, using confirmatory factor analysis to delineate the connections. A small Monte Carlo study was carried out to assess the comparability between the item and person statistics obtained from the frameworks of IRT and CTT with UNV assumption. Results show the frameworks of IRT and CTT with UNV assumption to be quite comparable, with neither framework showing an advantage over the other.},
author = {Kohli, Nidhi and Koran, Jennifer and Henn, Lisa},
doi = {10.1177/0013164414559071},
issn = {15523888},
journal = {Educational and Psychological Measurement},
keywords = {classical test theory,factor analysis,item response theory,relationship},
number = {3},
pages = {389--405},
title = {{Relationships Among Classical Test Theory and Item Response Theory Frameworks via Factor Analytic Models}},
volume = {75},
year = {2015}
}
@article{Mosquera2008,
abstract = {Insults elicit intense emotion. This study tests the hypothesis that one's social image, which is especially salient in honour cultures, influences the way in which one reacts to an insult. Seventy-seven honour-oriented and 72 non-honour oriented participants answered questions about a recent insult episode. Participants experienced both anger and shame in reaction to the insult. However, these emotions resulted in different behaviours. Anger led to verbal attack (i.e., criticising, insulting in return) among all participants. This relationship was explained by participants' motivation to punish the wrongdoer. Shame, on the other hand, was moderated by honour. Shame led to verbal disapproval of the wrongdoers behaviour, but only among the honour-oriented participants. This relationship was explained by these participants' motivation to protect their social image. By contrast, shame led to withdrawal among non-honour-oriented participants.},
author = {Mosquera, Patricia M.Rodriguez and Fischer, Agneta and Manstead, Antony and Zaalberg, Ruud},
doi = {10.1080/02699930701822272},
issn = {02699931},
journal = {Cognition and Emotion},
number = {8},
pages = {1471--1498},
title = {{Attack, disapproval, or withdrawal? The role of honour in anger and shame responses to being insulted}},
volume = {22},
year = {2008}
}
@article{Souza2017,
abstract = {Introduction: Theory of the Culture of Honor is one of the few models in criminology specifically geared toward homicide. It proposes that, in certain societies, men must never show weakness and are required to react violently to any perceived threats to their reputation, thereby increasing their probability of committing a homicide. This has been suggested as the main explanation for the high rates of this type of crime in Brazil, particularly in the Northeast. Underlying this explanation there are complex mechanisms and processes that have yet to be clarified. Objectives: The present research aimed to investigate the workings of the possible psychocultural mechanisms underlying the culture of honor and the process through which they might affect the individual propensity toward homicide. Methods: A total of 336 Brazilian adults were assessed regarding a broad range of sociodemographic, psychological, and sociocultural variables, including their attitudes toward homicide. The resulting dataset was analyzed using Smallest Space Analysis and Facet Theory. Results: It seems that certain cultural elements associated to traditional masculinity and enhanced anger tend to promote negative personality traits and increase one's propensity toward committing homicide. Conclusion: The findings obtained not only confirm the Theory of the Culture of Honor for the propensity toward homicide, but also explicit and clarify some of the psychocultural processes and mechanisms involved, suggesting a new scientific framework.},
author = {Souza, Monica G.T.C. and Souza, Bruno C. and Roazzi, Antonio and da Silva, Edson S.},
doi = {10.3389/fpsyg.2017.01872},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Criminology,Culture of honor,Facet Theory,Homicidal honor,Homicide,Smallest space analysis},
number = {NOV},
title = {{Psychocultural mechanisms of the propensity toward criminal homicide: A multidimensional view of the Culture of Honor}},
volume = {8},
year = {2017}
}
@book{DeVellis2017,
abstract = {A best-seller in its First Edition, Scale Development: Theory and Applications, Second Edition has been extensively updated and revised to address changes in the fielHogarty2005d and topics that have grown in importance since the First Edition. Widely adopted for graduate courses in departments such as Psychology, Public Health, Marketing, Nursing, and Education, this book will prove beneficial to applied researchers across the social sciences.New to the Second Edition:Figures and practical tips for studentsNew section on face validity (Chapter 4)Substantially expanded presentation of factor analysis (Chapter 6)New chapter (7) on item response theory (IRT)Coverage of qualitative procedures and issues related to differential item functioning (Chapter 8)Praise for the First Edition:"Very readable, well organized, and straightforward. I would recommend this book for practitioners, graduate students, and faculty members who are seeking a practical, rather than a psychometric, treatment of scale development. This book offers a clear overview for those interested in the development and validation of measurement scales."-EVALUATION PRACTICE},
address = {Los Angeles},
author = {DeVellis, Robert F.},
doi = {10.2307/2075704},
issn = {00943061},
language = {English},
publisher = {Sage Publications},
title = {{Scale Development: Theory and Applications.}},
year = {2017}
}
@book{Zeller2009,
address = {Cambridge},
author = {Zeller, Richard A and Carmines, Edward G},
isbn = {9780521299411 0521299411},
language = {English},
publisher = {Cambridge University Press},
title = {{Measurement in the social sciences : the link between theory and data}},
year = {2009}
}
@book{Bandalos2018,
address = {New York},
author = {Bandalos, Deborah L},
isbn = {9781462532131 1462532136},
language = {English},
publisher = {The Guilford Press},
title = {{Measurement theory and applications for the social sciences}},
year = {2018}
}
@article{TenBerge2004,
abstract = {To assess the reliability of congeneric tests, specifically designed reliability measures have been proposed. This paper emphasizes that such measures rely on a unidimensionality hypothesis, which can neither be confirmed nor rejected when there are only three test parts, and will invariably be rejected when there are more than three test parts. Jackson and Agunwamba's (1977) greatest lower bound to reliability is proposed instead. Although this bound has a reputation for overestimating the population value when the sample size is small, this is no reason to prefer the unidimensionality-based reliability. Firstly, the sampling bias problem of the glb does not play a role when the number of test parts is small, as is often the case with congeneric measures. Secondly, glb and unidimensionality based reliability are often equal when there are three test parts, and when there are more test parts, their numerical values are still very similar. To the extent that the bias problem of the greatest lower bound does play a role, unidimensionality-based reliability is equally affected. Although unidimensionality and reliability are often thought of as unrelated, this paper shows that, from at least two perspectives, they act as antagonistic concepts. A measure, based on the same framework that led to the greatest lower bound, is discussed for assessing how close is a set of variables to unidimensionality. It is the percentage of common variance that can be explained by a single factor. An empirical example is given to demonstrate the main points of the paper. {\textcopyright} 2004 The Psychometric Society.},
author = {{Ten Berge}, Jos M.F. and So{\v{c}}an, Gregor},
doi = {10.1007/BF02289858},
issn = {00333123},
journal = {Psychometrika},
keywords = {Congeneric test,Reliability,Unidimensionality of a test},
number = {4},
pages = {613--625},
title = {{The greatest lower bound to the reliability of a test and the hypothesis of unidimensionality}},
volume = {69},
year = {2004}
}
@article{Revelle2009,
abstract = {There are three fundamental problems in Sijtsma (Psychometrika, 2008): (1) contrary to the name, the glb is not the greatest lower bound of reliability but rather is systematically less than $\omega$ t (McDonald, Test theory: A unified treatment, Erlbaum, Hillsdale, 1999), (2) we agree with Sijtsma that when considering how well a test measures one concept, $\alpha$ is not appropriate, but recommend $\omega$ t rather than the glb, and (3) the end user needs procedures that are readily available in open source software. {\textcopyright} 2008 The Psychometric Society.},
author = {Revelle, William and Zinbarg, Richard E.},
doi = {10.1007/s11336-008-9102-z},
issn = {00333123},
journal = {Psychometrika},
keywords = {Coefficient alpha,Coefficient beta,Coefficient omega,Homogeneity,Internal consistency,Reliability,Test theory},
number = {1},
pages = {145--154},
title = {{Coefficients alpha, beta, omega, and the glb: Comments on sijtsma}},
volume = {74},
year = {2009}
}
@article{Green2009,
abstract = {A method is presented for estimating reliability using structural equation modeling (SEM) that allows for nonlinearity between factors and item scores. Assuming the focus is on consistency of summed item scores, this method for estimating reliability is preferred to those based on linear SEM models and to the most commonly reported estimate of reliability, coefficient alpha. {\textcopyright} 2008 The Psychometric Society.},
author = {Green, Samuel B. and Yang, Yanyun},
doi = {10.1007/s11336-008-9099-3},
issn = {00333123},
journal = {Psychometrika},
keywords = {Factor analysis of categorical data,Reliability,Reliability of scales with Likert items,Structural equation modeling},
number = {1},
pages = {155--167},
title = {{Reliability of summed item scores using structural equation modeling: An alternative to coefficient alpha}},
volume = {74},
year = {2009}
}
@article{Green2009a,
abstract = {The general use of coefficient alpha to assess reliability should be discouraged on a number of grounds. The assumptions underlying coefficient alpha are unlikely to hold in practice, and violation of these assumptions can result in nontrivial negative or positive bias. Structural equation modeling was discussed as an informative process both to assess the assumptions underlying coefficient alpha and to estimate reliability {\textcopyright} 2008 The Psychometric Society.},
author = {Green, Samuel B. and Yang, Yanyun},
doi = {10.1007/s11336-008-9098-4},
issn = {00333123},
journal = {Psychometrika},
keywords = {Coefficient alpha,Reliability,Structural equation modeling,Violation of assumptions},
number = {1},
pages = {121--135},
title = {{Commentary on coefficient alpha: A cautionary tale}},
volume = {74},
year = {2009}
}
@article{Fleishman1987,
abstract = {The present paper demonstrates how LISREL can be used to examine measurement model assumptions and to assess the reliability of a scale. LISREL was used to investigate (a) the nature of the basic measurement model for a scale, (b) scale invariance across time, and (c) scale invariance across groups. Information obtained in such analyses helps to assess accurately the estimate of the reliability of the scale. As an illustration, responses of 722 elementary school students to the Coopersmith Self-Esteem Inventory (Form B) were analyzed. Results indicated that a congeneric measurement model with correlated errors was most appropriate. Internal-consistency estimates were not affected by correlated errors, within each of the two time points (pretest and posttest). However, correlated errors across time produced an overestimate of the stability coefficient. Further, the factor structure of the scale was invariant across three ethnic groups, an invariance that implied similar reliability estimates across groups.},
author = {Fleishman, John and Benson, Jeri},
doi = {10.1177/0013164487474008},
issn = {0013-1644},
journal = {Educational and Psychological Measurement},
number = {4},
pages = {925--939},
title = {{Using Lisrel to Evaluate Measurement Models and Scale Reliability}},
volume = {47},
year = {1987}
}
@article{Cole2007,
abstract = {In practice, the inclusion of correlated residuals in latent-variable models is often regarded as a statistical sleight of hand, if not an outright form of cheating. Consequently, researchers have tended to allow only as many correlated residuals in their models as are needed to obtain a good fit to the data. The current article demonstrates that this strategy leads to the underinclusion of residual correlations that are completely justified on the basis of measurement theory and research design. In many designs, the absence of such correlations will not substantially harm the fit of the model; however, failure to include them can change the meaning of the extracted latent variables and generate potentially misleading results. Recommendations include (a) returning to the full multitrait-multimethod design when measurement theory implies the existence of shared method variance and (b) abandoning the evil-but-necessary attitude toward correlated residuals when they reflect intended features of the research design. {\textcopyright} 2007 American Psychological Association.},
author = {Cole, David A. and Ciesla, Jeffrey A. and Steiger, James H.},
doi = {10.1037/1082-989X.12.4.381},
issn = {1082989X},
journal = {Psychological Methods},
keywords = {correlated residuals,model modification,multitrait-multimethod (MTMM),shared method variance,structural equation modeling},
number = {4},
pages = {381--398},
pmid = {18179350},
title = {{The Insidious Effects of Failing to Include Design-Driven Correlated Residuals in Latent-Variable Covariance Structure Analysis}},
volume = {12},
year = {2007}
}
@misc{Jacoby1991,
abstract = {By examining some of the basic scaling questions, such as the importance of measurement levels, the kinds of variables needed for Likert or Guttman scales and when to use multidimensional scaling versus factor analysis, Jacoby introduces readers to the most appropriate scaling strategies for different research situations.},
address = {London},
author = {Jacoby, William G},
isbn = {9781412983860 141298386X},
language = {English},
publisher = {SAGE},
title = {{Data theory and dimensional analysis}},
year = {1991}
}
@book{McIver1981,
abstract = {This series of methodological works provides introductory explanations and demonstrations of various data analysis techniques applicable to the social sciences. Designed for readers with a limited background in statistics or mathematics, this series aims to make the assumptions and practices of quantitative analysis more readily accessible.},
address = {Beverly Hills},
author = {McIver, John P and Carmines, Edward G},
isbn = {0803917368 9780803917361},
language = {English},
publisher = {Sage Publications},
title = {{Unidimensional scaling}},
year = {1981}
}
@book{Shively2017,
abstract = {The Craft of Political Research is a non-technical introduction to research design and analysis in political science, emphasizing the choices we make when we design a research project and analyze its results. The book's approach centers on asking an interesting research question, and then designing inquiry into the question so as to eliminate as many alternative explanations as possible. How do we develop theory, and what constitutes a good research question? How do we develop measures and gather evidence to answer a question? How do we analyze our findings? Students will be introduced to such topics as multidimensional concepts, levels of measurement, validity, reliability, random and non-random measurement error, sampling, case selection, causality, experimental and quasi-experimental design, statistical inference, and regression and correlation analysis. Throughout, the emphasis is on understanding the “back story” of analysis-why do we measure in a particular way, why do we choose one design as against another, why do we conduct our analysis as we do. Emphasizing the internal logic of research methods and the collaborative nature of the research process, the greatest strength of the book is its clarity and the large range of political science examples it provides. It works at a conceptual level, seeking an understanding of the principles that underlie techniques and the reasons why we choose them. New to this edition: • Updated and international examples from the US, UK, Latin America and China amongst others, and international organizations such as the World Bank and the United Nations. • New section, “Reading Political Science” reviews sources of published political research, with some broad principles for how to find good sources, and advises students on what to look for when reading a research report. • New section, “Gathering Accurate Information” reviews published sources of data, such as UNESCO, and offers advice about how to use such sources. It advises students on how to gather data in personal interviews and it acquaints them with publicly available data sets for secondary analysis. • Online material featuring revised learning objectives for each chapter, and a new section offering projects and questions for each chapter.},
author = {Shively, W. Phillips},
booktitle = {The Craft of Political Research},
doi = {10.4324/9781315269559},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shively - 2017 - The craft of political research Tenth edition.pdf:pdf},
isbn = {9781351979429},
month = {jan},
pages = {1--186},
publisher = {Taylor and Francis},
title = {{The craft of political research}},
year = {2017}
}
@book{Borsboom2005,
abstract = {Is it possible to measure psychological attributes like intelligence, personality and attitudes and if so, how does that work? What does the term ‘measurement' mean in a psychological context? This fascinating and timely book discusses these questions and investigates the possible answers that can be given response. Denny Borsboom provides an in-depth treatment of the philosophical foundations of widely used measurement models in psychology. The theoretical status of classical test theory, latent variable theory and positioned in terms of the underlying philosophy of science. Special attention is devoted to the central concept of test validity and future directions to improve the theory and practice of psychological measurement are outlined.},
address = {Camebridge},
author = {Borsboom, Denny},
booktitle = {Measuring the Mind: Conceptual Issues in Contemporary Psychometrics},
doi = {10.1017/CBO9780511490026},
isbn = {9780511490026},
pages = {1--185},
publisher = {Cambridge University Press},
title = {{Measuring the mind: Conceptual issues in contemporary psychometrics}},
year = {2005}
}
@incollection{Costa2018,
author = {Costa, Victoria M.},
booktitle = {International Handbook of Philosophy of Education},
doi = {10.1007/978-3-319-72761-5_96},
editor = {Smeyers, P},
pages = {1389--1400},
publisher = {Springer, Cham},
title = {{Patriotism and Nationalism}},
year = {2018}
}
@incollection{Wickrama2020,
address = {New York, NY},
author = {Wickrama, Kandauda and Lee, Tae Kyoung and O'Neal, Catherine Walker and Lorenz, Frederick},
booktitle = {Higher-Order Growth Curves and Mixture Modeling with Mplus: a practical guide},
doi = {10.4324/9781315642741-11},
editor = {Wickrama, K. A. S. and Lee, T. K. and O'Neal, C.W. and Lorenz, F.O.},
month = {dec},
pages = {67--78},
publisher = {Routledge},
title = {{Longitudinal Confirmatory Factor Analysis and Curve-of-Factors Growth Curve Models}},
year = {2020}
}
@incollection{Ram2015,
abstract = {Advances in mobile and computing technology are opening new possibilities to obtain biobehavioral data, model it in real time, and remotely deploy interventions at population scale. The electronic devices many of us now carry with us as we go about our daily lives provide a wide array of opportunities to collect more and more data from more and more people and, potentially, to deliver fine- and context-specific guidance to them. Such data streams have tremendous implications for how biopsychosociocultural development can be approached, both in principle and in practice. As the "big data" arrive it shall be possible to track, model, and guide the progression of the incremental, stability-maintenance, and transformational change processes that shape individuals' development—in real-life and in real-time. Human development progresses.},
author = {Ram, Nilam and Grimm, Kevin J.},
booktitle = {Handbook of Child Psychology and Developmental Science},
doi = {10.1002/9781118963418.childpsy120},
editor = {Lerner, Richard M.},
month = {mar},
pages = {1--31},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Growth Curve Modeling and Longitudinal Factor Analysis}},
year = {2015}
}
@incollection{Bornstein2018,
address = {California},
author = {Bornstein, Marc H.},
booktitle = {The SAGE Encyclopedia of Lifespan Human Development},
doi = {10.4135/9781506307633.n373},
month = {mar},
publisher = {Thousand Oaks},
title = {{Growth Curve Modeling and Longitudinal Factor Analysis}},
year = {2018}
}
@article{Tisak1989,
abstract = {For multiple populatios, a longtidinal factor analytic model which is entirely exploratory, that is, no explicit identification constraints, is proposed. Factorial collapse and period/practice effects are allowed. An invariant and/or stationary factor pattern is permitted. This model is formulated stochastically. To implement this model a stagewise EM algorithm is developed. Finally a numerical illustration utilizing Nesselroade and Baltes' data is presented. {\textcopyright} 1989 The Psychometric Society.},
author = {Tisak, John and Meredith, William},
doi = {10.1007/BF02294520},
issn = {00333123},
journal = {Psychometrika},
keywords = {EM algorithm,longitudinal factor analysis,maximum likelihood extimation},
month = {jun},
number = {2},
pages = {261--281},
publisher = {Springer-Verlag},
title = {{Exploratory longitudinal factor analysis in multiple populations}},
volume = {54},
year = {1989}
}
@article{Nesselroade1972,
abstract = {The "longitudinal factor analysis" model, which uniquely resolves factors from two occasions of data representing the same persons measured on the same test battery, is shown to be derivable by application of canonical correlation procedures to factor scores. Interpreted in this light, it is suggested that, in attaining its objective, "longitudinal factor analysis" maximizes temporal stability of factor scores-an emphasis which may be warranted for some types of change but not for others. {\textcopyright} 1972 Psychometric Society.},
author = {Nesselroade, John R.},
doi = {10.1007/BF02306776},
issn = {00333123},
journal = {Psychometrika},
month = {jun},
number = {2},
pages = {187--191},
publisher = {Springer-Verlag},
title = {{Note on the "longitudinal factor analysis" model}},
volume = {37},
year = {1972}
}
@incollection{Tisak1990,
address = {Boston},
author = {Tisak, John and Meredith, William},
booktitle = {Statistical Methods in Longitudinal Research},
doi = {10.1016/b978-0-12-724960-5.50009-3},
editor = {von Eye, Alexander},
pages = {125--149},
publisher = {Academic Press},
title = {{Longitudinal Factor Analysis}},
year = {1990}
}
@article{Corballis1970,
abstract = {A model is presented for factor analysing scores on a set of psychological tests administered as both pre- and postmeasures in a study of change. The model assumes that the same factors underlie the tests on each occasion, but that factor scores as well as factor loadings may change between occasions. Factors are defined to be orthogonal between as well as within occasions. A two-stage least squares procedure for fitting the model is described, and generally provides a unique rotation solution for the factors on each occasion. {\textcopyright} 1970 Psychometric Society.},
author = {Corballis, M. C. and Traub, R. E.},
doi = {10.1007/BF02290595},
issn = {18600980},
journal = {Psychometrika},
number = {1},
pages = {79--98},
title = {{Longitudinal factor analysis}},
volume = {35},
year = {1970}
}
@book{McElreath2020,
abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds readers' knowledge of and confidence in statistical modeling. Reflecting the need for even minor programming in today's model-based statistics, the book pushes readers to perform step-by-step calculations that are usually automated. This unique computational approach ensures that readers understand enough of the details to make reasonable choices and interpretations in their own modeling work. The text presents generalized linear multilevel models from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. It covers from the basics of regression to multilevel models. The author also discusses measurement error, missing data, and Gaussian process models for spatial and network autocorrelation. By using complete R code examples throughout, this book provides a practical foundation for performing statistical inference. Designed for both PhD students and seasoned professionals in the natural and social sciences, it prepares them for more advanced or specialized statistical modeling. Web Resource The book is accompanied by an R package (rethinking) that is available on the author's website and GitHub. The two core functions (map and map2stan) of this package allow a variety of statistical models to be constructed from standard model formulas.},
address = {Boca Raton},
author = {McElreath, Richard},
booktitle = {Statistical Rethinking},
publisher = {CRC Press},
title = {{Statistical Rethinking}},
year = {2020}
}
@book{Gelman2020,
abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and GLM. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
booktitle = {Regression and Other Stories},
doi = {10.1017/9781139161879},
title = {{Regression and Other Stories}},
year = {2020}
}
@article{Saucier2016,
abstract = {Masculine honor, particularly as defined by the Southern culture of honor, centers on the belief that aggression is sometimes justifiable and necessary, such as in response to insult or threat. While masculine honor has been examined in terms of cultural differences, it has been less often examined in terms of individual differences. We developed a measure of masculine honor beliefs (MHBS) inspired by research on the Southern culture of honor. Four studies showed that the MHBS demonstrated internal consistency, test-retest reliability, convergent validity with measures of trait aggression and sexism, discriminant validity from measures of social desirability, conservatism and self-esteem, and predictive and incremental validity in predicting reactions to honor-related provocation above and beyond participants' sex and other measures of honor beliefs. The MHBS allows masculine honor beliefs to be examined as an important factor in understanding men's motivations for aggressive behavior, particularly in response to provocation.},
author = {Saucier, Donald A. and Stanford, Amanda J. and Miller, Stuart S. and Martens, Amanda L. and Miller, Alyssa K. and Jones, Tucker L. and McManus, Jessica L. and Burns, Mason D.},
doi = {10.1016/j.paid.2015.12.049},
issn = {01918869},
journal = {Personality and Individual Differences},
keywords = {Aggression,Culture of honor,Individual differences,Masculine honor},
pages = {7--15},
title = {{Masculine honor beliefs: Measurement and correlates}},
volume = {94},
year = {2016}
}
@article{VanOsch2013,
abstract = {Masculine honor has been found to explain the relationship between insults and aggression in the USA. However, detailed accounts of Mediterranean honor cultures suggest that family honor may be more important in explaining cross-cultural differences in aggression. Two studies revealed that people from Turkish honor culture intended to aggress more after being insulted than Dutch people from a nonhonor culture (Study 1), and that this effect was driven by differences in family honor rather than differences in masculine honor (Study 2). We posit that family honor may be a key factor in explaining insult-related aggression in Mediterranean honor cultures. {\textcopyright} The Author(s) 2013.},
author = {van Osch, Yvette and Breugelmans, Seger M. and Zeelenberg, Marcel and B{\"{o}}l{\"{u}}k, Pinar},
doi = {10.1177/1368430212467475},
issn = {13684302},
journal = {Group Processes and Intergroup Relations},
keywords = {Turkish culture,aggression,biculturals,family honor,honor,insults,masculine honor},
number = {3},
pages = {334--344},
title = {{A different kind of honor culture: Family honor and aggression in Turks}},
volume = {16},
year = {2013}
}
@article{Shackelford2005,
abstract = {A key element of cultures of honor is that men in these cultures are prepared to protect with violence the reputation for strength and toughness. Such cultures are likely to develop where (1) a man's resources can be thieved in full by other men and (2) the governing body is weak and thus cannot prevent or punish theft. Historically a herding culture operating outside of formal government, the southern United States has a rich culture of honor. In this article, I briefly review research conducted by Nisbett, Cohen, and colleagues on the southern culture of honor. I then present several important but unanswered questions about the development and maintenance of the southern culture of honor. I next argue that current models of the development and maintenance of cultures of honor and violence can be informed by an evolutionary psychological perspective. I conclude with a tentative evolutionary psychological analysis of the development and maintenance of the southern culture of honor.},
author = {Shackelford, Todd K.},
doi = {10.1177/147470490500300126},
issn = {1474-7049},
journal = {Evolutionary Psychology},
number = {1},
title = {{An Evolutionary Psychological Perspective on Cultures of Honor}},
volume = {3},
year = {2005}
}
@book{Nisbett2018,
address = {London},
author = {Nisbett, Richard E.},
publisher = {Taylor and Francis},
title = {{Culture Of Honor: the Psychology Of Violence In The South}},
year = {2018}
}
@article{Beck1996,
abstract = {Southerners [in the US] have been regarded as more violent than their northern counterparts. And . . . there consistently is a greater number of white-perpetrated murders in the South than in the North. "Culture of Honor" . . . explores the underlying reasons for this violence. This inclination to violence is the result of a culture of honor in which a man's reputation is key to his economic survival. "Culture of Honor": explains why homicides, especially those involving arguments, are more common in the South, and why rural areas show the highest rates; reveals that there is little regional difference in homicide rate for African-Americans, suggesting that it is something about white southern culture that causes the violence, instead of just living below the Mason-[Dixon] line; explores how the culture of honor, self-protection ethic, and a widespread presence of guns contribute to a cycle of violence in which arguments lead to deadly retribution; links southern white culture to urban ghetto culture, both of which encourage violent responses to a perceived affront; investigates how cultural ideas about gender and masculinity lead to the acceptance of violence to maintain a man's reputation as strong and powerful [and] uncovers the attitudes, beliefs, and behaviors concerning honor, self-protection, and violence. Using historical, archival and experimental data, the authors show why it is considered acceptable to be violent in response to an insult, to protect home and property, or to aid in socializing children.},
author = {Beck, E M},
journal = {The Georgia historical quarterly},
number = {4},
title = {{Culture of Honor: The Psychology of Violence in the South New Directions in Social Psychology Series}},
volume = {80},
year = {1996}
}
@article{III1997,
abstract = {Southerners [in the US] have been regarded as more violent than their northern counterparts. And . . . there consistently is a greater number of white-perpetrated murders in the South than in the North. "Culture of Honor" . . . explores the underlying reasons for this violence. {\textless}xh:br{\textgreater}{\textless}/xh:br{\textgreater} This inclination to violence is the result of a culture of honor in which a man's reputation is key to his economic survival. {\textless}xh:br{\textgreater}{\textless}/xh:br{\textgreater} "Culture of Honor": explains why homicides, especially those involving arguments, are more common in the South, and why rural areas show the highest rates; reveals that there is little regional difference in homicide rate for African-Americans, suggesting that it is something about white southern culture that causes the violence, instead of just living below the Mason-[Dixon] line; explores how the culture of honor, self-protection ethic, and a widespread presence of guns contribute to a cycle of violence in which arguments lead to deadly retribution; links southern white culture to urban ghetto culture, both of which encourage violent responses to a perceived affront; investigates how cultural ideas about gender and masculinity lead to the acceptance of violence to maintain a man's reputation as strong and powerful [and] uncovers the attitudes, beliefs, and behaviors concerning honor, self-protection, and violence. Using historical, archival and experimental data, the authors show why it is considered acceptable to be violent in response to an insult, to protect home and property, or to aid in socializing children. (PsycINFO Database Record (c) 2010 APA, all rights reserved)},
author = {Jackson, Harvey H. and Nisbett, Richard E. and Cohen, Dov},
doi = {10.2307/2211676},
issn = {00224642},
journal = {The Journal of Southern History},
number = {3},
pages = {661},
title = {{Culture of Honor: The Psychology of Violence in the South.}},
volume = {63},
year = {1997}
}
@book{Farcomeni2016,
abstract = {Robust Methods for Data Reduction gives a non-technical overview of robust data reduction techniques, encouraging the use of these important and useful methods in practical applications. The main areas covered include principal components analysis, sparse principal component analysis, canonical correlation analysis, factor analysis, clustering, double clustering, and discriminant analysis. The first part of the book illustrates how dimension reduction techniques synthesize available information by reducing the dimensionality of the data. The second part focuses on cluster and discriminant analysis. The authors explain how to perform sample reduction by finding groups in the data. Despite considerable theoretical achievements, robust methods are not often used in practice. This book fills the gap between theoretical robust techniques and the analysis of real data sets in the area of data reduction. Using real examples, the authors show how to implement the procedures in R. The code and data for the examples are available on the book's CRC Press web page.},
author = {Farcomeni, Alessio and Greco, Luca},
booktitle = {Robust Methods for Data Reduction},
doi = {10.1201/b18358},
isbn = {9781466590632},
pages = {1--266},
title = {{Robust methods for data reduction}},
year = {2016}
}
@techreport{Costa2003,
abstract = {The paper presents a comparison between an unidimensional approach to the measurement of poverty, obtained as a function of observable income, and a multidimensional approach, defined on the basis of economic, social, demographic and cultural indicators.},
author = {Costa, Michele},
number = {2003-02},
title = {{A comparison between unidimensional and multidimensional approaches to the measurement of poverty}},
type = {WorkingPaper},
year = {2003}
}
@article{Stone1999,
abstract = {The 26 Chapters of Measurement Essentials are Units of Stud-v. These units evolved during ourweekly discussions ofmeasurement on Sunday mornings from 1994 to 1998 . Each Sunday we met at Ben's kitchen table and worked together on the explanation of a measurement topic which had arisen in our work . Some chapters came together quickly. Others took weeks of reconsideration. As we worked we drew on whatever material seemed most useful . You will find excerpts from our 1979 book, BEST TEST DESIGN . This happens when we wanted to recon- sider fundamental material. You will find some chapters that return to a topic already discussed in an earlier chapter. This happens when we found we had more to say about a topic or wanted to explore a different perspective, but saw no particular fault with our first discussion . You will also find our notation inconsistent . Our notational variations, however, need not trouble you. Whether the conjoint additive relation between person ability and item difficulty is represented by "B-D", "B-D", "b-d", or "b-d" the meaning remains entirely clear. We enjoyed building these 26 Units of Study. We wrote them for ourselves, for our students and for you. We hope they will be useful to you . 1 . THE IDEA OF MEASUREMENT No discussion of scientific method is complete without an argument for the importance of fundamental measurement - measurement of the kind characterizing length and weight . Yet, few social scientists attempt to construct fundamental measures . This is not because social scientists disapprove of fundamental measurement. It is because they despair of obtaining it. The conviction that fundamental measurement is unattainable in social science and education has such a grip that we fail to see that our despair is unnecessary. Fundamental measurement is not only obtainable in social science but, in an unaware and hence incomplete form, is widely relied on. Social scientists are already practicing a kind of fundamental measurement but without knowing it and hence without enjoying its benefits or building on its strengths . The realization that fundamental measurements can be made in social science research is usuallv traced to Luce and Tukey (1964) who show that fundamental measurement can be constructed from an axiomatization ofcomparisons amongresponses to arbitrary pairs of objects of two specified kinds . But Thurstone's 1927 Law of Comparative Judgement (1928a,1928b, 1929) contains results whichare rough examples of fundamental measurement . Fundamental measurement also occurs in Bradley and Terry (1952) and in Rasch (1958, 1960/1980, 1966a, 1966b, 1967, 1977) . The fundamental measurement which follows from Rasch's "specific objectivity" is developed in Rasch 1960/1980, 1961, 1967 and 1977. Rasch's "specific objectivity" and R.A. Fisher's "estimation sufficiency" are twosides ofthesame implementation of inference . Andersen (1977) shows that the only measuring processes which support specific objectivity and hence fundamental measurement are those which have sufficient statistics for their parameters . It follows that sufficient statistics lead to and are necessary for fundamental measurement. Several authors connect "additive conjoint" fundamental measurement with Rasch's work (Keats, 1967, 1971 ; Fischer 1968 ; Brogden, 1977) . Perline, Wright and Vainer (1977) provide two empirical demonstrations oftheequivalence ofnon-metric multidimensional scaling (Kruskal,1964,1965) andthe Rasch process in realizing fundamental measurement. Wright and Stone (1979) show how to obtain fundamental measurementfrom mental tests . Wright andMasters (1982) give examples ofits successful application to rating scales and partial credit scoring. In spite of these publications advancing, explaining and illustrating the successful application of fundamental measurement in social science research, most contemporary psychometric tests and much practice are either unaware of the opportunity or mistake it for impractical.},
author = {Stone, Mark H and Wright, Benjamin D},
journal = {Measurement},
keywords = {Rasch,measurement},
pages = {205},
title = {{Measurement Essentials}},
year = {1999}
}
@book{Kruskal1978,
abstract = {This book is an excellent layman's introduction to the topic of Multi-Dimensional Scaling (MDS). There is just the right amount of detail to quickly get started in practical use of MDS. Some of the data and projects in this text are used as examples in StatSoft's Statistica software for PC's and Mac's; establishing a much needed connection between a reference text and software that incorporates the mathematics in the topic.},
address = {Newbury Park, California},
author = {Kruskal, Joseph B and Wish, Myron},
booktitle = {Quantitative Applications in Social Sciences},
isbn = {0803909403},
pages = {1--96},
publisher = {SAGE Publications Inc.},
title = {{Multidimensional Scaling (Quantitative Applications in the Social Sciences)}},
year = {1978}
}
@article{Marsh2006,
abstract = {We (Marsh {\&} Craven, 1997) have claimed that academic self-concept and achievement are mutually reinforcing, each leading to gains in the other. Baumeister, Campbell, Krueger, and Vohs (2003) have claimed that self-esteem has no benefits beyond seductive pleasure and may even be detrimental to subsequent performance. Integrating these seemingly contradictory conclusions, we distinguish between (a) older, unidimensional perspectives that focus on global self-esteem and underpin the Baumeister et al. review and (b) more recent, multidimensional perspectives that focus on specific components of self-concept and are the basis of our claim. Supporting the construct validity of a multidimensional perspective, studies show that academic achievement is substantially related to academic self-concept, but nearly unrelated to self-esteem. Consistent with this distinction, research based on our reciprocal-effects model (REM) and a recent meta-analysis show that prior academic self-concept (as opposed to self-esteem) and achievement both have positive effects on subsequent self-concept and achievement. We provide an overview of new support for the generality of the REM for young children, cross-cultural research in non-Western countries, health (physical activity), and nonelite (gymnastics) and elite (international swimming championships) sport. We conclude that future reviews elucidating the significant implications of self-concept for theory, policy, and practice need to account for current research supporting the REM and a multidimensional perspective of self-concept. {\textcopyright} 2006, Association for Psychological Science. All rights reserved.},
author = {Marsh, Herbert W and Craven, Rhonda G},
doi = {10.1111/j.1745-6916.2006.00010.x},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marsh, Craven - 2006 - Reciprocal Effects of Self-Concept and Performance From a Multidimensional Perspective Beyond Seductive Pleasure.pdf:pdf},
issn = {17456924},
journal = {Perspectives on Psychological Science},
month = {jun},
number = {2},
pages = {133--163},
pmid = {26151468},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Reciprocal Effects of Self-Concept and Performance From a Multidimensional Perspective: Beyond Seductive Pleasure and Unidimensional Perspectives}},
volume = {1},
year = {2006}
}
@incollection{Marsh2006a,
abstract = {In our research we have integrated specific and global self-esteem dimensions of self-concept into a single multidimensional, hierarchical model, but we argue that appropriately selected specific domains of self-concept are more useful than self-esteem in many research settings. Clearly it follows that our multidimensional perspective, which incorporates specific components of self-concept and self-esteem, is more useful than a unidimensional perspective that relies solely on self-esteem. Self-esteem is ephemeral in that it is more affected by short-term response biases, situation-specific context effects, short-term mood fluctuations, and other short-term time-specific influences. Self-esteem apparently cannot adequately reflect the diversity of specific self-domains. Indeed, as emphasized by Marsh and Yeung (1999), it is worrisome that a construct so central to the self seems to be so easily influenced by apparently trivial laboratory manipulations, bogus feedback, and short-term mood fluctuations. Despite the overwhelming empirical support for a multidimensional perspective on self-concept, we are not arguing that researchers should abandon self-esteem measures that have been used so widely and we include self-esteem as one of the scales on the Self Description Questionnaire (SDQ) instruments that are the basis of much of our research. Rather, we contend that researchers should consider multiple dimensions of self-concept particularly relevant to the concerns of their research, supplemented, perhaps, by self-esteem responses. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
address = {New York, NY, US},
author = {Marsh, Herbert W and Craven, Rhonda G and Martin, Andrew J},
booktitle = {Self-esteem issues and answers: A sourcebook of current perspectives.},
isbn = {1-84169-420-7},
keywords = {*Measurement,*Self-Concept,Self-Esteem},
pages = {16--24},
publisher = {Psychology Press},
title = {{What is the Nature of Self-Esteem? Unidimensional and Multidimensional Perspectives.}},
year = {2006}
}
@article{Garrido2019,
abstract = {Background: Factor-analysis based dimensional assessment of psychometric measures is a key step in the development of tests. However, current practices for deciding between a multiple-correlated or essentially unidimensional solution are clearly improvable. Method: A series of recent studies are reviewed, and an approach is proposed that combines multiple sources of information, which is expected to be used to make an informed judgement about the most appropriate dimensionality for the measure being studied. It uses both internal and external sources of information, and focuses on the properties of the scores derived from each of the solutions compared. Results: The proposal is applied to a reanalysis of a measure of symptoms of psychological distress. The results show that a clear and informed judgement about the most appropriate dimensionality of the measure in the target population can be obtained. Discussion: The proposal is useful and can be put into practice by using user-friendly, non-commercial software. We hope that this availability will result in good practice in the future.},
author = {Garrido, Caterina Calder{\'{o}}n and Gonz{\'{a}}lez, David Navarro and Seva, Urbano Lorenzo and Piera, Pere J.Ferrando},
doi = {10.7334/psicothema2019.153},
issn = {1886144X},
journal = {Psicothema},
keywords = {Essential unidimensionality factor score estimates,Exploratory factor analysis,External validity,Marginal reliability},
number = {4},
pages = {450--457},
pmid = {31634091},
publisher = {Colegio Oficial de Psicologos Asturias},
title = {{Multidimensional or essentially unidimensional? A multi-faceted factoranalytic approach for assessing the dimensionality of tests and items}},
volume = {31},
year = {2019}
}
@article{Hwang1970,
author = {Hwang, In Hong},
doi = {10.3946/kjme.2000.12.1.45},
issn = {2005-727X},
journal = {Korean Journal of Medical Education},
keywords = {Discrimination index,Item analysis,Item-total correlation},
month = {jun},
number = {1},
pages = {45--51},
title = {{The Usability of Item-Total Correlation as the Index of Item Discrimination}},
volume = {12},
year = {1970}
}
@article{Traub1997,
abstract = {What were the historic origins of classical test theory? What have been major milestones in its development?. {\textcopyright} 1997 Blackwell Publishing Ltd.},
author = {Traub, Ross E.},
doi = {10.1111/j.1745-3992.1997.tb00603.x},
issn = {17453992},
journal = {Educational Measurement: Issues and Practice},
number = {4},
pages = {8--14},
title = {{Classical test theory in historical perspective}},
volume = {16},
year = {1997}
}
@article{Miller1995,
abstract = {This article is a pedagogical piece on coefficient alpha ($\alpha$) and its uses. The classical approach to test reliability is explained. Test‐retest, alternative‐forms, and internal‐consistency methods ...},
author = {Miller, Michael B.},
doi = {10.1080/10705519509540013},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller - 1995 - Coefficient alpha A basic introduction from the perspectives of classical test theory and structural equation modeling.pdf:pdf},
issn = {15328007},
journal = {Structural Equation Modeling: A Multidisciplinary Journal},
month = {jan},
number = {3},
pages = {255--273},
publisher = {Taylor and Francis},
title = {{Coefficient alpha: A basic introduction from the perspectives of classical test theory and structural equation modeling}},
url = {https://doi.org/10.1080/10705519509540013},
volume = {2},
year = {1995}
}
@article{Wilcox1992,
abstract = {This paper describes a general class of reliability measures that contains the classical‐test‐theory measure as a special case. A particular member of this class is suggested for applied work, and some of its properties are studied. A lower bound for this new measure of reliability is derived which is a robust analogue of Cronbach's alpha. Here the term ‘robust' is being used to describe a measure of reliability that is relatively insensitive to small fluctuations in the tails of the distributions under study. The idea is that the value of a parameter, intended to reflect reliability, should be determined by the bulk of the distribution associated with whatever is being measured. If, for example, a test is considered unreliable when test scores have a normal distribution, it should not be possible to make the test appear to be reliable with a trivial change in the tails of the distributions. It is illustrated that Cronbach's alpha does not have this characteristic. In statistical jargon, Cronbach's alpha is not a resistant measure of reliability, while the reverse is true for the new measure of reliability introduced here. Included in this paper is a method for computing a confidence interval for the new lower bound. 1992 The British Psychological Society},
author = {Wilcox, Rand R.},
doi = {10.1111/j.2044-8317.1992.tb00990.x},
issn = {20448317},
journal = {British Journal of Mathematical and Statistical Psychology},
month = {nov},
number = {2},
pages = {239--254},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Robust generalizations of classical test reliability and Cronbach's alpha}},
volume = {45},
year = {1992}
}
@incollection{Adelson2019,
abstract = {The Reviewer's Guide to Quantitative Methods in the Social Sciences is designed for evaluators of research manuscripts and proposals in the social and behavioral sciences, and beyond. Its 31 uniquely structured chapters cover both traditional and emerging methods of quantitative data analysis, which neither junior nor veteran reviewers can bHancock, Gregory R Stapleton, Laura M O Mueller, Ralphe expected to know in detail. The book updates readers on each technique's key principles, appropriate usage, underlying assumptions, and limitations. It thereby assists reviewers to offer constructive commentary on works they evaluate, and also serves as an indispensable author's reference for preparing sound research manuscripts and proposals. Key features include: Comprehensive Coverage—Thirty-one chapters cover virtually all of the classic and emerging quan-titative techniques, thus helping reviewers to evaluate a manuscript's methodological approach to data analysis. In addition, the volume serves as an indispensable reference tool for those designing their own research. Unique Chapter Format—For ease of use, all chapters follow the same structure. • The opening page of each chapter defines and explains the purpose of that statistical method. • The next one or two pages provide a table listing various criteria that should be considered when evaluating and applying that methodological approach to data analysis. • The remainder of each chapter contains numbered sections corresponding to the criteria listed in the opening table. Each section explains the role and importance of that particular criterion. Expert Chapter Authors—Chapters are written by methodological and applied scholars who are expert in the particular quantitative method being reviewed.},
address = {New York},
author = {Adelson, Jill L. and Osborne, Jason W. and Crawford, Brittany F.},
booktitle = {The Reviewer's Guide to Quantitative Methods in the Social Sciences},
chapter = {5},
doi = {10.4324/9781315755649-5},
isbn = {9781138800120},
pages = {55--71},
publisher = {Routledge},
title = {{Correlation and Other Measures of Association}},
year = {2019}
}
@article{Nelsen1992,
abstract = {We show that Spearman's rho is a measure of average positive (and negative) quadrant dependence, and that Kendall's tau is a measure of average total positivity (and reverse regularity) of order two. {\textcopyright} 1992.},
author = {Nelsen, Roger B.},
doi = {10.1016/0167-7152(92)90056-B},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Kendall's tau,Measures of association,Spearman's rho,copulas,positive dependence properties,positive quadrant dependence,total positivity of order two},
month = {jul},
number = {4},
pages = {269--274},
publisher = {North-Holland},
title = {{On measures of association as measures of positive dependence}},
volume = {14},
year = {1992}
}
@incollection{Mair2018,
address = {Cham, Switzerland},
author = {Mair, Patrick},
booktitle = {Modern Psychometrics with R},
chapter = {1-2},
doi = {10.1007/978-3-319-93177-7_1},
pages = {1--34},
publisher = {Springer},
title = {{Classical Test Theory and Factor analysis}},
year = {2018}
}
@incollection{DeLeeuw2005,
author = {de Leeuw, Jan},
booktitle = {Encyclopedia of Statistics in Behavioral Science},
doi = {10.1002/0470013192.bsa700},
keywords = {fitting distances,multidimensional scaling},
month = {oct},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Unidimensional Scaling}},
year = {2005}
}
@incollection{Mair2015,
abstract = {This seventy pages long chapter covers following contents: Introduction to Scaling Thurston Scaling Likert Scaling (self-esteem as an illustration of this scale) Guttman Scaling (voting in the U.S. Senate on the creation of a Consumer Protection Agency and of the structure of participation in American politics as an illustration of this scale) Unifolding Thory (the measurement of cosmopolitianism and trust in government to illustrate the potential use of this theiry in social science research)},
author = {Mair, Patrick and Leeuw, Jan De},
booktitle = {Wiley StatsRef: Statistics Reference Online},
doi = {10.1002/9781118445112.stat06462.pub2},
keywords = {combinatorial optimization,fitting distances,multidimensional scaling},
month = {jun},
pages = {1--3},
publisher = {Wiley},
title = {{Unidimensional Scaling}},
year = {2015}
}
@article{Sijtsma2009,
abstract = {This discussion paper argues that both the use of Cronbach's alpha as a reliability estimate and as a measure of internal consistency suffer from major problems. First, alpha always has a value, which cannot be equal to the test score's reliability given the interitem covariance matrix and the usual assumptions about measurement error. Second, in practice, alpha is used more often as a measure of the test's internal consistency than as an estimate of reliability. However, it can be shown easily that alpha is unrelated to the internal structure of the test. It is further discussed that statistics based on a single test administration do not convey much information about the accuracy of individuals' test performance. The paper ends with a list of conclusions about the usefulness of alpha.},
author = {Sijtsma, Klaas},
doi = {10.1007/s11336-008-9101-0},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sijtsma - 2009 - On the use, the misuse, and the very limited usefulness of cronbach's alpha.pdf:pdf},
issn = {00333123},
journal = {Psychometrika},
keywords = {Cronbach's alpha,Internal consistency,Reliability,Unidimensionality},
month = {mar},
number = {1},
pages = {107--120},
pmid = {20037639},
title = {{On the use, the misuse, and the very limited usefulness of cronbach's alpha}},
url = {/record/2009-03053-007},
volume = {74},
year = {2009}
}
@article{Raykov2007,
abstract = {This paper demonstrates that the widely available and routinely used index 'coefficient alpha if item deleted' can be misleading in the process of construction and revision of multiple-component instruments with congeneric measures. An alternative approach to evaluation of scale reliability following deletion of each component in a given composite is outlined that can be recommended in general for scale development purposes. The method provides ranges of plausible values for instrument reliability when dispensing with single components in a tentative composite, and permits testing hypotheses about reliability of resulting scale versions. The proposed procedure is illustrated with an example. {\textcopyright} 2007 The British Psychological Society.},
author = {Raykov, Tenko},
doi = {10.1348/000711006X115954},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raykov - 2007 - Reliability if deleted, not 'alpha if deleted' Evaluation of scale reliability following component deletion.pdf:pdf},
issn = {00071102},
journal = {British Journal of Mathematical and Statistical Psychology},
month = {nov},
number = {2},
pages = {201--216},
pmid = {17971267},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Reliability if deleted, not 'alpha if deleted': Evaluation of scale reliability following component deletion}},
volume = {60},
year = {2007}
}
@article{Raykov1998,
abstract = {The relationship between Cronbach's coefficient alpha ($\alpha$) and the reliability of a composite of a prespecified set of interrelated nonhomogeneous components is examined. It is shown that $\alpha$ can over-or underestimate scale reliability at the population level. The bias is expressed in terms of structural parameters and illustrated on simulated data. The relevance of substantive considerations about, and examination of, the latent structure of an item set prior to estimation of composite reliability using $\alpha$ is emphasized and demonstrated using a structural equation modeling approach. Index terms: bias, classical test theory, coefficient alpha, composite reliability, exploratory factor analysis, structural equation modeling.},
author = {Raykov, Tenko},
doi = {10.1177/014662169802200407},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raykov - 1998 - Coefficient alpha and composite reliability with interrelated nonhomogeneous items.pdf:pdf},
issn = {01466216},
journal = {Applied Psychological Measurement},
number = {4},
pages = {375--385},
publisher = {SAGE Publications Inc.},
title = {{Coefficient alpha and composite reliability with interrelated nonhomogeneous items}},
volume = {22},
year = {1998}
}
@article{Raykov2001,
abstract = {The population discrepancy of coefficient $\alpha$ from the composite reliability coefficient for fixed congeneric measures with correlated errors is studied and expressed in terms of parameters of the measures. Use of structural equation modeling methodology is recommended for identifying cases in which this discrepancy can be large. The findings are demonstrated across several empirical conditions in a scale construction context.},
author = {Raykov, Tenko},
doi = {10.1177/01466216010251005},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raykov - 2001 - Bias of coefficient $\alpha$ for fixed congeneric measures with correlated errors.pdf:pdf},
issn = {01466216},
journal = {Applied Psychological Measurement},
keywords = {Coefficient $\alpha$,Scale reliability,Structural equation modeling},
number = {1},
pages = {69--76},
publisher = {SAGE Publications Inc.},
title = {{Bias of coefficient $\alpha$ for fixed congeneric measures with correlated errors}},
volume = {25},
year = {2001}
}
@article{Raykov1997,
abstract = {A structural equation model is described that permits estimation of the reliability index and coefficient of a composite test for congeneric measures. The method is also helpful in exploring the factorial structure of an item set, and its use in scale reliability estimation and development is illustrated. The estimator of composite reliability it yields does not possess the general underestimation property of Cronbach's coefficient $\alpha$.},
author = {Raykov, Tenko},
doi = {10.1177/01466216970212006},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raykov - 1997 - Estimation of composite reliability for congeneric measures.pdf:pdf},
issn = {01466216},
journal = {Applied Psychological Measurement},
keywords = {Classical test theory,Coefficient $\alpha$,Composite reliability,Congeneric tests,Scale development,Structural equation modeling},
month = {jul},
number = {2},
pages = {173--184},
publisher = {SAGE Publications Inc.},
title = {{Estimation of composite reliability for congeneric measures}},
volume = {21},
year = {1997}
}
@article{Livingston1972,
abstract = {A reliability coefficient for criterion‐referenced tests is developed from the assumptions of classical test theory. This coefficient is based on deviations of scores from the criterion score, rather than from the mean. The coefficient is shown to have several of the important properties of the conventional normreferenced reliability coefficient, including its interpretation as a ratio of variances and as a correlation between parallel forms, its relationship to test length, its estimation from a single form of a test, and its use in correcting for attenuation due to measurement error. Norm‐referenced measurement is considered as a special case of criterion‐referenced measurement. Copyright {\textcopyright} 1972, Wiley Blackwell. All rights reserved},
author = {Livingston, Samuel A.},
doi = {10.1111/j.1745-3984.1972.tb00756.x},
issn = {17453984},
journal = {Journal of Educational Measurement},
number = {1},
pages = {13--26},
title = {{Criterion Referenced Applications of Classical Test Theory}},
volume = {9},
year = {1972}
}
@article{Weiss1981,
abstract = {Reviews the test theory literature published between January 1975 and December 1979, focusing on procedures for measuring ability, aptitude, and other cognitive variables and for estimating the precision and validity of these measurements. Recent developments and refinements in classical test theory and methods, including reliability and generalizability theory; alternatives to classical test theory (criterion-referenced testing, latent trait test theory, and order models); content, construct, and criterion-related validity issues; and test fairness are examined. The need for future research to place less emphasis on classical test theory and the derivation of new formulas for already known concepts and more emphasis on the evaluation of alternative models for improving the design, construction, and implementation of psychological measuring instruments is discussed. (7 p ref) (PsycINFO Database Record (c) 2000 APA, all rights reserved)(unassigned)},
author = {Weiss, D J and Davison, M L},
doi = {10.1146/annurev.ps.32.020181.003213},
issn = {0066-4308},
journal = {Annual Review of Psychology},
number = {1},
pages = {629--658},
title = {{Test Theory and Methods}},
volume = {32},
year = {1981}
}
@incollection{Marais2013,
abstract = {The unidimensional Rasch model for dichotomous items and the unidimensional Rasch model for more than two ordered categories rely on the assumption of local independence. This chapter discusses tests of the assumption of local independence of responses and the implications of violations of this assumption in data. Local dependence is related to subtest structure and to estimates of reliability and therefore these topics are reviewed in the chapter briefly. Some examples of response dependence in health outcome scales are given. The chapter outlines effects of response dependence on estimates, using evidence from simulation studies. Different ways by which response dependence can be detected and diagnosed are discussed and illustrated. {\textcopyright} 2013 by John Wiley {\&} Sons, Inc.},
author = {Marais, Ida},
booktitle = {Rasch Models in Health},
doi = {10.1002/9781118574454.ch7},
isbn = {9781848212220},
keywords = {Estimates,Health outcome scales,Item fit,Item residual correlations,Local dependence,Local independence,Rasch models,Reliability},
pages = {111--130},
title = {{Local Dependence}},
year = {2013}
}
@article{Henning1989,
abstract = {This paper reviews a variety of definitions of local (conditional) independence presented in the testing literature. An attempt is made to clarify and differen tiate among conflicting conceptualizations of this fundamental measurement principle. In particular, it is argued that local independence, unidimension ality, and noninvasiveness are important but distinct concepts that may, but need not necessarily, overlap. Methods of testing for the presence of local independence in several of its conceptualizations are also presented. {\textcopyright} 1989, Sage Publications. All rights reserved.},
author = {Henning, Grant},
doi = {10.1177/026553228900600108},
issn = {14770946},
journal = {Language Testing},
number = {1},
pages = {95--108},
title = {{Meanings and implications of the principle of local independence}},
volume = {6},
year = {1989}
}
@article{VanSchuur1989,
abstract = {This paper discusses a number of problems with existing unfolding models and proposes a strategy of analysis to overcome these problems. This strategy assumes dichotomous or dichotomized data, and derives unfoldability criteria from information about ordered triples of stimuli. A unidimensional unfolding scale conforming to these criteria can be found for a maximal subset of stimuli. This procedure can be applied to full or partial rank orders of preference, which are dichotomized to “pick k/n” data, and to Likert-type rating scales, which are dichotomized to “pick any/n” data. This procedure is applicable to large data sets, such as survey data. As an example, the procedure is applied to preferences for five German political parties in electoral surveys in 1969, 1972, and 1980. A dominant left-right unfolding dimension is found, and violations of this representation are discussed. {\textcopyright} 1989 ELSEVIER B.V.},
author = {van Schuur, Wijbrandt H.},
doi = {10.1016/S0166-4115(08)60240-X},
issn = {01664115},
journal = {Advances in Psychology},
month = {jan},
number = {C},
pages = {259--290},
publisher = {North-Holland},
title = {{Unfolding the German Political Parties: A Description and Application of Multiple Unidimensional Unfolding}},
volume = {60},
year = {1989}
}
@misc{Andrich2004,
abstract = {The development of Rasch models in educational and psychologic measurement in the 1960s coincided with the introduction of other similar models, now described as models of item response theory (IRT). The application of IRT models has now extended to other social sciences, including health. Originally, there was substantial controversy between those who saw Rasch models as simply special cases of IRT models and those who saw them as essentially different. Because these different perspectives continue to manifest themselves in various ways, it seems relevant to understand the source of the original controversy. This paper attempts to do so by invoking Kuhn's studies in the history and philosophy of science at 3 levels. First, it suggests that the 2 perspectives reflect Kuhn's concept of legitimate, incompatible paradigms in which controversy is a typical manifestation. Second, because Kuhn recognizes individual histories in the development of paradigms, Rasch's own shift in perspective is summarized. Third, because proponents of the Rasch models emphasize the models' compatibility with fundamental measurement found in physical science, an analogy is made between how Kuhn explains the role of measurement in the physical sciences and how proponents of Rasch models explain the role of these models in the social sciences. In particular, these roles cannot be gleaned from textbooks in science and statistics, respectively.},
author = {Andrich, David},
booktitle = {Medical care},
doi = {10.1097/01.mlr.0000103528.48582.7c},
issn = {00257079},
number = {1 Suppl},
title = {{Controversy and the Rasch model: a characteristic of incompatible paradigms?}},
volume = {42},
year = {2004}
}
@article{Wilson1995,
abstract = {This paper discusses the application of a class of Rasch models to situations where test items are grouped into subsets and the common attributes of items within these subsets brings into question the usual assumption of conditional independence. The models are all expressed as particular cases of the random coefficients multinomial logit model developed by Adams and Wilson. This formulation allows a very flexible approach to the specification of alternative models, and makes model testing particularly straightforward. The use of the models is illustrated using item bundles constructed in the framework of the SOLO taxonomy of Biggs and Collis. {\textcopyright} 1995 The Psychometric Society.},
author = {Wilson, Mark and Adams, Raymond J},
doi = {10.1007/BF02301412},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson, Adams - 1995 - RASCH MODELS FOR ITEM BUNDLES.pdf:pdf},
issn = {00333123},
journal = {Psychometrika},
keywords = {Rasch model,conditional independence,item bundles,partial credit model},
number = {2},
pages = {181--198},
title = {{Rasch models for item bundles}},
volume = {60},
year = {1995}
}
@book{Gregory2014,
abstract = {Psychological testing is one of the primary methodologies used by psychologists in research and practice settings. This article describes the essential features of this methodology, starting with a definition of what a psychological test is and a history of how testing started and developed in the early part of the twentieth century. The dual nature of psychological tests as both professional tools and commercial products is discussed and a clear distinction is made between the different purposes tests serve and the different bases on which they are evaluated when viewed as tools and as products.},
address = {Boston, Amerstdam},
author = {Gregory, Robert J.},
booktitle = {Encyclopedia of Mental Health: Second Edition},
isbn = {9780123970459},
keywords = {Computer adaptive testing,Criterion-referenced,Intelligence quotient (IQ),Item response theory,Measure,Measurement error,Norm-referenced,Psychological assessment,Psychological test,Reliability,Standardization,Test publisher,Validity},
pages = {590},
publisher = {Pearson},
title = {{Psychological Testing History, Principles, and Applications seventh edition}},
year = {2014}
}
@incollection{Brown2013,
abstract = {Classical test theory (CTT) was first developed and predominated in measurement circles during the early to middle twentieth century and is “a measurement theory which consists of a set of assumptions about the relationships between actual or observed test scores and the factors that affect these scores, which are generally referred to as error” (Association of Language Testers in Europe, 1998: 138). “At the heart of CTT is the assertion that an observed score is determined by the actual state of the unobservable variable of interest plus error contributed by all other influences on the observable variable. The actual state of the unobserved variable is its hypothetical true score” (DeVellis, 2006: S50). Clearly, the notions of error and true scores are central to CTT and therefore to all else that follows in this chapter. The recognition that all measurement contains errors can be traced back at least as far as Spearman (1904: 76), who observed that individuals created “accidental deviations” in any measurement which become “variable errors” for groups of individuals. Put in the parlance of CTT, observed scores (i.e., the examinees' actual scores on a test) contain errors (unsystematic effects due to factors not being measured), which in turn contribute to error variance (unsystematic variation in the scores that is due solely to random errors). Such error variance is taken to be random because it can arise from a variety of extraneous, non-systematic sources, which typically have nothing to do with the purposes for which the test was designed, for example, sources of error arise in the environment (e.g., noise, heat, lighting, etc.), administration procedures (e.g., directions, equipment, timing, etc.), scoring procedures (subjectivity, math errors, scorer biases, etc.), test items (e.g., item types, item quality, test security, etc.), the examinees themselves (e.g., health, fatigue, motivation, etc.), and so forth (Brown, 2005: 171-75). Unless the scores on a particular test are completely random (i.e., 100{\%} error), some pro-portion of the observed score variance that results from the test will be attributable to the construct that the test was designed to measure. True scores are hypothetical representations of the scores that would result if there were no errors in measurement. Variation among examinees in their hypothetical true scores is called true score variance. Logically, any set of observed scores will be made up of both true score variance and error variance. Let's represent these relationships as follows: Total test variance = true score variance + error variance VarTotal 1/4 VarTrue Score p VarError o1p This notion that observed score variance is made up of true score variance plus error variance underlies the entire framework of CTT.},
address = {New York, NY},
author = {Brown, James Dean J.D.},
booktitle = {The Routledge Handbook of Language Testing},
doi = {10.4324/9780203181287-35},
isbn = {9781136590863},
pages = {323--335},
publisher = {Springer New York},
title = {{Classical test theory}},
year = {2013}
}
@incollection{McGartlandRubio2004,
author = {{McGartland Rubio}, Doris},
booktitle = {Encyclopedia of Social Measurement},
doi = {10.1016/B0-12-369398-5/00395-9},
isbn = {9780123693983},
month = {jan},
pages = {59--63},
publisher = {Elsevier Inc.},
title = {{Alpha Reliability}},
year = {2004}
}
@article{Streiner2003,
abstract = {Cronbach's $\alpha$ is the most widely used index of the reliability of a scale. However, its use and interpretation can be subject to a number of errors. This article discusses the historical development of $\alpha$ from other indexes of internal consistency (split-half reliability and Kuder-Richardson 20) and discusses four myths associated with $\alpha$: (a) that it is a fixed property of the scale, (b) that it measures only the internal consistency of the scale, (c) that higher values are always preferred over lower ones, and (d) that it is restricted to the range of 0 to 1. It provides some recommendations for acceptable values of $\alpha$ in different situations.},
author = {Streiner, David L.},
doi = {10.1207/S15327752JPA8001_18},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Streiner - 2003 - Starting at the beginning An introduction to coefficient alpha and internal consistency.pdf:pdf},
issn = {00223891},
journal = {Journal of Personality Assessment},
number = {1},
pages = {99--103},
pmid = {12584072},
publisher = {Lawrence Erlbaum Associates Inc.},
title = {{Starting at the beginning: An introduction to coefficient alpha and internal consistency}},
volume = {80},
year = {2003}
}
@article{Green1977,
abstract = {Confusion in the literature between the concepts of internal consistency and homogeneity has led to a misuse of coefficient alpha as an index of item homogeneity. Coefficient alpha is actually a complexly determined test statistic, item homogeneity only being one influence on its magnitude. The related statistic, the average intercorrelation, has similar difficulties. Several indices of item homogeneity derived from the model of common factor analysis are offered as alternatives. {\textcopyright} 1977, Sage Publications. All rights reserved.},
author = {Green, Samuel B. and Lissitz, Robert W. and Mulaik, Stanlen A.},
doi = {10.1177/001316447703700403},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Green, Lissitz, Mulaik - 1977 - Limitations of coefficient alpha as an index of test unidimensionality1.pdf:pdf},
issn = {15523888},
journal = {Educational and Psychological Measurement},
month = {jul},
number = {4},
pages = {827--838},
publisher = {Sage PublicationsSage CA: Thousand Oaks, CA},
title = {{Limitations of coefficient alpha as an index of test unidimensionality1}},
url = {https://journals.sagepub.com/doi/abs/10.1177/001316447703700403?casa{\_}token=qELS6qQlbjYAAAAA{\%}3A-FuZedzfBUQ1F7pTWgxeAhbQ515Fcw8ySfkR8Mot4yw87O-d140SHNxjUafaE-wahdoNYh8m0f4},
volume = {37},
year = {1977}
}
@misc{Hattie1985,
abstract = {Various methods for determining unidimensionality are reviewed and the rationale of these methods is as sessed. Indices based on answer patterns, reliability, components and factor analysis, and latent traits are reviewed. It is shown that many of the indices lack a rationale, and that many are adjustments of a previous index to take into account some criticisms of it. After reviewing many indices, it is suggested that those based on the size of residuals after fitting a two- or three-parameter latent trait model may be the most useful to detect unidimensionality. An attempt is made to clarify the term unidimensional, and it is shown how it differs from other terms often used inter changeably such as reliability, internal consistency, and homogeneity. Reliability is defined as the ratio of true score variance to observed score variance. Inter nal consistency denotes a group of methods that are intended to estimate reliability, are based on the vari ances and the covariances of test items, and depend on only one administration of a test. Homogeneity seems to refer more specifically to the similarity of the item correlations, but the term is often used as a synonym for unidimensionality. The usefulness of the terms in ternal consistency and homogeneity is questioned. Uni dimensionality is defined as the existence of one latent trait underlying the data. {\textcopyright} 1985, Sage Publications. All rights reserved.},
author = {Hattie, John},
booktitle = {Applied Psychological Measurement},
doi = {10.1177/014662168500900204},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hattie - 1985 - Methodology Review Assessing Unidimensionality of Tests and Items.pdf:pdf},
issn = {15523497},
number = {2},
pages = {139--164},
title = {{Methodology Review: Assessing Unidimensionality of Tests and Items}},
volume = {9},
year = {1985}
}
@article{Klein2014,
abstract = {This study presents a new approach to assessing commitment reflecting the Klein, Molloy, and Brinsfield (2012) reconceptualization. Klein et al. recast the construct to address issues hindering commitment scholarship, but their claims cannot be tested with existing measures. This paper presents a 4-item measure consistent with the Klein et al. conceptual definition, a measure intended to be unidimensional and applicable across all workplace targets. Our purpose is to present the development of and provide initial validity evidence for this new commitment measure and to compare it to existing alternative measures. Hypotheses around these objectives were tested with data gathered across 5 samples yielding 2,487 participants representing a wide range of jobs, organizations, and industries. Each sample examined a unique set of variables and targets that together provide a comprehensive test of this new measure relative to 8 different targets, several constructs within the nomological network, and 4 prior commitment measures. Results support our hypotheses regarding (a) the measure's properties and structure, (b) convergence and divergence with prior measures of commitment and other constructs in the nomological network, and (c) advantages over prior measures. These findings support the validity of this new approach to assessing commitment, laying the foundation for future research to address critiques of the commitment construct; better examine the multiple commitments individuals simultaneously hold; and bring consistency, synergy, and integration to commitment scholarship across workplace targets. The conceptual, methodological, and practical benefits of the measure are discussed, along with study limitations and future research opportunities. {\textcopyright} 2013 American Psychological Association.},
author = {Klein, Howard J and Cooper, Joseph T and Molloy, Janice C. and Swanson, Jacqueline A.},
doi = {10.1037/a0034751},
issn = {00219010},
journal = {Journal of Applied Psychology},
keywords = {Commitment,Measurement,Validation},
number = {2},
pages = {222--238},
pmid = {24188389},
title = {{The assessment of commitment: Advantages of a unidimensional, target-free approach}},
volume = {99},
year = {2014}
}
@article{Gardner1995,
abstract = {Summated ratings attitude scales commonly consist of numerous items whose scores are summed to yield a total score. An assumption underlying this technique is that the items in the scale reflect a common construct. If this is not met, the procedure produces uninterpretable data. Although this psychometric principle has been known for a long time, numerous studies in the literature demonstrate a neglect of it. Some make no attempt to conceptualise the construct to be measured; others conceptualise the construct but then ignore the possibility that it may be multidimensional; still others contain evidence indicating that the construct is multidimensional and then proceed to ignore that evidence. A possible contributor to the confusion is the misunderstanding of the related yet distinct concepts of internal consistency and unidimensionality. This paper presents examples of poor and good instrument design, in the hope that clarification of the issues might make a difference in the future. {\textcopyright} 1995 Australasian Science Education Research Association.},
author = {Gardner, Paul L},
doi = {10.1007/BF02357402},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gardner - 1995 - Measuring Attitudes to Science Unidimensionality and Internal Consistency Revisited.pdf:pdf},
issn = {0157244X},
journal = {Research in Science Education},
number = {3},
pages = {283--289},
title = {{Measuring attitudes to science: Unidimensionality and internal consistency revisited}},
volume = {25},
year = {1995}
}
@article{DeVellis2006,
abstract = {Classical test theory (CTT) comprises a set of concepts and methods that provide a basis for many of the measurement tools currently used in health research. The assumptions and concepts underlying CTT are discussed. These include item and scale characteristics that derive from CTT as well as types of reliability and validity. Procedures commonly used in the development of scales under CTT are summarized, including factor analysis and the creation of scale scores. The advantages and disadvantages of CTT, its use across populations, and its continued use in the face of more recent measurement models are also discussed. {\textcopyright} 2006 Lippincott Williams {\&} Wilkins, Inc.},
author = {DeVellis, Robert F.},
doi = {10.1097/01.mlr.0000245426.10853.30},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/DeVellis - 2006 - Classical test theory.pdf:pdf},
issn = {00257079},
journal = {Medical Care},
keywords = {Classical test theory,Measurement,Reliability},
month = {nov},
number = {11 SUPPL. 3},
pmid = {17060836},
publisher = {Med Care},
title = {{Classical test theory}},
volume = {44},
year = {2006}
}
@article{Martin1997,
abstract = {Recent research has suggested that the Social Physique Anxiety Scale (SPAS) is a multidimensional rather than a unidimensional measure. The present study challenged this position on both conceptual and empirical grounds. After deleting three questionable items from the SPAS, a series of confirmatory factor analyses were conducted across four samples of women who had completed the scale. Across all samples, the model fit indices (i.e., all {\textgreater} .90) suggested that a nine-item, single factor model of the SPAS is more parsimonious and conceptually clear than a two-factor model. It is recommended that researchers of social physique anxiety begin to use the nine-item version of the SPAS described in this paper.},
author = {Martin, Kathleen A. and Rejeski, W. Jack and Leary, Mark R. and McAuley, Edward and Bane, Susan},
doi = {10.1123/jsep.19.4.359},
issn = {08952779},
journal = {Journal of Sport and Exercise Psychology},
keywords = {Psychometrics,Scale development},
month = {dec},
number = {4},
pages = {359--367},
publisher = {Human Kinetics Publishers Inc.},
title = {{Is the social physique anxiety scale really multidimensional? Conceptual and statistical arguments for a unidimensional model}},
volume = {19},
year = {1997}
}
@article{Dodds2012,
abstract = {Miller (1956) identified his famous limit of 7±2 items based in part on absolute identification-the ability to identify stimuli that differ on a single physical dimension, such as lines of different length. An important aspect of this limit is its independence from perceptual effects and its application across all stimulus types. Recent research, however, has identified several exceptions. We investigate an explanation for these results that reconciles them with Miller's work. We find support for the hypothesis that the exceptional stimulus types have more complex psychological representations, which can therefore support better identification. Our investigation uses data sets with thousands of observations for each participant, which allows the application of a new technique for identifying psychological representations: the structural forms algorithm of Kemp and Tenenbaum (2008). This algorithm supports inferences not possible with previous techniques, such as multidimensional scaling. {\textcopyright} 2012 Cognitive Science Society, Inc.},
author = {Dodds, Pennie and Rae, Babette and Brown, Scott},
doi = {10.1111/cogs.12003},
file = {:home/steven/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dodds, Rae, Brown - 2012 - Perhaps Unidimensional Is Not Unidimensional.pdf:pdf},
issn = {03640213},
journal = {Cognitive Science},
keywords = {Absolute identification,Miller,Multidimensional scaling,Structural forms algorithm,Unidimensional},
month = {nov},
number = {8},
pages = {1542--1555},
pmid = {23033989},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Perhaps Unidimensional Is Not Unidimensional}},
volume = {36},
year = {2012}
}
@article{CrawfordClark2002,
abstract = {Pain is now regarded as 'the fifth vital sign' and patients are frequently asked to score the intensity of their pain on a numerical pain rating scale (NPRS). However, the use of a unidimensional scale is questionable in view of the belief, overwhelmingly supported by clinical experience as well as by empirical evidence from multidimensional scaling and other sources, that pain has at least two dimensions: somatosensory qualities and affect. We used a Chinese translation of the 101 descriptor multidimensional affect and pain survey (MAPS) questionnaire to determine the relative contributions of various dimensions of postoperative pain to a patient's score on a unidimensional NPRS. MAPS and NPRS were administered postoperatively to 69 patients with descending colon carcinoma who were recovering from left hemi-colectomy. Multiple linear regression revealed that the emotional pain qualities supercluster (P=0.0005) and four of its eight subclusters, anxiety, depressed mood, fear and anger, significantly (P=0.001-0.007) predicted a patient's score on the unidimensional NPRS. Notably, none of the 17 subclusters in the somatosensory pain qualities supercluster predicted NPRS scores. It may be concluded that patient scores on unidimensional pain intensity scales reflect the emotional qualities of pain much more than its sensory intensity or other qualities. Accordingly such scales are poor indicators of analgesic requirement. The results also suggest that patients' postoperative anxiety and depression are inadequately treated. Based on our findings we present six unidimensional scales that should yield a more accurate assessment of the sources of a patient's pain. {\textcopyright} 2002 International Association for the Study of Pain. Published by Elsevier Science B.V. All rights reserved.},
author = {{Crawford Clark}, W. and Yang, Joseph C. and Tsui, Siu Lun and Ng, Kwok Fu and {Bennett Clark}, Susanne},
doi = {10.1016/S0304-3959(01)00474-2},
issn = {03043959},
journal = {Pain},
keywords = {Multidimensional affect and pain survey,Pain,Pain measurement,Pain postoperative,Questionnaire},
month = {aug},
number = {3},
pages = {241--247},
pmid = {12127025},
publisher = {Elsevier},
title = {{Unidimensional pain rating scales: A multidimensional affect and pain survey (MAPS) analysis of what they really measure}},
volume = {98},
year = {2002}
}
